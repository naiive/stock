#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡å†å²æ•°æ®æ‰¹é‡å¯¼å…¥ MySQL è„šæœ¬ (Bulk Insert Optimized)
åŠŸèƒ½ï¼šå¹¶å‘æŠ“å–å…¨å¸‚åœº A è‚¡æ—¥çº¿æ•°æ®ï¼Œåˆå¹¶åæ‰¹é‡å¯¼å…¥ï¼Œä½¿ç”¨å…¨å±€ DELETE å®ç°è¦†ç›–æ›´æ–°ã€‚
============================================================
"""
import os
import json
import time
import random
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, wait, TimeoutError as ThreadingTimeoutError

import pandas as pd
import akshare as ak
import asyncio
from tqdm import tqdm

# --- MySQL ä¾èµ– ---
from sqlalchemy import create_engine, text

# ============================================================
# æ¨¡å— 1ï¼šé…ç½® (Configuration)
# ============================================================
CONFIG = {
    # --- MySQL é…ç½® ---
    "DB": {
        "host": "localhost",
        "port": 3306,
        "user": "root",  # è¯·æ›¿æ¢
        "password": "Elaiza112233",  # è¯·æ›¿æ¢
        "database": "stock"  # è¯·æ›¿æ¢
    },
    "MYSQL_TABLE": "a_stock_daily",  # ç›®æ ‡è¡¨å

    # --- æ—¶é—´èŒƒå›´ ---
    "DAYS": 500,  # æŠ“å–çš„å†å²æ•°æ®æ—¶é•¿

    # --- è¿‡æ»¤æ¡ä»¶ ---
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ï¼‰
    "EXCLUDE_BJ": True,   # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",  # å¤æƒæ–¹å¼: 'qfq' (å‰å¤æƒ)

    # --- æŠ½æ ·/å¹¶å‘ ---
    "SAMPLE_SIZE": 0,  # 0 æˆ– None è¡¨ç¤ºå…¨é‡ï¼Œ>0 è¡¨ç¤ºéšæœºæŠ½æ ·æ•°é‡
    "MAX_WORKERS": 8,  # çº¿ç¨‹æ•°
    "REQUEST_TIMEOUT": 30,  # AkShare å•æ¬¡è¯·æ±‚æ•´ä½“è¶…æ—¶ä¿æŠ¤ï¼ˆç§’ï¼‰
    "CACHE_FILE": "stock_list_cache.json",
}


# ============================================================
# å·¥å…·ï¼šæ•°æ®åº“è¿æ¥
# ============================================================
def get_db_engine():
    """åˆ›å»ºå¹¶è¿”å›æ•°æ®åº“è¿æ¥å¼•æ“"""
    db_conf = CONFIG["DB"]
    url = f"mysql+pymysql://{db_conf['user']}:{db_conf['password']}@{db_conf['host']}:{db_conf['port']}/{db_conf['database']}?charset=utf8mb4"
    try:
        engine = create_engine(url, pool_recycle=3600)
        return engine
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise


# ============================================================
# å·¥å…·ï¼šé‡è¯•è£…é¥°å™¨
# ============================================================
def retry(max_retries=3, delay=2):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == max_retries - 1:
                        raise
                    time.sleep(delay)

        return wrapper

    return decorator


# ============================================================
# æ¨¡å— 2ï¼šè·å–/ç¼“å­˜ å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
# ============================================================
@retry(max_retries=2, delay=1)
def fetch_stock_list_safe():
    """è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ï¼Œé‡‡ç”¨é™çº§ç­–ç•¥ä»¥æé«˜ç¨³å®šæ€§ã€‚"""
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            return df[["code", "name"]]
    except Exception:
        pass

    try:
        df = ak.stock_zh_a_spot_em()
        return df[["code", "name"]]
    except Exception as e:
        raise Exception(f"æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨æ¥å£å‡ä¸å¯ç”¨: {e}")


def get_stock_list_manager():
    """ç¼“å­˜ç®¡ç†å™¨ï¼šä¼˜å…ˆè¯»å–æœ¬åœ°ç¼“å­˜ï¼Œè¿‡æœŸæˆ–ä¸å­˜åœ¨åˆ™è”ç½‘æ›´æ–°ã€‚"""
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                return pd.DataFrame(cache["data"])
        except Exception:
            pass

    print("[ç³»ç»Ÿ] æ­£åœ¨è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")
    df = fetch_stock_list_safe()

    if not df.empty:
        with open(cache_file, "w", encoding="utf-8") as f:
            data = {
                "time": today_str,
                "data": df.to_dict(orient="records")
            }
            json.dump(data, f, ensure_ascii=False, indent=2)

    return df


def filter_stock_list(df):
    if df is None or df.empty:
        return []
    df["code"] = df["code"].astype(str)
    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]:
        mask |= df["code"].str.startswith("300")
    if CONFIG["EXCLUDE_KCB"]:
        mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]:
        mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns:
        mask |= df["name"].str.contains("ST|é€€", na=False)  # æ’é™¤ ST/é€€å¸‚è‚¡

    # æ„é€  AkShare æ‰€éœ€çš„ symbol æ ¼å¼ (sh600xxx, sz00xxxx)
    df['symbol'] = df["code"].apply(
        lambda x: f"sh{x}" if x.startswith("6") else f"sz{x}"
    )
    df['symbol'] = df['symbol'].astype(str)  # ç¡®ä¿ symbol æ˜¯å­—ç¬¦ä¸²

    return df[~mask][['code', 'name', 'symbol']].to_dict('records')


# ============================================================
# æ¨¡å— 3ï¼šæ•°æ®æŠ“å– (ä»…æŠ“å–ï¼Œä¸å¯¼å…¥)
# ============================================================
def fetch_data_with_timeout(symbol, start_date, end_date, adjust, timeout):
    """
    ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œåœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­æ‰§è¡Œ akshare è¯·æ±‚ï¼Œå¹¶ä½¿ç”¨ Future/wait å®æ–½è¶…æ—¶ã€‚
    """

    def _fetch():
        # akshare.stock_zh_a_daily é»˜è®¤è¿”å› 9 åˆ—
        return ak.stock_zh_a_daily(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            adjust=adjust
        )

    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(_fetch)
        try:
            done, not_done = wait([future], timeout=timeout)
            if future in done:
                return future.result()
            elif future in not_done:
                future.cancel()
                raise ThreadingTimeoutError(f"è¯·æ±‚è¶…æ—¶ ({timeout}s)")
        except Exception as e:
            raise e


def fetch_data_only(item: dict, start_date: str, end_date: str):
    """
    ğŸ¯ æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šè·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®å¹¶è¿”å› DataFrameã€‚
    """
    # ğŸ”´ æ–°å¢ï¼šå¼•å…¥éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹Ÿäººç±»æ“ä½œï¼Œå‡è½»æœåŠ¡å™¨å‹åŠ›
    time.sleep(random.uniform(0.8, 1))  # éšæœºç­‰å¾… 0.1 åˆ° 0.5 ç§’

    code = item['code']
    symbol = item['symbol']
    name = item['name']
    adjust_type = CONFIG["ADJUST"]

    try:
        # 1. è·å–æ•°æ® (å¸¦è¶…æ—¶ä¿æŠ¤)
        df = fetch_data_with_timeout(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            adjust=adjust_type,
            timeout=CONFIG["REQUEST_TIMEOUT"]
        )

        if df is None or df.empty:
            return None

        # 2. æ•°æ®æ¸…æ´—ä¸å‡†å¤‡ (AkShare è¿”å› 9 åˆ—ï¼Œåªä¿ç•™æ ¸å¿ƒ 7 åˆ—)
        # å­—æ®µé¡ºåº: æ—¥æœŸ, å¼€ç›˜, æ”¶ç›˜, æœ€é«˜, æœ€ä½, æˆäº¤é‡, æˆäº¤é¢, æŒ¯å¹…, æ¢æ‰‹ç‡
        # ç¡®ä¿ DataFrame æœ‰è¶³å¤Ÿçš„åˆ—æ•°
        if df.shape[1] < 7:
            return None

        df = df.iloc[:, :7]
        df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount']

        # è°ƒæ•´åˆ—é¡ºåº
        df = df[['date', 'open', 'high', 'low', 'close', 'volume', 'amount']].copy()

        # æ·»åŠ è”åˆä¸»é”®éœ€è¦çš„å­—æ®µ
        df['code'] = code
        df['adjust'] = adjust_type

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        df['date'] = pd.to_datetime(df['date']).dt.date

        return df

    except ThreadingTimeoutError:
        print(f"[è¶…æ—¶] {name} ({code}) è¯·æ±‚è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return None

    except Exception as e:
        print(f"[å¤±è´¥] è·å– {name} ({code}) å¤±è´¥: {e}")
        return None


# ============================================================
# æ¨¡å— 4ï¼šå¹¶å‘è°ƒåº¦å™¨ (Async Scheduler)
# ============================================================
async def main_scheduler(target_list):
    """
    ä¸»è°ƒåº¦å™¨ï¼šå¹¶å‘æŠ“å–æ‰€æœ‰æ•°æ®ï¼Œå¹¶æ‰§è¡Œæ‰¹é‡å¯¼å…¥ã€‚
    """
    end_date = datetime.now().strftime("%Y%m%d")
    start_date = (datetime.now() - timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
    table_name = CONFIG["MYSQL_TABLE"]
    adjust_type = CONFIG["ADJUST"]
    total_stocks = len(target_list)

    print(f"\n[ä»»åŠ¡å¯åŠ¨] æŠ“å–èŒƒå›´: {start_date} ~ {end_date}")
    print(f"[é…ç½®] ç›®æ ‡: {total_stocks} æ”¯ | çº¿ç¨‹: {CONFIG['MAX_WORKERS']} | å¤æƒ: {adjust_type}")

    loop = asyncio.get_running_loop()
    all_results_df = []

    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:
        tasks = [
            loop.run_in_executor(pool, fetch_data_only, item, start_date, end_date)
            for item in target_list
        ]

        # ä½¿ç”¨ tqdm è¿›è¡Œè¿›åº¦æ¡æ˜¾ç¤º
        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")

        fetched_count = 0

        for coro in pbar:
            df_single = await coro

            if df_single is not None and not df_single.empty:
                all_results_df.append(df_single)
                fetched_count += 1

            pbar.set_postfix({"æˆåŠŸæŠ“å–": fetched_count, "æ€»æ•°": total_stocks})

    if not all_results_df:
        print("\n[è­¦å‘Š] æœªæŠ“å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œå¯¼å…¥ç»ˆæ­¢ã€‚")
        return

    # 1. ğŸŸ¢ æ‰¹é‡æ’å…¥ä¼˜åŒ–æ­¥éª¤ 1: åˆå¹¶æ‰€æœ‰æ•°æ®
    final_df = pd.concat(all_results_df, ignore_index=True)

    print(f"\n[å¯¼å…¥] æ­£åœ¨å‡†å¤‡æ‰¹é‡å¯¼å…¥ {len(final_df)} æ¡æ•°æ®...")

    try:
        engine = get_db_engine()

        # 2. ğŸŸ¢ æ‰¹é‡æ’å…¥ä¼˜åŒ–æ­¥éª¤ 2: å…¨å±€åˆ é™¤ (ä¸€æ¬¡æ€§æ¸…é™¤æ‰€æœ‰æ—§æ•°æ®)
        with engine.connect() as connection:
            delete_sql = f"DELETE FROM {table_name} WHERE adjust='{adjust_type}'"
            connection.execute(text(delete_sql))
            connection.commit()
            print(f"[å¯¼å…¥] å·²åˆ é™¤æ‰€æœ‰æ—§çš„ {adjust_type} å†å²æ•°æ®ã€‚")

        # 3. ğŸŸ¢ æ‰¹é‡æ’å…¥ä¼˜åŒ–æ­¥éª¤ 3: ä¸€æ¬¡æ€§å¯¼å…¥
        # ä½¿ç”¨ chunksize ä¼˜åŒ– Pandas å¯¼å…¥æ€§èƒ½
        final_df.to_sql(
            name=table_name,
            con=engine,
            if_exists='append',
            index=False,
            chunksize=50000
        )
        print(f"[å¯¼å…¥] æ‰¹é‡å¯¼å…¥æˆåŠŸã€‚å…±å¯¼å…¥ {len(final_df)} æ¡è®°å½•ã€‚")

    except Exception as e:
        print(f"âŒ æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}")


# ============================================================
# æ¨¡å— 5ï¼šä¸»å…¥å£
# ============================================================
def main():
    start_time = time.time()

    # 1. è·å–å¹¶è¿‡æ»¤è‚¡ç¥¨åˆ—è¡¨
    try:
        df_base = get_stock_list_manager()
    except Exception as e:
        print(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}")
        return

    target_list = filter_stock_list(df_base)
    if not target_list:
        print("[ç»ˆæ­¢] è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¿‡æ»¤æ¡ä»¶ã€‚")
        return

    # æŠ½æ ·å¤„ç†
    sample_size = CONFIG["SAMPLE_SIZE"]
    if sample_size > 0 and len(target_list) > sample_size:
        print(f"[æŠ½æ ·æ¨¡å¼] éšæœºæŠ½å– {sample_size} æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•...")
        target_list = random.sample(target_list, sample_size)
    else:
        print(f"[å…¨é‡æ¨¡å¼] æ‰«ææ‰€æœ‰ {len(target_list)} æ”¯æœ‰æ•ˆè‚¡ç¥¨...")

    # 2. å¹¶å‘è°ƒåº¦æ‰§è¡Œ
    try:
        asyncio.run(main_scheduler(target_list))
    except Exception as e:
        print(f"\nâŒ ä¸»è°ƒåº¦å™¨è¿è¡Œå‡ºé”™: {e}")

    print("\n" + "=" * 60)
    print(f"âœ… å†å²æ•°æ®å¯¼å…¥ä»»åŠ¡å®Œæˆ | æ€»è€—æ—¶: {time.time() - start_time:.1f}s")
    print("=" * 60)


if __name__ == "__main__":
    main()