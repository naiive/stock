# -*- coding: utf-8 -*-
"""
Module: MarketScanner
Description: å…¨å¸‚åœºå¼‚æ­¥æ‰«æå¼•æ“ã€‚é‡‡ç”¨ "Async IO + ThreadPool Multi-threading" æ··åˆæ¶æ„ã€‚
- Async: è´Ÿè´£éé˜»å¡çš„åˆ†æ‰¹è°ƒåº¦ã€ç­‰å¾…å’Œè¿›åº¦æ¡åˆ·æ–°ã€‚
- ThreadPool: è´Ÿè´£å¯†é›†çš„ CPU è®¡ç®—ï¼ˆç­–ç•¥æŒ‡æ ‡è®¡ç®—ï¼‰å’Œé˜»å¡å¼ IOï¼ˆæ•°æ®åº“è¯»å–ï¼‰ã€‚
"""

import asyncio
import time
import os
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from core.data_handler import DataHandler
from conf.config import SYSTEM_CONFIG, PATH_CONFIG
from strategies.breakout_strategy import run_breakout_strategy


class MarketScanner:
    """
    æ‰«æè°ƒåº¦å¼•æ“ç±»
    """

    def __init__(self):
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨ï¼Œå†…éƒ¨åŒ…å«æ•°æ®åº“å’Œ API å®¢æˆ·ç«¯
        self.handler = DataHandler()
        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡ç­›é€‰å‡ºçš„ä¹°å…¥ä¿¡å·ç»“æœ
        self.matched_list = []

    def _worker(self, symbol):
        """
        å•åªè‚¡ç¥¨çš„è®¡ç®—å•å…ƒ (Worker)
        ã€è¿è¡Œç¯å¢ƒã€‘ï¼šæ­¤æ–¹æ³•è¿è¡Œåœ¨çº¿ç¨‹æ± çš„å·¥ä½œçº¿ç¨‹ä¸­ï¼Œä¸å ç”¨å¼‚æ­¥äº‹ä»¶å¾ªç¯çš„ä¸»çº¿ç¨‹ã€‚

        Args:
            symbol (str): è‚¡ç¥¨ä»£ç ã€‚
        Returns:
            dict or None: å¦‚æœå‘½ä¸­ç­–ç•¥è¿”å›ç»“æœå­—å…¸ï¼Œå¦åˆ™è¿”å› Noneã€‚
        """
        # 1. è·å–ç¼åˆåçš„å®Œæ•´æ•°æ® (MySQL å†å² + å†…å­˜å®æ—¶å¿«ç…§)
        df = self.handler.get_full_data(symbol)

        # 2. è°ƒç”¨æ ¸å¿ƒç­–ç•¥å‡½æ•°è®¡ç®—ä¿¡å·
        # è¿™é‡Œä¼ å…¥ df å’Œ symbolï¼Œç­–ç•¥å†…éƒ¨æ‰§è¡ŒæŒ‡æ ‡è®¡ç®—é€»è¾‘
        return run_breakout_strategy(df, symbol)

    async def run_full_scan(self, symbols=None):
        """
        å…¨å¸‚åœºæ‰«æå¼‚æ­¥ä¸»å…¥å£

        Args:
            symbols (list, optional): æŒ‡å®šæ‰«æçš„è‚¡ç¥¨åˆ—è¡¨ã€‚è‹¥ä¸º None åˆ™æ‰«ææ•°æ®åº“å…¨é‡åå•ã€‚
        """
        # å¦‚æœæœªä¼ å…¥åå•ï¼Œåˆ™é€šè¿‡ handler ä»æ•°æ®åº“åŠ è½½æ¸…æ´—åçš„å…¨é‡ä»£ç 
        if symbols is None:
            symbols = self.handler.get_target_list()

        if not symbols:
            print("âŒ æ‰«æç»ˆæ­¢ï¼šå¾…å¤„ç†åå•ä¸ºç©ºã€‚")
            return

        # 1. ã€æ€§èƒ½å…³é”®ç‚¹ã€‘ï¼šé¢„å–å…¨å¸‚åœºå®æ—¶å¿«ç…§
        # åœ¨è¿›å…¥å¤šçº¿ç¨‹è®¡ç®—å‰ï¼Œä¸€æ¬¡æ€§æ‹‰å–æ‰€æœ‰è‚¡ç¥¨å½“å‰ä»·ï¼Œåç»­çº¿ç¨‹ç›´æ¥ä»å†…å­˜è¯»ï¼Œé¿å… 5000 æ¬¡ç½‘ç»œè¯·æ±‚
        self.handler.prepare_realtime_data()

        # 2. ä»»åŠ¡åˆ†æ‰¹åŒ–å¤„ç† (Batching)
        # ç›®çš„ï¼šé˜²æ­¢ä¸€æ¬¡æ€§æäº¤å‡ åƒä¸ªçº¿ç¨‹å¯¼è‡´å†…å­˜æº¢å‡ºï¼ŒåŒæ—¶ç»™åº•å±‚æ¥å£ç•™å‡ºå“åº”é—´éš™
        batch_size = SYSTEM_CONFIG.get("BATCH_SIZE", 500)
        batches = list(self.handler.chunk_symbols(symbols, batch_size))
        print(f"âœ… å¼‚æ­¥æ‰«æå°±ç»ªï¼Œå…± {len(symbols)} åªï¼Œåˆ† {len(batches)} æ‰¹ã€‚")

        # è¯»å–å¹¶å‘å‚æ•°
        max_workers = SYSTEM_CONFIG.get("MAX_WORKERS", 10)  # å¹¶è¡Œçº¿ç¨‹æ•°
        interval = SYSTEM_CONFIG.get("BATCH_INTERVAL_SEC", 1)  # æ‰¹æ¬¡é—´ä¼‘æ¯æ—¶é—´

        # è·å–å½“å‰çš„å¼‚æ­¥äº‹ä»¶å¾ªç¯å¥æŸ„ï¼Œç”¨äºåœ¨çº¿ç¨‹æ± ä¸­è·‘ä»»åŠ¡
        loop = asyncio.get_running_loop()

        for i, batch in enumerate(batches):
            print(f"\nğŸ“¦ æ‰¹æ¬¡ {i + 1}/{len(batches)} (è§„æ¨¡: {len(batch)})")
            batch_matched = []

            # 3. çº¿ç¨‹æ± ä¸Šä¸‹æ–‡ç®¡ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ä½¿ç”¨ loop.run_in_executor å°†åŒæ­¥çš„ _worker åŒ…è£…æˆå¼‚æ­¥ä»»åŠ¡ (Task)
                # è¿™æ ·å¯ä»¥åˆ©ç”¨ await ç­‰å¾…ç»“æœï¼Œè€Œä¸ä¼šé˜»å¡å¼‚æ­¥ä¸»å¾ªç¯
                tasks = [
                    loop.run_in_executor(executor, self._worker, s)
                    for s in batch
                ]

                # ä½¿ç”¨ tqdm å®æ—¶å±•ç¤ºå½“å‰æ‰¹æ¬¡çš„è¿›åº¦
                # asyncio.as_completed(tasks) ä¿è¯è°å…ˆç®—å®Œï¼Œè¿›åº¦æ¡å°±å…ˆèµ°ä¸€æ­¥
                pbar = tqdm(asyncio.as_completed(tasks),
                            total=len(tasks),
                            desc=f"è¿›åº¦{i + 1}",
                            ncols=80)
                # åˆå§‹åŒ–æ˜¾ç¤ºï¼Œé¿å…â€œè¿›åº¦æ¡â€æŠ–åŠ¨
                pbar.set_postfix({"å‘½ä¸­": len(self.matched_list)})

                for task in pbar:
                    res = await task  # å¼‚æ­¥ç­‰å¾…çº¿ç¨‹è¿”å›ç»“æœ
                    if res:
                        batch_matched.append(res)
                        # å®æ—¶æ›´æ–°è¿›åº¦æ¡å³ä¾§æ˜¾ç¤ºçš„ç´¯è®¡å‘½ä¸­æ•°
                        pbar.set_postfix({"å‘½ä¸­": len(batch_matched) + len(self.matched_list)})

            # æ±‡æ€»å½“å‰æ‰¹æ¬¡çš„ç­›é€‰ç»“æœ
            self.matched_list.extend(batch_matched)

            # 4. ã€éé˜»å¡ä¼‘æ¯ã€‘ï¼šé˜²æ­¢ CPU æŒç»­æ»¡è½½ï¼Œç»™ç³»ç»Ÿâ€œå–˜æ¯â€æ—¶é—´
            # ä½¿ç”¨ await asyncio.sleep è€Œä¸æ˜¯ time.sleepï¼Œç¡®ä¿å¼‚æ­¥å¾ªç¯ä¸è¢«æŒ‚èµ·
            if i < len(batches) - 1 and interval > 0:
                await asyncio.sleep(interval)

        # 5. å¯¼å‡ºç»“æœï¼ŒåŒ…æ‹¬æ‹¼æ¥å…¶ä»–è¯¦ç»†ï¼ŒCSVæ–‡ä»¶å¯¼å‡ºï¼ŒEmailå‘é€ï¼ŒtelegramèŠå¤©æœºå™¨äººç­‰
        self.export_results()

    def _enrich_results(self, df_res):
        """
        [ç‹¬ç«‹åŠŸèƒ½å‡½æ•°] ç»“æœå¢å¼ºï¼šä¸ºæ‰«æç»“æœæ³¨å…¥å®æ—¶è´¢åŠ¡æ˜ å°„ä¿¡æ¯
        Args:
            df_res (pd.DataFrame): åŸå§‹æ‰«æå‘½ä¸­çš„ DataFrame
        Returns:
            pd.DataFrame: å¢å¼ºåçš„ DataFrame
        """
        if df_res.empty:
            return df_res

        try:
            print("ğŸ” [ç³»ç»Ÿ] æ­£åœ¨æ‰§è¡Œæ•°æ®å¢å¼ºï¼šæŠ“å–æœ€æ–°æ¢æ‰‹ç‡ã€å¸‚å€¼ç­‰ä¿¡æ¯...")
            # è°ƒç”¨ API è·å–å…¨å¸‚åœºå¿«ç…§
            df_live = self.handler.api_client.fetch_realtime_snapshot()

            if not df_live.empty:
                """
                # 1. å®šä¹‰éœ€è¦æ˜ å°„çš„å­—æ®µ
                'ä»£ç ': 'code',
                'åç§°': 'name',
                'æ¢æ‰‹ç‡': 'turnover',
                'å¸‚ç›ˆç‡-åŠ¨æ€': 'pe',
                'æ€»å¸‚å€¼': 'mcap',
                'æµé€šå¸‚å€¼': 'ffmc',
                'å¹´æ¶¨å¹…': 'ytd'
                """
                info_cols = ['code', 'name', 'turnover', 'pe', 'mcap', 'ffmc', 'ytd']

                df_info = df_live[[c for c in info_cols if c in df_live.columns]]

                # 2. æ‰§è¡Œåˆå¹¶
                df_enriched = pd.merge(df_res, df_info, left_on='ä»£ç ', right_on='code', how='left')

                # 3. æ¸…ç†ä¸é‡æ’
                if 'code' in df_enriched.columns:
                    df_enriched.drop(columns=['code'], inplace=True)

                # å®šä¹‰ç¾åŒ–åçš„åˆ—åº
                head_cols = ['turnover', 'pe', 'mcap', 'ffmc', 'ytd']
                head_cols = [c for c in head_cols if c in df_enriched.columns]
                others = [c for c in df_enriched.columns if c not in head_cols]
                # å°†åç§°æ”¾åˆ°ç¬¬äºŒä¸ªä½ç½®
                others = [c for c in others if c != 'name']
                others.insert(1, 'name')

                sorted_df = df_enriched[others + head_cols]

                # é‡å‘½å
                export_map ={
                    'name': 'åç§°',
                    'turnover':'æ¢æ‰‹ç‡(%)',
                    'pe':'å¸‚ç›ˆç‡(åŠ¨)',
                    'mcap':'æ€»å¸‚å€¼(äº¿)',
                    'ffmc':'æµé€šå¸‚å€¼(äº¿)',
                    'ytd':'å¹´æ¶¨å¹…(%)'
                }

                return sorted_df.rename(columns=export_map)

        except Exception as e:
            print(f"âš ï¸ [è­¦å‘Š] æ•°æ®å¢å¼ºè¿‡ç¨‹å‡ºé”™: {e}")

        return df_res  # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®ï¼Œä¿è¯ç¨‹åºä¸ä¸­æ–­

    def export_results(self):
        """
        [å¯¼å‡ºåŠŸèƒ½] è´Ÿè´£ç»“æœçš„æœ€ç»ˆè½ç›˜
        æœ‰å‚æ•°æ§åˆ¶æ˜¯å¦å¯¼å‡ºï¼šåç§°ã€å¸‚å€¼å¤§å°ã€å¸‚ç›ˆç‡ç­‰ä¿¡æ¯
        """
        if not self.matched_list:
            print("\nğŸ æ‰«æå®Œæˆï¼Œæœªå‘ç°åŒ¹é…ä¿¡å·ã€‚")
            return

        # 1. è½¬æ¢ä¸ºåˆå§‹ DataFrame
        final_df = pd.DataFrame(self.matched_list)

        # 2. è°ƒç”¨ç‹¬ç«‹å¢å¼ºå‡½æ•° (æ ¹æ®å¼€å…³å‚æ•°)
        if SYSTEM_CONFIG.get("ENABLE_RESULT_ENRICHMENT", False):
            final_df = self._enrich_results(final_df)
        else:
            print("â„¹ï¸ [ç³»ç»Ÿ] è·³è¿‡æ•°æ®å¢å¼ºï¼Œç›´æ¥å¯¼å‡ºåŸå§‹ç»“æœã€‚")

        # 3. æ‰§è¡Œæ–‡ä»¶å†™å…¥
        self._write_to_csv(final_df)

    @staticmethod
    def _write_to_csv(df):
        """
        å†…éƒ¨æ–¹æ³•ï¼šè´Ÿè´£ç‰©ç†å†™å…¥ CSV æ–‡ä»¶
        """
        date_str = time.strftime('%Y%m%d')
        save_dir = os.path.join(PATH_CONFIG["OUTPUT_FOLDER_BASE"], date_str)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        file_path = os.path.join(save_dir, f"scan_res_{time.strftime('%H%M%S')}.csv")
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶è·¯å¾„: {file_path}")