# -*- coding: utf-8 -*-
"""
Module: MarketScanner
Description: å…¨å¸‚åœºå¼‚æ­¥æ‰«æå¼•æ“ã€‚é‡‡ç”¨ "Async IO + ThreadPool Multi-threading" æ··åˆæ¶æ„ã€‚
- Async: è´Ÿè´£éé˜»å¡çš„åˆ†æ‰¹è°ƒåº¦ã€ç­‰å¾…å’Œè¿›åº¦æ¡åˆ·æ–°ã€‚
- ThreadPool: è´Ÿè´£å¯†é›†çš„ CPU è®¡ç®—ï¼ˆç­–ç•¥æŒ‡æ ‡è®¡ç®—ï¼‰å’Œé˜»å¡å¼ IOï¼ˆæ•°æ®åº“è¯»å–ï¼‰ã€‚
"""

import asyncio
import time
import os
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from core.data_handler import DataHandler
from conf.config import SYSTEM_CONFIG, PATH_CONFIG
from strategies.breakout_strategy import run_breakout_strategy


class MarketScanner:
    """
    æ‰«æè°ƒåº¦å¼•æ“ç±»
    """

    def __init__(self):
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨ï¼Œå†…éƒ¨åŒ…å«æ•°æ®åº“å’Œ API å®¢æˆ·ç«¯
        self.handler = DataHandler()
        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡ç­›é€‰å‡ºçš„ä¹°å…¥ä¿¡å·ç»“æœ
        self.matched_list = []

    def _worker(self, symbol):
        """
        å•åªè‚¡ç¥¨çš„è®¡ç®—å•å…ƒ (Worker)
        ã€è¿è¡Œç¯å¢ƒã€‘ï¼šæ­¤æ–¹æ³•è¿è¡Œåœ¨çº¿ç¨‹æ± çš„å·¥ä½œçº¿ç¨‹ä¸­ï¼Œä¸å ç”¨å¼‚æ­¥äº‹ä»¶å¾ªç¯çš„ä¸»çº¿ç¨‹ã€‚

        Args:
            symbol (str): è‚¡ç¥¨ä»£ç ã€‚
        Returns:
            dict or None: å¦‚æœå‘½ä¸­ç­–ç•¥è¿”å›ç»“æœå­—å…¸ï¼Œå¦åˆ™è¿”å› Noneã€‚
        """
        # 1. è·å–ç¼åˆåçš„å®Œæ•´æ•°æ® (MySQL å†å² + å†…å­˜å®æ—¶å¿«ç…§)
        df = self.handler.get_full_data(symbol)

        # 2. è°ƒç”¨æ ¸å¿ƒç­–ç•¥å‡½æ•°è®¡ç®—ä¿¡å·
        # è¿™é‡Œä¼ å…¥ df å’Œ symbolï¼Œç­–ç•¥å†…éƒ¨æ‰§è¡ŒæŒ‡æ ‡è®¡ç®—é€»è¾‘
        return run_breakout_strategy(df, symbol)

    async def run_full_scan(self, symbols=None):
        """
        å…¨å¸‚åœºæ‰«æå¼‚æ­¥ä¸»å…¥å£

        Args:
            symbols (list, optional): æŒ‡å®šæ‰«æçš„è‚¡ç¥¨åˆ—è¡¨ã€‚è‹¥ä¸º None åˆ™æ‰«ææ•°æ®åº“å…¨é‡åå•ã€‚
        """
        # å¦‚æœæœªä¼ å…¥åå•ï¼Œåˆ™é€šè¿‡ handler ä»æ•°æ®åº“åŠ è½½æ¸…æ´—åçš„å…¨é‡ä»£ç 
        if symbols is None:
            symbols = self.handler.get_target_list()

        if not symbols:
            print("âŒ æ‰«æç»ˆæ­¢ï¼šå¾…å¤„ç†åå•ä¸ºç©ºã€‚")
            return

        # 1. ã€æ€§èƒ½å…³é”®ç‚¹ã€‘ï¼šé¢„å–å…¨å¸‚åœºå®æ—¶å¿«ç…§
        # åœ¨è¿›å…¥å¤šçº¿ç¨‹è®¡ç®—å‰ï¼Œä¸€æ¬¡æ€§æ‹‰å–æ‰€æœ‰è‚¡ç¥¨å½“å‰ä»·ï¼Œåç»­çº¿ç¨‹ç›´æ¥ä»å†…å­˜è¯»ï¼Œé¿å… 5000 æ¬¡ç½‘ç»œè¯·æ±‚
        self.handler.prepare_realtime_data()

        # 2. ä»»åŠ¡åˆ†æ‰¹åŒ–å¤„ç† (Batching)
        # ç›®çš„ï¼šé˜²æ­¢ä¸€æ¬¡æ€§æäº¤å‡ åƒä¸ªçº¿ç¨‹å¯¼è‡´å†…å­˜æº¢å‡ºï¼ŒåŒæ—¶ç»™åº•å±‚æ¥å£ç•™å‡ºå“åº”é—´éš™
        batch_size = SYSTEM_CONFIG.get("BATCH_SIZE", 500)
        batches = list(self.handler.chunk_symbols(symbols, batch_size))
        print(f"âœ… å¼‚æ­¥æ‰«æå°±ç»ªï¼Œå…± {len(symbols)} åªï¼Œåˆ† {len(batches)} æ‰¹ã€‚")

        # è¯»å–å¹¶å‘å‚æ•°
        max_workers = SYSTEM_CONFIG.get("MAX_WORKERS", 10)  # å¹¶è¡Œçº¿ç¨‹æ•°
        interval = SYSTEM_CONFIG.get("BATCH_INTERVAL_SEC", 1)  # æ‰¹æ¬¡é—´ä¼‘æ¯æ—¶é—´

        # è·å–å½“å‰çš„å¼‚æ­¥äº‹ä»¶å¾ªç¯å¥æŸ„ï¼Œç”¨äºåœ¨çº¿ç¨‹æ± ä¸­è·‘ä»»åŠ¡
        loop = asyncio.get_running_loop()

        for i, batch in enumerate(batches):
            print(f"\nğŸ“¦ æ‰¹æ¬¡ {i + 1}/{len(batches)} (è§„æ¨¡: {len(batch)})")
            batch_matched = []

            # 3. çº¿ç¨‹æ± ä¸Šä¸‹æ–‡ç®¡ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ä½¿ç”¨ loop.run_in_executor å°†åŒæ­¥çš„ _worker åŒ…è£…æˆå¼‚æ­¥ä»»åŠ¡ (Task)
                # è¿™æ ·å¯ä»¥åˆ©ç”¨ await ç­‰å¾…ç»“æœï¼Œè€Œä¸ä¼šé˜»å¡å¼‚æ­¥ä¸»å¾ªç¯
                tasks = [
                    loop.run_in_executor(executor, self._worker, s)
                    for s in batch
                ]

                # ä½¿ç”¨ tqdm å®æ—¶å±•ç¤ºå½“å‰æ‰¹æ¬¡çš„è¿›åº¦
                # asyncio.as_completed(tasks) ä¿è¯è°å…ˆç®—å®Œï¼Œè¿›åº¦æ¡å°±å…ˆèµ°ä¸€æ­¥
                pbar = tqdm(asyncio.as_completed(tasks),
                            total=len(tasks),
                            desc=f"è¿›åº¦{i + 1}",
                            ncols=80)

                for task in pbar:
                    res = await task  # å¼‚æ­¥ç­‰å¾…çº¿ç¨‹è¿”å›ç»“æœ
                    if res:
                        batch_matched.append(res)
                        # å®æ—¶æ›´æ–°è¿›åº¦æ¡å³ä¾§æ˜¾ç¤ºçš„ç´¯è®¡å‘½ä¸­æ•°
                        pbar.set_postfix({"å‘½ä¸­": len(batch_matched) + len(self.matched_list)})

            # æ±‡æ€»å½“å‰æ‰¹æ¬¡çš„ç­›é€‰ç»“æœ
            self.matched_list.extend(batch_matched)

            # 4. ã€éé˜»å¡ä¼‘æ¯ã€‘ï¼šé˜²æ­¢ CPU æŒç»­æ»¡è½½ï¼Œç»™ç³»ç»Ÿâ€œå–˜æ¯â€æ—¶é—´
            # ä½¿ç”¨ await asyncio.sleep è€Œä¸æ˜¯ time.sleepï¼Œç¡®ä¿å¼‚æ­¥å¾ªç¯ä¸è¢«æŒ‚èµ·
            if i < len(batches) - 1 and interval > 0:
                await asyncio.sleep(interval)

        # 5. æ‰«æç»“æŸï¼Œå¯¼å‡º CSV æŠ¥è¡¨
        self.export_results()

    def export_results(self):
        """
        ç»“æœæŒä¹…åŒ–ï¼šå°†å‘½ä¸­çš„ä¿¡å·å¯¼å‡ºä¸º CSV æ–‡ä»¶ã€‚
        """
        if not self.matched_list:
            print("\nğŸ æ‰«æå®Œæˆï¼Œæœªå‘ç°åŒ¹é…ä¿¡å·ã€‚")
            return

        # 1. æ•´ç†æ•°æ®ä¸º DataFrame
        df_res = pd.DataFrame(self.matched_list)

        # 2. æ„å»ºä¿å­˜è·¯å¾„ï¼ˆæŒ‰æ—¥æœŸåˆ†æ–‡ä»¶å¤¹å­˜å‚¨ï¼‰
        date_str = time.strftime('%Y%m%d')
        save_dir = os.path.join(PATH_CONFIG["OUTPUT_FOLDER_BASE"], date_str)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # 3. ä»¥æ—¶åˆ†ç§’å‘½åæ–‡ä»¶ï¼Œé˜²æ­¢å¤šæ¬¡æ‰«æè¦†ç›–ç»“æœ
        file_path = os.path.join(save_dir, f"scan_res_{time.strftime('%H%M%S')}.csv")

        # 4. å†™å…¥æ–‡ä»¶ï¼Œutf-8-sig ç¼–ç æ”¯æŒ Excel ç›´æ¥æ‰“å¼€ä¸”ä¸ä¹±ç 
        df_res.to_csv(file_path, index=False, encoding='utf-8-sig')

        print(f"\nğŸ‰ æ‰«æç»“æŸï¼")
        print(f"ğŸ“ˆ ç´¯è®¡å‘½ä¸­æ•°é‡: {len(self.matched_list)}")
        print(f"ğŸ’¾ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {file_path}")