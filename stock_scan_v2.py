#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡çªç ´æ‰«æç³»ç»Ÿï¼ˆPivot + SQZMOM + MA200ï¼‰
ç‰ˆæœ¬ï¼šv1.4 (ä¸“ä¸šæ—¥å¿—ç®¡ç†ç‰ˆ)

ã€æ ¸å¿ƒä¿®æ”¹ã€‘
1. åºŸå¼ƒé«˜å¤±è´¥ç‡çš„åˆ†é’Ÿæ•°æ®æ¥å£ã€‚
2. å®æ—¶æ•°æ®è·å–æ”¹ä¸ºä¸²è¡Œè°ƒç”¨è…¾è®¯å®æ—¶å¿«ç…§æ¥å£ (stock_zh_a_spot)ï¼Œå¹¶å¢åŠ é‡è¯•å’Œå»¶è¿Ÿã€‚
3. å®æ—¶å¿«ç…§æ•°æ® (df_spot) ä½œä¸ºå‚æ•°ä¼ é€’ç»™å¹¶å‘æ‰§è¡Œå™¨ï¼Œå®ç° O(1) æŸ¥æ‰¾æœ€æ–°ä»·ã€‚
4. ä¿®å¤ append_today_realtime_snapshot å‡½æ•°ä¸­çš„åˆ—åå…¼å®¹æ€§é—®é¢˜ã€‚
5. å¢å¼º LogRedirector ç±»ï¼šå®ç°æ—¥å¿—æ–‡ä»¶æŒ‰å¤§å° (20MB) è‡ªåŠ¨è½®æ¢ã€‚
6. æ—¥å¿—å­˜å‚¨è·¯å¾„ï¼šDay_Stocks/logs/YYYYMMDD/YYYYMMDD_HHMMSS.log
============================================================
"""
import os
import sys
import json
import time
import random
import math
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, wait, TimeoutError as ThreadingTimeoutError

import pandas as pd
import numpy as np
import akshare as ak
import asyncio
from tqdm import tqdm

try:
    from stock_query import stock_zh_a_daily_mysql
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ stock_zh_a_daily_mysqlã€‚è¯·ç¡®ä¿æ‚¨çš„ stock_query.py æ–‡ä»¶å­˜åœ¨ã€‚")
    def stock_zh_a_daily_mysql(*args, **kwargs):
        raise NameError("stock_zh_a_daily_mysql å°šæœªå®šä¹‰æˆ–å¯¼å…¥å¤±è´¥ã€‚")

# ============================================================
# æ¨¡å— 1ï¼šé…ç½® (Configuration)
# ============================================================
CONFIG = {
    # --- ğŸ†•  æ—¶é—´èŒƒå›´ ---
    "DAYS": 365,  # æ‰«æå›æº¯å¤©æ•° (ç”¨äºè®¡ç®— MA200)

    # --- ğŸ†•  è¿‡æ»¤æ¡ä»¶ ---
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ã€689ï¼‰
    "EXCLUDE_BJ": True,   # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ã€92ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",      # å¤æƒæ–¹å¼

    # --- ğŸ†• SQZç­–ç•¥å‚æ•° ---
    "SQZ": {
        "length": 20,
        "mult": 2.0,
        "lengthKC": 20,
        "multKC": 1.5,
        "useTrueRange": True
    },

    # --- ğŸ†• PIVOTç­–ç•¥å‚æ•° ---
    "PIVOT_LEFT": 15,   # å·¦ä¾§ K çº¿æ•°é‡
    "PIVOT_RIGHT": 15,  # å³ä¾§ K çº¿æ•°é‡

    # --- ğŸ†• æ–‡ä»¶è·¯å¾„/åç§° ---
    "CACHE_FILE": "stock_list_cache.json",
    "EXPORT_ENCODING": "utf-8-sig",       # CSVæ–‡ä»¶å¯¼å‡ºç¼–ç 
    "OUTPUT_FILENAME_BASE": "Buy_Stocks", # è¾“å‡ºæ–‡ä»¶å‰ç¼€
    "OUTPUT_FOLDER_BASE": "Day_Stocks",   # LogRedirector ä¹Ÿä½¿ç”¨æ­¤æ–‡ä»¶å¤¹

    # --- ğŸ†• å¹¶å‘ ---
    "MAX_WORKERS": 10,      # é™ä½çº¿ç¨‹æ•°åˆ° 10
    "REQUEST_TIMEOUT": 20,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 20s

    # --- ğŸ†• æ•°æ®æºæ§åˆ¶ ---
    # True:  ä½¿ç”¨æœ¬åœ° stock_zh_a_daily_mysql å‡½æ•°
    # False: ä½¿ç”¨ ak.stock_zh_a_daily (AkShare)
    "USE_LOCAL_MYSQL": True,

    # --- ğŸ†• å®æ—¶æ•°æ®å¼€å…³ ---
    # True:  ä½¿ç”¨è…¾è®¯å®æ—¶è‚¡ç¥¨å…¨é‡æ¥å£ (fetch_realtime_snapshot)
    # False: ä¸ä½¿ç”¨ï¼Œè·³è¿‡å®æ—¶æ•°æ®è·å–ï¼ˆç”¨äºç¦»çº¿å›æµ‹æˆ–éäº¤æ˜“æ—¥ï¼‰
    "USE_REAL_TIME_DATA": False,

    # --- ğŸ†• æ˜¯å¦å…¨é‡/åˆ†æ‰¹æ§åˆ¶ ---
    "SAMPLE_SIZE": 0,          # 0 æˆ– None è¡¨ç¤ºå…¨é‡
    "BATCH_SIZE": 200,         # SAMPLE_SIZE å…¨é‡æ‰å¼€å¯åˆ†æ‰¹åŠŸèƒ½ï¼Œæ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
    "BATCH_INTERVAL_SEC": 8,   # æ‰¹æ¬¡é—´éš”ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰

    # --- ğŸ†• æ‰‹åŠ¨è¾“å…¥ ---
    # ç¤ºä¾‹: ["600519", "000001", "300751"]ã€‚å¦‚æœéç©ºï¼Œåˆ™è·³è¿‡å…¨é‡æ‰«æã€‚
    "MANUAL_STOCK_LIST": [
        # "000807",
        # "000708",
        # "002830",
        # "301517",
        # "000408",
        # "600879",
        # "600595",
        # "601168",
        # "002595",
        # "301028",
        # "002429"
    ]
}

# ============================================================
# æ¨¡å— 0ï¼šæ—¥å¿—é‡å®šå‘ç±»
# ============================================================
class LogRedirector:
    """
    å°† sys.stdout çš„è¾“å‡ºåŒæ—¶é‡å®šå‘åˆ°ç»ˆç«¯å’Œæ—¥å¿—æ–‡ä»¶ï¼Œå¹¶å®ç°æŒ‰å¤§å°è½®æ¢ï¼ˆä¿ç•™æ—§æ–‡ä»¶ï¼‰ã€‚
    """
    # 20MB è½®æ¢é™åˆ¶
    MAX_BYTES = 20 * 1024 * 1024

    def __init__(self, folder="Day_Stocks"):
        # æ—¥å¿—è·¯å¾„: Day_Stocks/logs/YYYYMMDD/
        self.today_str = datetime.now().strftime('%Y%m%d')
        self.log_dir = os.path.join(folder, "logs", self.today_str)
        os.makedirs(self.log_dir, exist_ok=True)

        self.terminal = sys.stdout
        self.log_file = None
        self.current_log_path = None
        self.is_active = False

    def _get_new_log_path(self):
        """ç”Ÿæˆæ–°çš„æ—¥å¿—æ–‡ä»¶åï¼šYYYYMMDD_HHMMSS.log"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"{timestamp}.log"
        return os.path.join(self.log_dir, log_filename)

    def _check_and_rotate(self):
        """æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœè¶…è¿‡é™åˆ¶åˆ™å…³é—­æ—§æ–‡ä»¶å¹¶åˆ›å»ºæ–°æ–‡ä»¶ã€‚"""
        if self.log_file is None:
            # é¦–æ¬¡åˆ›å»º
            self.current_log_path = self._get_new_log_path()
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')
            return True

        # æ£€æŸ¥å¤§å°
        # æ³¨æ„ï¼šæ­¤å¤„ä½¿ç”¨ os.path.getsize æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œè¿™æ˜¯è½®æ¢çš„å…³é”®
        if os.path.getsize(self.current_log_path) > self.MAX_BYTES:
            self.log_file.write(f"\n[è½®æ¢] æ—¥å¿—è¾¾åˆ° {self.MAX_BYTES / 1024 / 1024:.0f}MB é™åˆ¶ï¼Œæ­£åœ¨åˆ‡æ¢æ–‡ä»¶...\n")
            self.log_file.close()  # <--- å…³é—­æ—§æ–‡ä»¶ï¼Œä¿ç•™åœ¨ç£ç›˜ä¸Š

            # åˆ›å»ºæ–°æ–‡ä»¶
            self.current_log_path = self._get_new_log_path()  # <--- ç”Ÿæˆå…¨æ–°çš„ã€ä¸é‡å¤çš„æ–‡ä»¶å
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')  # <--- æ‰“å¼€æ–°æ–‡ä»¶ç»§ç»­å†™å…¥
            return True

        return False

    def __enter__(self):
        try:
            self._check_and_rotate()  # é¦–æ¬¡åˆ›å»ºæ—¥å¿—æ–‡ä»¶
            sys.stdout = self
            self.is_active = True
            self.write(
                f"\n{'=' * 70}\n[ä¼šè¯å¼€å§‹] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n[æ—¥å¿—æ–‡ä»¶] {self.current_log_path}\n{'=' * 70}\n")
            return self
        except Exception as e:
            print(f"[é”™è¯¯] æ—¥å¿—ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}", file=self.terminal)
            sys.stdout = self.terminal
            return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.is_active:
            self.write(f"\n{'=' * 70}\n[ä¼šè¯ç»“æŸ] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'=' * 70}\n")
            sys.stdout = self.terminal
            if self.log_file:
                self.log_file.close()
            print(f"æ—¥å¿—æ–‡ä»¶å·²ä¿å­˜è‡³: {self.current_log_path}")

    def write(self, message):
        self.terminal.write(message)
        self.terminal.flush()

        if self.log_file:
            self._check_and_rotate()  # å†™å…¥å‰æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢
            if not message.startswith('\r'):
                self.log_file.write(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
            else:
                self.log_file.write(message)
            self.log_file.flush()

    def flush(self):
        self.terminal.flush()
        if self.log_file:
            self.log_file.flush()


# ============================================================
# å·¥å…·ï¼šé‡è¯•è£…é¥°å™¨ (Retry Decorator)
# ============================================================
def retry(max_retries=10, delay=15):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == max_retries - 1:
                        raise
                    print(f"[è­¦å‘Š] {func.__name__} å¤±è´¥ ({type(e).__name__})ï¼Œ{delay}s åé‡è¯•...")
                    time.sleep(delay)

        return wrapper

    return decorator


# ============================================================
# æ¨¡å— 2ï¼šè·å–/ç¼“å­˜ å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
# ============================================================
@retry(max_retries=2, delay=1)
def fetch_stock_list_safe():
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_info_a_code_name æ¥å£")
            return df[["code", "name"]]
    except Exception as e:
        print(f"[è­¦å‘Š] è½»é‡æ¥å£å¤±è´¥ ({e})ï¼Œå°è¯•å¤‡ç”¨æ¥å£...")
    try:
        df = ak.stock_zh_a_spot_em()
        print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_zh_a_spot_em æ¥å£")
        if 'ä»£ç ' in df.columns:
            df = df.rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
        return df[["code", "name"]]
    except Exception as e:
        raise Exception(f"æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨æ¥å£å‡ä¸å¯ç”¨: {e}")


def get_stock_list_manager():
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                print(f"[ç³»ç»Ÿ] åŠ è½½æœ¬æ—¥ç¼“å­˜ï¼Œå…± {len(cache['data'])} æ”¯è‚¡ç¥¨")
                return pd.DataFrame(cache["data"])
        except Exception:
            pass

    df = fetch_stock_list_safe()
    if not df.empty:
        with open(cache_file, "w", encoding="utf-8") as f:
            data = {
                "time": today_str,
                "data": df.to_dict(orient="records")
            }
            json.dump(data, f, ensure_ascii=False, indent=2)

    return df


def filter_stock_list(df):
    if df is None or df.empty:
        return []
    df["code"] = df["code"].astype(str).str.zfill(6)
    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]:
        mask |= df["code"].str.startswith("300")
    if CONFIG["EXCLUDE_KCB"]:
        mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]:
        mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns:
        mask |= df["name"].str.contains("ST|é€€", na=False)
    return df[~mask]["code"].tolist()


# ============================================================
# 3ï¼šæŠ€æœ¯æŒ‡æ ‡ï¼ˆSQZMOM / linreg / true_range / color / sqz_idï¼‰
# ============================================================
def tv_linreg(y, length):
    if pd.isna(y).any(): return np.nan
    x = np.arange(length)
    y = y.values
    if len(y) < 2: return np.nan
    A = np.vstack([x, np.ones(length)]).T
    try:
        m, b = np.linalg.lstsq(A, y, rcond=None)[0]
    except np.linalg.LinAlgError:
        return np.nan
    return m * (length - 1) + b


def true_range(df):
    prev_close = df['close'].shift(1)
    tr1 = df['high'] - df['low']
    tr2 = (df['high'] - prev_close).abs()
    tr3 = (df['low'] - prev_close).abs()
    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)


def get_color_cn(v, v_prev):
    if pd.isna(v) or pd.isna(v_prev): return None
    if v == 0: return "ä¸­æ€§"
    if v > 0: return "å¼ºå¤š" if v > v_prev else "å¼±å¤š"
    return "å¼ºç©º" if v < v_prev else "å¼±ç©º"


def add_squeeze_counter(df):
    counter = 0
    current_state = None
    sqz_id_list = []
    for status in df["sqz_status"]:
        if status in ["æŒ¤å‹", "é‡Šæ”¾"]:
            if status == current_state:
                counter += 1
            else:
                current_state = status
                counter = 1
            sqz_id_list.append(counter)
        else:
            current_state = None
            counter = 0
            sqz_id_list.append(0)
    df["sqz_id"] = sqz_id_list
    return df


def squeeze_momentum(df, length=None, mult=None, lengthKC=None, multKC=None, useTrueRange=True):
    if length is None: length = CONFIG["SQZ"]["length"]
    if mult is None: mult = CONFIG["SQZ"]["mult"]
    if lengthKC is None: lengthKC = CONFIG["SQZ"]["lengthKC"]
    if multKC is None: multKC = CONFIG["SQZ"]["multKC"]

    close, high, low = df['close'], df['high'], df['low']

    basis = close.rolling(length).mean()
    dev = multKC * close.rolling(length).std(ddof=0)
    upperBB, lowerBB = basis + dev, basis - dev
    bb_width = (upperBB - lowerBB) / basis.replace(0, np.nan)

    ma = close.rolling(lengthKC).mean()
    r = true_range(df) if useTrueRange else (high - low)
    rangema = r.rolling(lengthKC).mean()
    upperKC, lowerKC = ma + rangema * multKC, ma - rangema * multKC

    sqzOn = (lowerBB > lowerKC) & (upperBB < upperKC)
    sqzOff = (lowerBB < lowerKC) & (upperBB > upperKC)
    df["sqz_status"] = np.select([sqzOn, sqzOff], ["æŒ¤å‹", "é‡Šæ”¾"], default="æ— ")

    highest_h = high.rolling(lengthKC).max()
    lowest_l = low.rolling(lengthKC).min()
    avg_hl = (highest_h + lowest_l) / 2
    sma_close = close.rolling(lengthKC).mean()
    mid = (avg_hl + sma_close) / 2
    source_mid = close - mid

    val = source_mid.rolling(lengthKC).apply(lambda x: tv_linreg(pd.Series(x), lengthKC), raw=False)
    df["val"] = val
    df["val_prev"] = val.shift(1)
    df["val_color"] = df.apply(lambda r: get_color_cn(r["val"], r["val_prev"]), axis=1)

    df["BB_pct"] = bb_width
    df = add_squeeze_counter(df)
    return df


# ============================================================
# æ¨¡å— 4ï¼šPivot é«˜ç‚¹ï¼ˆå‰é˜»åŠ›ä½ï¼‰
# ============================================================
def calculate_pivot_high_vectorized(df, left=None, right=None):
    if left is None: left = CONFIG["PIVOT_LEFT"]
    if right is None: right = CONFIG["PIVOT_RIGHT"]

    highs = df['high'].values
    n = len(highs)
    pivots = np.full(n, np.nan)

    for i in range(left, n - right):
        left_max = np.max(highs[i - left:i])
        right_max = np.max(highs[i + 1:i + 1 + right])
        if highs[i] > left_max and highs[i] > right_max:
            pivots[i] = highs[i]

    return pd.Series(pivots, index=df.index).ffill()


# ============================================================
# æ¨¡å— 4ï¼šä»Šæ—¥å®æ—¶Kè¡¥å……
# ============================================================

@retry(max_retries=10, delay=15)
def fetch_realtime_snapshot():
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§ (è…¾è®¯æ¥å£)...")
    try:
        df = ak.stock_zh_a_spot()
    except Exception:
        raise
    df = df.rename(columns={'ä»£ç ': 'code', 'æœ€æ–°ä»·': 'close', 'æˆäº¤é‡': 'volume'})
    df = df[['code', 'close', 'volume']]
    df['code'] = df['code'].astype(str).str.zfill(6)
    print(f"[ç³»ç»Ÿ] æˆåŠŸè·å– {len(df)} æ¡å®æ—¶å¿«ç…§æ•°æ®ã€‚")
    return df


def append_today_realtime_snapshot(code: str, df_daily: pd.DataFrame, df_spot: pd.DataFrame) -> pd.DataFrame:
    code_match = code
    spot_row = df_spot[df_spot['code'] == code_match]
    if spot_row.empty: return df_daily

    latest_data = spot_row.iloc[0]
    latest_date = datetime.now().date()
    close_price = latest_data['close']
    latest_volume = latest_data['volume']

    if not df_daily.empty:
        prev_day = df_daily.iloc[-1]
        open_price = prev_day['open']
        high_price = max(prev_day['high'], close_price)
        low_price = min(prev_day['low'], close_price)
    else:
        open_price, high_price, low_price = close_price, close_price, close_price

    new_row_data = {
        'date': latest_date, 'open': open_price, 'high': high_price, 'low': low_price, 'close': close_price,
        'volume': latest_volume,
        'amount': None, 'outstanding_share': None, 'turnover': None, 'adjust': CONFIG.get("ADJUST", ""), 'code': code,
    }

    df_new_day = pd.DataFrame([new_row_data], columns=df_daily.columns)
    df_daily['date_compare'] = pd.to_datetime(df_daily['date']).dt.date
    df_daily_filtered = df_daily[df_daily['date_compare'] != latest_date].drop(columns=['date_compare'])
    df_final = pd.concat([df_daily_filtered, df_new_day], ignore_index=True)
    return df_final


def fetch_data_with_timeout(symbol, start_date, end_date, adjust, timeout):
    def _fetch():
        if CONFIG["USE_LOCAL_MYSQL"]:
            try:
                return stock_zh_a_daily_mysql(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)
            except NameError:
                print(f"\n[é”™è¯¯] {symbol} å°è¯•ä½¿ç”¨ MySQL æ¥å£å¤±è´¥ (NameError)ï¼Œè‡ªåŠ¨é™çº§åˆ° AkShareã€‚\n")
                pass

        return ak.stock_zh_a_daily(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)

    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(_fetch)
        try:
            done, not_done = wait([future], timeout=timeout)
            if future in done:
                return future.result()
            elif future in not_done:
                future.cancel()
                raise ThreadingTimeoutError(f"è¯·æ±‚è¶…æ—¶ ({timeout}s)")
        except Exception as e:
            raise e


# ============================================================
# æ¨¡å— 5ï¼šå•åªè‚¡ç¥¨ç­–ç•¥ï¼ˆæ•´åˆï¼‰
# ============================================================
def strategy_single_stock(code, start_date, end_date, df_spot):
    symbol = f"sh{code}" if code.startswith("6") else f"sz{code}"

    try:
        df = fetch_data_with_timeout(symbol=symbol, start_date=start_date, end_date=end_date, adjust=CONFIG["ADJUST"],
                                     timeout=CONFIG["REQUEST_TIMEOUT"])

        if df is None or df.empty or len(df) < 220: return None

        if CONFIG["USE_REAL_TIME_DATA"]:
            df = append_today_realtime_snapshot(code, df, df_spot)

        current_close = float(df['close'].iloc[-1])
        prev_close = float(df['close'].iloc[-2])
        pct_chg = (current_close - prev_close) / prev_close * 100

        ma200_series = df['close'].rolling(200).mean()
        if pd.isna(ma200_series.iloc[-1]): return None
        ma200 = ma200_series.iloc[-1]

        pivot_series = calculate_pivot_high_vectorized(df)
        if pd.isna(pivot_series.iloc[-1]): return None
        last_pivot = pivot_series.iloc[-1]

        condition_trend = current_close > ma200
        condition_break = current_close > last_pivot
        condition_up = pct_chg > 0

        if not (condition_trend and condition_break and condition_up): return None

        df = squeeze_momentum(df, useTrueRange=CONFIG["SQZ"]["useTrueRange"])
        last = df.iloc[-1]
        prev = df.iloc[-2]

        break_strength = (current_close - last_pivot) / last_pivot * 100
        signal = "æ— "

        cond_now_strong_release = (
                last.get("val_color") == "å¼ºå¤š" and
                last.get("sqz_status") == "é‡Šæ”¾" and
                int(last.get("sqz_id", 0)) == 1
        )
        cond_prev_squeeze_long = (
                prev.get("sqz_status") == "æŒ¤å‹" and
                int(prev.get("sqz_id", 0)) >= 6
        )

        if cond_now_strong_release and cond_prev_squeeze_long:
            signal = "ä¹°å…¥"

        last_val = last.get("val")

        return {
            "ä»£ç ": code,
            "ä¿¡å·": signal,
            "å½“å‰ä»·": round(current_close, 2),
            "æ¶¨å¹…%": round(pct_chg, 2),
            "MA200": round(ma200, 2),
            "å‰é˜»åŠ›ä½": round(float(last_pivot), 2),
            "çªç ´åŠ›åº¦%": round(break_strength, 2),
            "BBå€¼": None if pd.isna(last_val) else round(float(last_val), 2),
            "BBä¸­æ–‡": last.get("val_color")
        }

    except ThreadingTimeoutError:
        print(f"[è¶…æ—¶] {code} è¯·æ±‚å†å²æ•°æ®è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return None

    except Exception as e:
        print(f"[é”™è¯¯] {code} å¤„ç†å¤±è´¥: {e}")
        return None


# ============================================================
# æ¨¡å— 6ï¼šå¹¶å‘æ‰«æ (Async Scheduler)
# ============================================================
async def main_scanner_async(stock_codes, df_spot):
    end_date = datetime.now().strftime("%Y%m%d")
    start_date = (datetime.now() - timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")

    results = []
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:

        tasks = [
            loop.run_in_executor(pool, strategy_single_stock, code, start_date, end_date, df_spot)
            for code in stock_codes
        ]

        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")
        for coro in pbar:
            res = await coro
            if res:
                results.append(res)
                pbar.set_postfix({"å‘½ä¸­": len(results)})

    return results


async def batch_scan_manager_async(target_codes, df_spot):
    all_results = []
    batch_size = CONFIG.get("BATCH_SIZE", 120)
    interval = CONFIG.get("BATCH_INTERVAL_SEC", 8)
    total_stocks = len(target_codes)
    num_batches = math.ceil(total_stocks / batch_size)

    print(f"\n[è°ƒåº¦å™¨] æ€»è®¡ {total_stocks} æ”¯è‚¡ç¥¨ï¼Œå°†åˆ†ä¸º {num_batches} æ‰¹æ¬¡ ({batch_size} æ”¯/æ‰¹)ã€‚")

    for i in range(num_batches):
        start_index = i * batch_size
        end_index = min((i + 1) * batch_size, total_stocks)
        batch_codes = target_codes[start_index:end_index]

        print("\n" + "=" * 60)
        print(f"--- ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i + 1}/{num_batches} ({len(batch_codes)} æ”¯) ---")

        batch_results = await main_scanner_async(batch_codes, df_spot)
        all_results.extend(batch_results)

        if i < num_batches - 1:
            print(f"--- ğŸ˜´ æ‰¹æ¬¡ {i + 1} å®Œæˆï¼Œå½“å‰æ€»å‘½ä¸­: {len(all_results)}ï¼Œä¼‘æ¯ {interval} ç§’... ---")
            time.sleep(interval)
        else:
            print("--- ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚---")

    return all_results


# ============================================================
# æ¨¡å— 7ï¼šä¸»å…¥å£
# ============================================================
def main():
    start_time = time.time()

    # ä½¿ç”¨ LogRedirector å¯åŠ¨æ—¥å¿—ç®¡ç†
    with LogRedirector(folder=CONFIG['OUTPUT_FOLDER_BASE']) as log_redirector:

        end_date = datetime.now().strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
        print(f"\n[ä»»åŠ¡å¯åŠ¨] æ‰«æèŒƒå›´: {start_date} ~ {end_date}")
        print(f"[é…ç½®] ç›®æ ‡çº¿ç¨‹: {CONFIG['MAX_WORKERS']} | è¶…æ—¶: {CONFIG['REQUEST_TIMEOUT']}s")

        # 1. ä¸²è¡Œè·å–å®æ—¶å¿«ç…§ (å—å¼€å…³æ§åˆ¶)
        df_spot = pd.DataFrame()

        if CONFIG["USE_REAL_TIME_DATA"]:
            try:
                df_spot = fetch_realtime_snapshot()
                if df_spot.empty:
                    print("[ç»ˆæ­¢] æ— æ³•è·å–å®æ—¶è¡Œæƒ…å¿«ç…§ï¼Œç»ˆæ­¢æ‰«æã€‚")
                    return
            except Exception as e:
                print(f"[è‡´å‘½ç»ˆæ­¢] è·å–å®æ—¶è¡Œæƒ…å¿«ç…§å¤±è´¥: {e}")
                return
        else:
            print("[é…ç½®] å®æ—¶æ•°æ®è·å–å¼€å…³å…³é—­ (USE_REAL_TIME_DATA=False)ï¼Œè·³è¿‡å…¨å¸‚åœºå¿«ç…§è·å–ã€‚")

        # 2. è·å–è‚¡ç¥¨åˆ—è¡¨å’Œè¿‡æ»¤
        manual_list = CONFIG["MANUAL_STOCK_LIST"]
        df_base = pd.DataFrame()
        target_codes = []

        if manual_list and len(manual_list) > 0:
            target_codes = [str(c).zfill(6) for c in manual_list]
            print(f"[æ‰‹åŠ¨æ¨¡å¼] ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥åˆ—è¡¨ï¼Œå…± {len(target_codes)} æ”¯è‚¡ç¥¨ã€‚")
            try:
                df_base = get_stock_list_manager()
            except Exception:
                df_base = pd.DataFrame({"code": target_codes, "name": ["æœªçŸ¥"] * len(target_codes)})
        else:
            try:
                df_base = get_stock_list_manager()
            except Exception as e:
                print(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}")
                return

            valid_codes = filter_stock_list(df_base)

            sample_size = CONFIG["SAMPLE_SIZE"]
            if isinstance(sample_size, int) and sample_size > 0 and len(valid_codes) > sample_size:
                print(f"[æŠ½æ ·æ¨¡å¼] éšæœºæŠ½å– {sample_size} æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•...")
                target_codes = random.sample(valid_codes, sample_size)
            else:
                print(f"[å…¨é‡æ¨¡å¼] æ‰«ææ‰€æœ‰ {len(valid_codes)} æ”¯æœ‰æ•ˆè‚¡ç¥¨...")
                target_codes = valid_codes

        # 3. å¹¶å‘æ‰«æ
        if CONFIG["SAMPLE_SIZE"] > 0 or len(target_codes) <= CONFIG["BATCH_SIZE"]:
            final_data = asyncio.run(main_scanner_async(target_codes, df_spot))
        else:
            final_data = asyncio.run(batch_scan_manager_async(target_codes, df_spot))

        # 4. ç»“æœæ•´ç†ä¸å¯¼å‡º
        if final_data:
            res_df = pd.DataFrame(final_data)
            res_df = res_df[res_df["ä¿¡å·"] == "ä¹°å…¥"].copy()

            if res_df.empty:
                print("\n[ç»“æœ] è¿‡æ»¤åæ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
                return

            if not df_base.empty:
                name_map = dict(zip(df_base["code"].astype(str), df_base["name"]))
                res_df.insert(1, "åç§°", res_df["ä»£ç "].map(name_map).fillna("æœªçŸ¥"))
            else:
                res_df.insert(1, "åç§°", "æœªçŸ¥")

            signal_order = {"ä¹°å…¥": 0, "è§‚å¯Ÿ": 1, "æ— ": 2}
            res_df["ä¿¡å·æ’åº"] = res_df["ä¿¡å·"].map(signal_order).fillna(3)
            res_df = res_df.sort_values(["ä¿¡å·æ’åº", "çªç ´åŠ›åº¦%"], ascending=[True, False]).drop(columns=["ä¿¡å·æ’åº"])

            # å¯¼å‡º CSV
            today_date_str = datetime.now().strftime('%Y-%m-%d')
            folder_path = os.path.join(CONFIG["OUTPUT_FOLDER_BASE"], today_date_str)
            os.makedirs(folder_path, exist_ok=True)
            timestamp = datetime.now().strftime('%H%M%S')
            file_name = f"{CONFIG['OUTPUT_FILENAME_BASE']}_{timestamp}.csv"
            full_file_path = os.path.join(folder_path, file_name)
            res_df.to_csv(full_file_path, index=False, encoding=CONFIG["EXPORT_ENCODING"])

            print("\n" + "=" * 60)
            print(f"âœ… æ‰«æå®Œæˆ | è€—æ—¶: {time.time() - start_time:.1f}s")
            print(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {full_file_path}")
            # æ³¨æ„ï¼šæ—¥å¿—æ–‡ä»¶çš„è·¯å¾„ç”± LogRedirector è‡ªèº«çš„ __exit__ æ–¹æ³•åœ¨é€€å‡ºæ—¶æ‰“å°
            print(f"ğŸ“ˆ å‘½ä¸­æ•°é‡: {len(res_df)}")
            print("=" * 60)
            print("--- å‘½ä¸­è‚¡ç¥¨ Top 10 ---")
            print(res_df.head(10).to_string(index=False))
            return res_df

        else:
            print("\n[ç»“æœ] æ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
            return pd.DataFrame()


# ============================================================
# å…¥å£
# ============================================================
if __name__ == "__main__":
    main()