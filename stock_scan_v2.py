#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡çªç ´æ‰«æç³»ç»Ÿï¼ˆPivot + SQZMOM + MA200ï¼‰
ç‰ˆæœ¬ï¼šv1.2 (è…¾è®¯å¿«ç…§ç¨³å®šç‰ˆ)

ã€æ ¸å¿ƒä¿®æ”¹ã€‘
1. åºŸå¼ƒé«˜å¤±è´¥ç‡çš„åˆ†é’Ÿæ•°æ®æ¥å£ã€‚
2. å®æ—¶æ•°æ®è·å–æ”¹ä¸ºä¸²è¡Œè°ƒç”¨è…¾è®¯å®æ—¶å¿«ç…§æ¥å£ (stock_zh_a_spot)ï¼Œå¹¶å¢åŠ é‡è¯•å’Œå»¶è¿Ÿã€‚
3. å®æ—¶å¿«ç…§æ•°æ® (df_spot) ä½œä¸ºå‚æ•°ä¼ é€’ç»™å¹¶å‘æ‰§è¡Œå™¨ï¼Œå®ç° O(1) æŸ¥æ‰¾æœ€æ–°ä»·ã€‚
4. ä¿®å¤ append_today_realtime_snapshot å‡½æ•°ä¸­çš„åˆ—åå…¼å®¹æ€§é—®é¢˜ã€‚
============================================================
"""
import os
import json
import time
import random
import math
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, wait, TimeoutError as ThreadingTimeoutError

import pandas as pd
import numpy as np
import akshare as ak
import asyncio
from tqdm import tqdm

from stock_query import stock_zh_a_daily_mysql


# ============================================================
# æ¨¡å— 1ï¼šé…ç½® (Configuration)
# ============================================================
CONFIG = {
    # --- æ—¶é—´èŒƒå›´ ---
    "DAYS": 365,  # æ‰«æå›æº¯å¤©æ•° (ç”¨äºè®¡ç®— MA200)

    # --- è¿‡æ»¤æ¡ä»¶ ---
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ã€689ï¼‰
    "EXCLUDE_BJ": True,   # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ã€92ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",      # å¤æƒæ–¹å¼

    # --- SQZç­–ç•¥å‚æ•° ---
    "SQZ": {
        "length": 20,
        "mult": 2.0,
        "lengthKC": 20,
        "multKC": 1.5,
        "useTrueRange": True
    },

    # --- PIVOTç­–ç•¥å‚æ•° ---
    "PIVOT_LEFT": 15,   # å·¦ä¾§ K çº¿æ•°é‡
    "PIVOT_RIGHT": 15,  # å³ä¾§ K çº¿æ•°é‡

    # --- æ–‡ä»¶è·¯å¾„/åç§° ---
    "CACHE_FILE": "stock_list_cache.json",
    "EXPORT_ENCODING": "utf-8-sig",  # CSVæ–‡ä»¶å¯¼å‡ºç¼–ç 
    "OUTPUT_FILENAME_BASE": "Buy_Stocks",
    "OUTPUT_FOLDER_BASE": "Day_Stocks",

    # --- æŠ½æ ·/å¹¶å‘ (å·²ä¼˜åŒ–) ---
    "SAMPLE_SIZE": 300,     # 0 æˆ– None è¡¨ç¤ºå…¨é‡
    "MAX_WORKERS": 10,      # é™ä½çº¿ç¨‹æ•°åˆ° 10
    "REQUEST_TIMEOUT": 20,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 20s

    # --- ğŸ†• æ•°æ®æºæ§åˆ¶ ---
    # True: ä½¿ç”¨æœ¬åœ° stock_zh_a_daily_mysql å‡½æ•°
    # False: ä½¿ç”¨ ak.stock_zh_a_daily (AkShare)
    "USE_LOCAL_MYSQL": True, # ğŸ‘ˆ é»˜è®¤ä½¿ç”¨ AkShare

    # å®æ—¶æ•°æ®å¼€å…³ (ç”¨äºæ§åˆ¶æ˜¯å¦è·å–ä»Šæ—¥å®æ—¶å¿«ç…§)
    # True: ä½¿ç”¨è…¾è®¯å®æ—¶è‚¡ç¥¨å…¨é‡æ¥å£ (fetch_realtime_snapshot)
    # False: ä¸ä½¿ç”¨ï¼Œè·³è¿‡å®æ—¶æ•°æ®è·å–ï¼ˆç”¨äºç¦»çº¿å›æµ‹æˆ–éäº¤æ˜“æ—¥ï¼‰
    "USE_REAL_TIME_DATA": False,

    # --- åˆ†æ‰¹æ§åˆ¶ ---
    "BATCH_SIZE": 120,        # æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
    "BATCH_INTERVAL_SEC": 8,  # æ‰¹æ¬¡é—´éš”ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰

    # --- ğŸ†• æ‰‹åŠ¨è¾“å…¥ ---
    # ç¤ºä¾‹: ["600519", "000001", "300751"]ã€‚å¦‚æœéç©ºï¼Œåˆ™è·³è¿‡å…¨é‡æ‰«æã€‚
    "MANUAL_STOCK_LIST": [
                            # "000807",
                            # "000708",
                            # "002830",
                            # "301517",
                            # "000408",
                            # "600879",
                            # "600595",
                            # "601168",
                            # "002595",
                            # "301028",
                            # "002429"
                        ]
}


# ============================================================
# å·¥å…·ï¼šé‡è¯•è£…é¥°å™¨ (Retry Decorator)
# ============================================================
def retry(max_retries=10, delay=15):  # å¢åŠ é‡è¯•æ¬¡æ•°åˆ° 5ï¼Œå»¶è¿Ÿåˆ° 5 ç§’
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == max_retries - 1:
                        raise
                    print(f"[è­¦å‘Š] {func.__name__} å¤±è´¥ ({type(e).__name__})ï¼Œ{delay}s åé‡è¯•...")
                    time.sleep(delay)

        return wrapper

    return decorator


# ============================================================
# æ¨¡å— 2ï¼šè·å–/ç¼“å­˜ å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ (ä»£ç ä¸å˜)
# ============================================================
@retry(max_retries=2, delay=1)
def fetch_stock_list_safe():
    """è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ï¼Œé‡‡ç”¨é™çº§ç­–ç•¥ä»¥æé«˜ç¨³å®šæ€§ã€‚"""
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")

    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_info_a_code_name æ¥å£")
            return df[["code", "name"]]
    except Exception as e:
        print(f"[è­¦å‘Š] è½»é‡æ¥å£å¤±è´¥ ({e})ï¼Œå°è¯•å¤‡ç”¨æ¥å£...")

    try:
        df = ak.stock_zh_a_spot_em()
        print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_zh_a_spot_em æ¥å£")
        # ç¡®ä¿åˆ—åä¸€è‡´
        if 'ä»£ç ' in df.columns:
            df = df.rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
        return df[["code", "name"]]
    except Exception as e:
        raise Exception(f"æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨æ¥å£å‡ä¸å¯ç”¨: {e}")


def get_stock_list_manager():
    """ç¼“å­˜ç®¡ç†å™¨ï¼šä¼˜å…ˆè¯»å–æœ¬åœ°ç¼“å­˜ï¼Œè¿‡æœŸæˆ–ä¸å­˜åœ¨åˆ™è”ç½‘æ›´æ–°ã€‚"""
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                print(f"[ç³»ç»Ÿ] åŠ è½½æœ¬æ—¥ç¼“å­˜ï¼Œå…± {len(cache['data'])} æ”¯è‚¡ç¥¨")
                return pd.DataFrame(cache["data"])
        except Exception:
            pass

    df = fetch_stock_list_safe()

    if not df.empty:
        with open(cache_file, "w", encoding="utf-8") as f:
            data = {
                "time": today_str,
                "data": df.to_dict(orient="records")
            }
            json.dump(data, f, ensure_ascii=False, indent=2)

    return df


def filter_stock_list(df):
    if df is None or df.empty:
        return []
    df["code"] = df["code"].astype(str).str.zfill(6)  # ç»Ÿä¸€æ ¼å¼ä¸º 6 ä½
    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]:
        mask |= df["code"].str.startswith("300")
    if CONFIG["EXCLUDE_KCB"]:
        mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]:
        mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns:
        mask |= df["name"].str.contains("ST|é€€", na=False)
    return df[~mask]["code"].tolist()

# ============================================================
# 3ï¼šæŠ€æœ¯æŒ‡æ ‡ï¼ˆSQZMOM / linreg / true_range / color / sqz_idï¼‰
# ============================================================
def tv_linreg(y, length):
    if pd.isna(y).any():
        return np.nan
    x = np.arange(length)
    y = y.values
    # é¿å… numpy.linalg.LinAlgError: Singular matrix in least squares
    if len(y) < 2:
        return np.nan

    A = np.vstack([x, np.ones(length)]).T
    try:
        m, b = np.linalg.lstsq(A, y, rcond=None)[0]
    except np.linalg.LinAlgError:
        return np.nan  # æå°‘æ•°æƒ…å†µå‡ºç°å¥‡å¼‚çŸ©é˜µ

    return m * (length - 1) + b


def true_range(df):
    prev_close = df['close'].shift(1)
    tr1 = df['high'] - df['low']
    tr2 = (df['high'] - prev_close).abs()
    tr3 = (df['low'] - prev_close).abs()
    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)


def get_color_cn(v, v_prev):
    if pd.isna(v) or pd.isna(v_prev):
        return None
    if v == 0:
        return "ä¸­æ€§"
    if v > 0:
        return "å¼ºå¤š" if v > v_prev else "å¼±å¤š"
    return "å¼ºç©º" if v < v_prev else "å¼±ç©º"


def add_squeeze_counter(df):
    counter = 0
    current_state = None
    sqz_id_list = []
    for status in df["sqz_status"]:
        if status in ["æŒ¤å‹", "é‡Šæ”¾"]:
            if status == current_state:
                counter += 1
            else:
                current_state = status
                counter = 1
            sqz_id_list.append(counter)
        else:
            current_state = None
            counter = 0
            sqz_id_list.append(0)
    df["sqz_id"] = sqz_id_list
    return df


def squeeze_momentum(df, length=None, mult=None, lengthKC=None, multKC=None, useTrueRange=True):
    # å…è®¸é€šè¿‡å‚æ•°è¦†ç›– CONFIG
    if length is None:
        length = CONFIG["SQZ"]["length"]
    if mult is None:
        mult = CONFIG["SQZ"]["mult"]
    if lengthKC is None:
        lengthKC = CONFIG["SQZ"]["lengthKC"]
    if multKC is None:
        multKC = CONFIG["SQZ"]["multKC"]

    close = df['close']
    high = df['high']
    low = df['low']

    # Bollinger Bands (æ³¨æ„ LazyBear ä½¿ç”¨ multKC)
    basis = close.rolling(length).mean()
    dev = multKC * close.rolling(length).std(ddof=0)
    upperBB = basis + dev
    lowerBB = basis - dev
    # ä¸ºè¾“å‡º BB å€¼ (ç”¨ % è·ç¦» basis)
    bb_width = (upperBB - lowerBB) / basis.replace(0, np.nan)

    # Keltner Channel
    ma = close.rolling(lengthKC).mean()
    r = true_range(df) if useTrueRange else (high - low)
    rangema = r.rolling(lengthKC).mean()
    upperKC = ma + rangema * multKC
    lowerKC = ma - rangema * multKC

    sqzOn = (lowerBB > lowerKC) & (upperBB < upperKC)
    sqzOff = (lowerBB < lowerKC) & (upperBB > upperKC)

    df["sqz_status"] = np.select([sqzOn, sqzOff], ["æŒ¤å‹", "é‡Šæ”¾"], default="æ— ")

    highest_h = high.rolling(lengthKC).max()
    lowest_l = low.rolling(lengthKC).min()
    avg_hl = (highest_h + lowest_l) / 2
    sma_close = close.rolling(lengthKC).mean()
    mid = (avg_hl + sma_close) / 2
    source_mid = close - mid

    # ä½¿ç”¨ apply å’Œ tv_linreg æ¥è®¡ç®— momentum
    val = source_mid.rolling(lengthKC).apply(lambda x: tv_linreg(pd.Series(x), lengthKC), raw=False)
    df["val"] = val
    df["val_prev"] = val.shift(1)
    df["val_color"] = df.apply(lambda r: get_color_cn(r["val"], r["val_prev"]), axis=1)

    df["BB_pct"] = bb_width  # ç”¨äºè¾“å‡º BB å€¼æ¯”ä¾‹ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰
    df = add_squeeze_counter(df)
    return df


# ============================================================
# æ¨¡å— 4ï¼šPivot é«˜ç‚¹ï¼ˆå‰é˜»åŠ›ä½ï¼‰
# ============================================================
def calculate_pivot_high_vectorized(df, left=None, right=None):
    if left is None:
        left = CONFIG["PIVOT_LEFT"]
    if right is None:
        right = CONFIG["PIVOT_RIGHT"]

    highs = df['high'].values
    n = len(highs)
    pivots = np.full(n, np.nan)

    # ç®€å•æ˜äº†çš„éå†ï¼ˆç›¸å¯¹å®‰å…¨ï¼‰
    for i in range(left, n - right):
        left_max = np.max(highs[i - left:i])
        right_max = np.max(highs[i + 1:i + 1 + right])
        # ä¸¥æ ¼é«˜ç‚¹ï¼šå·¦ä¾§å’Œå³ä¾§çš„æœ€é«˜ä»·éƒ½ä½äºå½“å‰é«˜ç‚¹
        if highs[i] > left_max and highs[i] > right_max:
            pivots[i] = highs[i]

    return pd.Series(pivots, index=df.index).ffill()



# ============================================================
# æ¨¡å— 4ï¼šä»Šæ—¥å®æ—¶Kè¡¥å…… (å·²ä¿®æ”¹ä¸ºå…¨å±€å¿«ç…§æŸ¥æ‰¾æ¨¡å¼)
# ============================================================

@retry(max_retries=10, delay=15)  # ä½¿ç”¨é‡è¯•æœºåˆ¶ç¡®ä¿å¿«ç…§è·å–æˆåŠŸ
def fetch_realtime_snapshot():
    """
    ã€é‡è¦ä¿®æ”¹ã€‘ä½¿ç”¨è…¾è®¯æ¥å£è·å–æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°å®æ—¶å¿«ç…§ã€‚
    æ­¤æ“ä½œå¿…é¡»ä¸²è¡Œä¸”ç‹¬ç«‹äºå¹¶å‘ä»»åŠ¡æ‰§è¡Œã€‚
    """
    # ...
    # å¢åŠ ä¸€ä¸ªå‡çš„æµè§ˆå™¨å¤´ï¼Œä»¥æ›´å¥½åœ°ä¼ªè£…
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§ (è…¾è®¯æ¥å£)...")

    # æ³¨æ„: AkShare çš„ stock_zh_a_spot æ¥å£ä¸èƒ½ç›´æ¥ä¼ é€’ headers å‚æ•°ã€‚
    # æˆ‘ä»¬åªèƒ½ä¾èµ–å®ƒå†…éƒ¨çš„ user-agentï¼Œæˆ–åœ¨è°ƒç”¨å‰å°è¯•è¦†ç›– AkShare çš„å†…éƒ¨è®¾ç½®ã€‚
    # æœ€ç®€å•çš„åšæ³•æ˜¯ï¼š
    try:
        # ä¾èµ– AkShare å†…éƒ¨å·²è®¾ç½®çš„ headers
        df = ak.stock_zh_a_spot()  # è…¾è®¯æ¥å£
    except Exception:
        # å¦‚æœ stock_zh_a_spot å¤±è´¥ï¼Œå°è¯•ç”¨ requests + pandas.read_csv ç»•è¿‡ï¼Œä½†ä»£ç å¤æ‚
        # é‰´äºç›®æ ‡æ˜¯æœ€å°ä¿®æ”¹ï¼Œæˆ‘ä»¬ä¿æŒåŸæ ·ï¼Œå¹¶ä¾èµ–å¤–éƒ¨é‡è¯•å’Œå»¶è¿Ÿã€‚
        raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘å¤–éƒ¨ @retry æœºåˆ¶

    # è§„èŒƒåŒ–åˆ—å (è…¾è®¯æ¥å£åˆ—å)
    df = df.rename(columns={'ä»£ç ': 'code', 'æœ€æ–°ä»·': 'close', 'æˆäº¤é‡': 'volume'})

    # ä»…ä¿ç•™å¿…è¦åˆ—å¹¶æ ¼å¼åŒ–
    df = df[['code', 'close', 'volume']]
    df['code'] = df['code'].astype(str).str.zfill(6)

    print(f"[ç³»ç»Ÿ] æˆåŠŸè·å– {len(df)} æ¡å®æ—¶å¿«ç…§æ•°æ®ã€‚")
    return df


def append_today_realtime_snapshot(code: str, df_daily: pd.DataFrame, df_spot: pd.DataFrame) -> pd.DataFrame:
    """
    ä»é¢„å…ˆè·å–çš„å®æ—¶å¿«ç…§ df_spot ä¸­æŸ¥æ‰¾å½“å‰è‚¡ç¥¨æ•°æ®ï¼Œå¹¶è¿½åŠ åˆ° df_dailyã€‚
    """
    code_match = code  # å¿«ç…§ä¸­çš„ code å·²ç»æ˜¯ 6 ä½æ•°å­—

    spot_row = df_spot[df_spot['code'] == code_match]

    if spot_row.empty:
        return df_daily  # æ— æ³•æ‰¾åˆ°å®æ—¶å¿«ç…§ï¼Œè¿”å›åŸå§‹æ•°æ®

    latest_data = spot_row.iloc[0]
    latest_date = datetime.now().date()

    close_price = latest_data['close']
    latest_volume = latest_data['volume']

    # æ„é€ è¦è¿½åŠ çš„æ–°è¡Œ (ä½¿ç”¨æ˜¨æ—¥çš„å¼€/é«˜/ä½/æ”¶ä½œä¸ºä»Šæ—¥ OHLC çš„åŸºç¡€)
    # å®æ—¶å¿«ç…§æ¥å£ä¸ç›´æ¥æä¾›å½“æ—¥ OHLCï¼Œè¿™é‡Œç”¨ close å¡«å……

    # å°è¯•ä½¿ç”¨æ˜¨å¤©çš„ OHLCV ä½œä¸ºåŸºç¡€ï¼Œå°† close æ›¿æ¢ä¸ºå®æ—¶ä»·
    # è¿™æ˜¯é˜²æ­¢å¤æƒæ•°æ®ä¸­ç¼ºå¤± OHLC ä¿¡æ¯çš„ä¸´æ—¶æ–¹æ¡ˆ
    if not df_daily.empty:
        prev_day = df_daily.iloc[-1]
        open_price = prev_day['open']
        high_price = max(prev_day['high'], close_price)  # ç¡®ä¿æœ€æ–°ä»·è½åœ¨ OHLCV èŒƒå›´å†…
        low_price = min(prev_day['low'], close_price)
    else:
        open_price = close_price
        high_price = close_price
        low_price = close_price

    new_row_data = {
        'date': latest_date,
        'open': open_price,
        'high': high_price,
        'low': low_price,
        'close': close_price,
        'volume': latest_volume,

        # å¡«å…… df_daily ä¸­ç¼ºå¤±çš„åˆ—ï¼Œé¿å… concat é”™è¯¯
        'amount': None,
        'outstanding_share': None,
        'turnover': None,
        'adjust': CONFIG.get("ADJUST", ""),
        'code': code,
    }

    df_new_day = pd.DataFrame([new_row_data], columns=df_daily.columns)

    # è¦†ç›–é€»è¾‘ï¼šåˆ é™¤æ—§çš„å½“æ—¥æ•°æ® (å¦‚æœæœ‰çš„è¯)
    df_daily['date_compare'] = pd.to_datetime(df_daily['date']).dt.date
    df_daily_filtered = df_daily[df_daily['date_compare'] != latest_date].drop(columns=['date_compare'])

    df_final = pd.concat([df_daily_filtered, df_new_day], ignore_index=True)
    return df_final


def fetch_data_with_timeout(symbol, start_date, end_date, adjust, timeout):
    """
    ã€å…³é”®ä¿®æ”¹ã€‘å¸¦è¶…æ—¶ä¿æŠ¤çš„å‡½æ•°ï¼Œç”¨äºè·å–å†å²æ•°æ®ï¼Œæ ¹æ® CONFIG åˆ‡æ¢æ•°æ®æºã€‚
    """
    def _fetch():
        if CONFIG["USE_LOCAL_MYSQL"]:
            # ğŸ”´ æ¥å£ 1: æœ¬åœ° MySQL æ¥å£
            try:
                return stock_zh_a_daily_mysql(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date,
                    adjust=adjust
                )
            except NameError:
                print("\n[é”™è¯¯] è¯·ç¡®ä¿å·²å¯¼å…¥æˆ–å®šä¹‰ 'stock_zh_a_daily_mysql' å‡½æ•°ï¼\n")
                # è‡ªåŠ¨é™çº§åˆ° AkShare
                pass

        # ğŸ”µ æ¥å£ 2: AkShare æ¥å£ (å¦‚æœéæœ¬åœ°æ¨¡å¼æˆ–æœ¬åœ°æ¨¡å¼å¤±è´¥)
        return ak.stock_zh_a_daily(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            adjust=adjust
        )

    # ä½¿ç”¨ä¸€ä¸ªä¸´æ—¶çš„çº¿ç¨‹æ± æ¥ç®¡ç†è¶…æ—¶ä»»åŠ¡
    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(_fetch)
        try:
            done, not_done = wait([future], timeout=timeout)
            if future in done:
                return future.result()
            elif future in not_done:
                future.cancel()
                raise ThreadingTimeoutError(f"è¯·æ±‚è¶…æ—¶ ({timeout}s)")
        except Exception as e:
            raise e


# ============================================================
# æ¨¡å— 5ï¼šå•åªè‚¡ç¥¨ç­–ç•¥ï¼ˆæ•´åˆï¼‰(ä¿®æ”¹å‚æ•°ä»¥æ¥æ”¶å¿«ç…§æ•°æ®)
# ============================================================
def strategy_single_stock(code, start_date, end_date, df_spot):  # æ¥æ”¶ df_spot
    """
    å•åªè‚¡ç¥¨ç­–ç•¥åˆ†æï¼Œè¿”å› dict æˆ– None
    """
    # å‡è®¾è‚¡ç¥¨ä»£ç æ˜¯ 6 ä½çº¯æ•°å­—ï¼Œæ ¹æ®å¼€å¤´åˆ¤æ–­æ˜¯æ²ªå¸‚(sh)è¿˜æ˜¯æ·±å¸‚(sz)
    symbol = f"sh{code}" if code.startswith("6") else f"sz{code}"

    try:
        # 1. è·å–å†å²æ•°æ®
        df = fetch_data_with_timeout(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            adjust=CONFIG["ADJUST"],
            timeout=CONFIG["REQUEST_TIMEOUT"]
        )

        if df is None or df.empty or len(df) < 220:
            return None

        # 2. æ·»åŠ å®æ—¶æ•°æ®ï¼ˆå½“æ—¥å¿«ç…§ï¼‰
        # ã€å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å…¨å±€å¿«ç…§æ•°æ®ï¼Œé¿å…ç½‘ç»œI/Oã€‘
        if CONFIG["USE_REAL_TIME_DATA"]:
            df = append_today_realtime_snapshot(code, df, df_spot)

        # 3. æ•°æ®æ ¡éªŒä¸çŸ­è·¯ä¼˜åŒ–
        current_close = float(df['close'].iloc[-1])
        prev_close = float(df['close'].iloc[-2])
        pct_chg = (current_close - prev_close) / prev_close * 100

        # MA200
        ma200_series = df['close'].rolling(200).mean()
        if pd.isna(ma200_series.iloc[-1]): return None
        ma200 = ma200_series.iloc[-1]

        # Pivot (å‰é˜»åŠ›ä½)
        pivot_series = calculate_pivot_high_vectorized(df)
        if pd.isna(pivot_series.iloc[-1]): return None
        last_pivot = pivot_series.iloc[-1]

        condition_trend = current_close > ma200
        condition_break = current_close > last_pivot
        condition_up = pct_chg > 0

        # ğŸŸ¢ çŸ­è·¯ä¼˜åŒ–ï¼šåªè¦æœ‰ä¸€ä¸ªæ¡ä»¶ä¸æ»¡è¶³ï¼Œç›´æ¥è¿”å› None
        if not (condition_trend and condition_break and condition_up):
            return None

        # 4. SQZMOM è®¡ç®—ä¸ä¿¡å·åˆ¤å®š
        df = squeeze_momentum(df, useTrueRange=CONFIG["SQZ"]["useTrueRange"])
        last = df.iloc[-1]
        prev = df.iloc[-2]

        break_strength = (current_close - last_pivot) / last_pivot * 100
        signal = "æ— "

        cond_now_strong_release = (
                last.get("val_color") == "å¼ºå¤š" and
                last.get("sqz_status") == "é‡Šæ”¾" and
                int(last.get("sqz_id", 0)) == 1
        )
        cond_prev_squeeze_long = (
                prev.get("sqz_status") == "æŒ¤å‹" and
                int(prev.get("sqz_id", 0)) >= 6
        )

        if cond_now_strong_release and cond_prev_squeeze_long:
            signal = "ä¹°å…¥"

        last_val = last.get("val")

        return {
            "ä»£ç ": code,
            "ä¿¡å·": signal,
            "å½“å‰ä»·": round(current_close, 2),
            "æ¶¨å¹…%": round(pct_chg, 2),
            "MA200": round(ma200, 2),
            "å‰é˜»åŠ›ä½": round(float(last_pivot), 2),
            "çªç ´åŠ›åº¦%": round(break_strength, 2),
            "BBå€¼": None if pd.isna(last_val) else round(float(last_val), 2),
            "BBä¸­æ–‡": last.get("val_color")
        }

    except ThreadingTimeoutError:
        print(f"[è¶…æ—¶] {code} è¯·æ±‚å†å²æ•°æ®è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return None

    except Exception as e:
        print(f"[é”™è¯¯] {code} å¤„ç†å¤±è´¥: {e}")
        return None


# ============================================================
# æ¨¡å— 6ï¼šå¹¶å‘æ‰«æ (Async Scheduler) (ä¿®æ”¹å‚æ•°ä»¥æ¥æ”¶å¿«ç…§æ•°æ®)
# ============================================================
async def main_scanner_async(stock_codes, df_spot):  # æ¥æ”¶ df_spot
    end_date = datetime.now().strftime("%Y%m%d")
    start_date = (datetime.now() - timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")

    results = []
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:

        tasks = [
            # ä¼ é€’ df_spot
            loop.run_in_executor(pool, strategy_single_stock, code, start_date, end_date, df_spot)
            for code in stock_codes
        ]

        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")
        for coro in pbar:
            res = await coro
            if res:
                results.append(res)
                pbar.set_postfix({"å‘½ä¸­": len(results)})

    return results


async def batch_scan_manager_async(target_codes, df_spot):  # æ¥æ”¶ df_spot
    """
    è´Ÿè´£å°†æ‰€æœ‰è‚¡ç¥¨ä»£ç åˆ†æ‰¹ã€æŒ‰é¡ºåºæ‰§è¡Œæ‰«æï¼Œå¹¶åœ¨æ‰¹æ¬¡é—´ä¼‘æ¯ã€‚
    """
    all_results = []

    batch_size = CONFIG.get("BATCH_SIZE", 120)
    interval = CONFIG.get("BATCH_INTERVAL_SEC", 8)
    total_stocks = len(target_codes)

    num_batches = math.ceil(total_stocks / batch_size)

    print(f"\n[è°ƒåº¦å™¨] æ€»è®¡ {total_stocks} æ”¯è‚¡ç¥¨ï¼Œå°†åˆ†ä¸º {num_batches} æ‰¹æ¬¡ ({batch_size} æ”¯/æ‰¹)ã€‚")

    # æŒ‰æ‰¹æ¬¡è¿›è¡Œåˆ‡ç‰‡å’Œå¤„ç†
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = min((i + 1) * batch_size, total_stocks)
        batch_codes = target_codes[start_index:end_index]

        print("\n" + "=" * 60)
        print(f"--- ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i + 1}/{num_batches} ({len(batch_codes)} æ”¯) ---")

        # ä¼ é€’ df_spot
        batch_results = await main_scanner_async(batch_codes, df_spot)
        all_results.extend(batch_results)

        # æ‰¹æ¬¡é—´ä¼‘æ¯
        if i < num_batches - 1:
            print(f"--- ğŸ˜´ æ‰¹æ¬¡ {i + 1} å®Œæˆï¼Œå½“å‰æ€»å‘½ä¸­: {len(all_results)}ï¼Œä¼‘æ¯ {interval} ç§’... ---")
            time.sleep(interval)
        else:
            print("--- ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚---")

    return all_results


# ============================================================
# æ¨¡å— 7ï¼šä¸»å…¥å£ (åœ¨å¹¶å‘å‰ä¸²è¡Œè·å–å¿«ç…§)
# ============================================================
def main():
    start_time = time.time()

    end_date = datetime.now().strftime("%Y%m%d")
    start_date = (datetime.now() - timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
    print(f"\n[ä»»åŠ¡å¯åŠ¨] æ‰«æèŒƒå›´: {start_date} ~ {end_date}")
    print(f"[é…ç½®] ç›®æ ‡çº¿ç¨‹: {CONFIG['MAX_WORKERS']} | è¶…æ—¶: {CONFIG['REQUEST_TIMEOUT']}s")

    # 1. ä¸²è¡Œè·å–å®æ—¶å¿«ç…§ (å—å¼€å…³æ§åˆ¶)
    df_spot = pd.DataFrame()

    if CONFIG["USE_REAL_TIME_DATA"]:
        try:
            df_spot = fetch_realtime_snapshot()
            if df_spot.empty:
                print("[ç»ˆæ­¢] æ— æ³•è·å–å®æ—¶è¡Œæƒ…å¿«ç…§ï¼Œç»ˆæ­¢æ‰«æã€‚")
                return
        except Exception as e:
            print(f"[è‡´å‘½ç»ˆæ­¢] è·å–å®æ—¶è¡Œæƒ…å¿«ç…§å¤±è´¥: {e}")
            return
    else:
        print("[é…ç½®] å®æ—¶æ•°æ®è·å–å¼€å…³å…³é—­ (USE_REAL_TIME_DATA=False)ï¼Œè·³è¿‡å…¨å¸‚åœºå¿«ç…§è·å–ã€‚")

    # 2. è·å–è‚¡ç¥¨åˆ—è¡¨å’Œè¿‡æ»¤ (é€»è¾‘ä¸å˜)
    manual_list = CONFIG["MANUAL_STOCK_LIST"]
    df_base = pd.DataFrame()
    target_codes = []

    if manual_list and len(manual_list) > 0:
        target_codes = [str(c).zfill(6) for c in manual_list]
        print(f"[æ‰‹åŠ¨æ¨¡å¼] ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥åˆ—è¡¨ï¼Œå…± {len(target_codes)} æ”¯è‚¡ç¥¨ã€‚")
        try:
            df_base = get_stock_list_manager()
        except Exception:
            df_base = pd.DataFrame({"code": target_codes, "name": ["æœªçŸ¥"] * len(target_codes)})
    else:
        # å…¨å¸‚åœºè·å–å¹¶è¿‡æ»¤
        try:
            df_base = get_stock_list_manager()
        except Exception as e:
            print(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}")
            return

        valid_codes = filter_stock_list(df_base)

        sample_size = CONFIG["SAMPLE_SIZE"]
        if isinstance(sample_size, int) and sample_size > 0 and len(valid_codes) > sample_size:
            print(f"[æŠ½æ ·æ¨¡å¼] éšæœºæŠ½å– {sample_size} æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•...")
            target_codes = random.sample(valid_codes, sample_size)
        else:
            print(f"[å…¨é‡æ¨¡å¼] æ‰«ææ‰€æœ‰ {len(valid_codes)} æ”¯æœ‰æ•ˆè‚¡ç¥¨...")
            target_codes = valid_codes

    # 3. å¹¶å‘æ‰«æ (ä¼ é€’ df_spot)
    if CONFIG["SAMPLE_SIZE"] > 0 or len(target_codes) <= CONFIG["BATCH_SIZE"]:
        # æŠ½æ ·æˆ–å°æ‰¹é‡ï¼Œç›´æ¥è¿è¡Œ
        final_data = asyncio.run(main_scanner_async(target_codes, df_spot))
    else:
        # å…¨é‡å¤§æ‰¹é‡ï¼Œä½¿ç”¨åˆ†æ‰¹ç®¡ç†å™¨
        final_data = asyncio.run(batch_scan_manager_async(target_codes, df_spot))

    # 4. ç»“æœæ•´ç†ä¸å¯¼å‡º (é€»è¾‘ä¸å˜)
    if final_data:
        res_df = pd.DataFrame(final_data)

        res_df = res_df[res_df["ä¿¡å·"] == "ä¹°å…¥"].copy()

        if res_df.empty:
            print("\n[ç»“æœ] è¿‡æ»¤åæ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
            return

        # è¡¥å…¨è‚¡ç¥¨åç§°
        if not df_base.empty:
            name_map = dict(zip(df_base["code"].astype(str), df_base["name"]))
            res_df.insert(1, "åç§°", res_df["ä»£ç "].map(name_map).fillna("æœªçŸ¥"))
        else:
            res_df.insert(1, "åç§°", "æœªçŸ¥")

        # æ’åº
        signal_order = {"ä¹°å…¥": 0, "è§‚å¯Ÿ": 1, "æ— ": 2}
        res_df["ä¿¡å·æ’åº"] = res_df["ä¿¡å·"].map(signal_order).fillna(3)
        res_df = res_df.sort_values(["ä¿¡å·æ’åº", "çªç ´åŠ›åº¦%"], ascending=[True, False]).drop(columns=["ä¿¡å·æ’åº"])

        # å¯¼å‡º CSV
        today_date_str = datetime.now().strftime('%Y-%m-%d')
        folder_path = os.path.join(CONFIG["OUTPUT_FOLDER_BASE"], today_date_str)
        os.makedirs(folder_path, exist_ok=True)
        timestamp = datetime.now().strftime('%H%M%S')
        file_name = f"{CONFIG['OUTPUT_FILENAME_BASE']}_{timestamp}.csv"
        full_file_path = os.path.join(folder_path, file_name)
        res_df.to_csv(full_file_path, index=False, encoding=CONFIG["EXPORT_ENCODING"])

        print("\n" + "=" * 60)
        print(f"âœ… æ‰«æå®Œæˆ | è€—æ—¶: {time.time() - start_time:.1f}s")
        print(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {full_file_path}")
        print(f"ğŸ“ˆ å‘½ä¸­æ•°é‡: {len(res_df)}")
        print("=" * 60)
        print("--- å‘½ä¸­è‚¡ç¥¨ Top 10 ---")
        print(res_df.head(10).to_string(index=False))
        return res_df

    else:
        print("\n[ç»“æœ] æ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
        return pd.DataFrame()


# ============================================================
# å…¥å£
# ============================================================
if __name__ == "__main__":
    main()