#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡çªç ´æ‰«æç³»ç»Ÿï¼ˆPivot + SQZMOM + MA200ï¼‰
ç‰ˆæœ¬ï¼šv1.5 (äº¤æ˜“æ—¥åˆ¤æ–­ + å®æ—¶æ•°æ®è¿½åŠ ä¿®æ­£ç‰ˆ)

ã€æ ¸å¿ƒä¿®æ”¹ã€‘
1. åºŸå¼ƒé«˜å¤±è´¥ç‡çš„åˆ†é’Ÿæ•°æ®æ¥å£ã€‚
2. å®æ—¶æ•°æ®è·å–æ”¹ä¸ºä¸²è¡Œè°ƒç”¨è…¾è®¯å®æ—¶å¿«ç…§æ¥å£ (stock_zh_a_spot)ï¼Œå¹¶å¢åŠ é‡è¯•å’Œå»¶è¿Ÿã€‚
3. å®æ—¶å¿«ç…§æ•°æ® (df_spot) ä½œä¸ºå‚æ•°ä¼ é€’ç»™å¹¶å‘æ‰§è¡Œå™¨ï¼Œå®ç° O(1) æŸ¥æ‰¾æœ€æ–°ä»·ã€‚
4. ä¿®å¤ append_today_realtime_snapshot å‡½æ•°ä¸­çš„åˆ—åå…¼å®¹æ€§é—®é¢˜ã€‚
5. å¼•å…¥äº¤æ˜“æ—¥å†åŠ è½½å’Œåˆ¤æ–­æœºåˆ¶ï¼Œç¡®ä¿åªåœ¨äº¤æ˜“æ—¥è¿½åŠ å®æ—¶æ•°æ®ã€‚
6. ä¿®æ­£ append_today_realtime_snapshot é€»è¾‘ï¼š
    - ä¸¥æ ¼æ ¹æ®æ—¥æœŸåˆ¤æ–­æ˜¯å¦è¿½åŠ ï¼ˆéäº¤æ˜“æ—¥/å†å²æ•°æ®å·²åŒ…å«ä»Šæ—¥æ•°æ®åˆ™ä¸è¿½åŠ ï¼‰ã€‚
    - ä½¿ç”¨å®æ—¶å¿«ç…§æä¾›çš„ O/H/L/C/Volume æ„é€ å½“æ—¥ K çº¿ã€‚
============================================================
"""
import os
import sys
import json
import time
import random
import math
import datetime
from concurrent.futures import ThreadPoolExecutor, wait, TimeoutError as ThreadingTimeoutError

import pandas as pd
import numpy as np
import akshare as ak
import asyncio
from tqdm import tqdm

try:
    from api.stock_query import stock_zh_a_daily_mysql
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ stock_zh_a_daily_mysqlã€‚è¯·ç¡®ä¿æ‚¨çš„ stock_query.py æ–‡ä»¶å­˜åœ¨ã€‚")
    def stock_zh_a_daily_mysql(*args, **kwargs):
        raise NameError("stock_zh_a_daily_mysql å°šæœªå®šä¹‰æˆ–å¯¼å…¥å¤±è´¥ã€‚")

# ============================================================
# æ¨¡å— 1ï¼šé…ç½® (Configuration)
# ============================================================
CONFIG = {
    # --- ğŸ†• æ—¶é—´èŒƒå›´ ---
    "DAYS": 365,  # æ‰«æå›æº¯å¤©æ•° (ç”¨äºè®¡ç®— MA200)

    # --- ğŸ†• è¿‡æ»¤æ¡ä»¶ ---
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ã€301ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ã€689ï¼‰
    "EXCLUDE_BJ": True,   # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ã€92ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",      # å¤æƒæ–¹å¼

    # --- ğŸ†• SQZç­–ç•¥å‚æ•° ---
    "SQZ": {
        "length": 20,
        "mult": 2.0,
        "lengthKC": 20,
        "multKC": 1.5,
        "useTrueRange": True
    },

    # --- ğŸ†• PIVOTç­–ç•¥å‚æ•° ---
    "PIVOT_LEFT": 15,   # å·¦ä¾§ K çº¿æ•°é‡
    "PIVOT_RIGHT": 15,  # å³ä¾§ K çº¿æ•°é‡

    # --- ğŸ†• æ–‡ä»¶è·¯å¾„/åç§° ---
    "CACHE_FILE": "stock_list_cache.json",
    "EXPORT_ENCODING": "utf-8-sig",        # CSVæ–‡ä»¶å¯¼å‡ºç¼–ç 
    "OUTPUT_FILENAME_BASE": "Buy_Stocks",  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
    "OUTPUT_FOLDER_BASE": "stocks",    # LogRedirector ä¹Ÿä½¿ç”¨æ­¤æ–‡ä»¶å¤¹

    # --- ğŸ†• å¹¶å‘ ---
    "MAX_WORKERS": 10,      # é™ä½çº¿ç¨‹æ•°åˆ° 10
    "REQUEST_TIMEOUT": 20,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 20s

    # --- ğŸ†• æ•°æ®æºæ§åˆ¶ ---
    # True:  ä½¿ç”¨æœ¬åœ° stock_zh_a_daily_mysql å‡½æ•°
    # False: ä½¿ç”¨ ak.stock_zh_a_daily (AkShare)
    "USE_LOCAL_MYSQL": True,

    # --- ğŸ†• å®æ—¶æ•°æ®å¼€å…³ ---
    # True:  ä½¿ç”¨è…¾è®¯å®æ—¶è‚¡ç¥¨å…¨é‡æ¥å£ (fetch_realtime_snapshot)
    # False: ä¸ä½¿ç”¨ï¼Œè·³è¿‡å®æ—¶æ•°æ®è·å–ï¼ˆç”¨äºç¦»çº¿å›æµ‹æˆ–éäº¤æ˜“æ—¥ï¼‰
    "USE_REAL_TIME_DATA": True,

    # --- ğŸ†• æ˜¯å¦å…¨é‡/åˆ†æ‰¹æ§åˆ¶ ---
    "SAMPLE_SIZE": 0,         # 0 æˆ– None è¡¨ç¤ºå…¨é‡
    "BATCH_SIZE": 500,        # SAMPLE_SIZE å…¨é‡æ‰å¼€å¯åˆ†æ‰¹åŠŸèƒ½ï¼Œæ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
    "BATCH_INTERVAL_SEC": 5,  # æ‰¹æ¬¡é—´éš”ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰

    # --- ğŸ†• æ‰‹åŠ¨è¾“å…¥ ---
    # ç¤ºä¾‹: ["600519", "000001", "300751"]ã€‚å¦‚æœéç©ºï¼Œåˆ™è·³è¿‡å…¨é‡æ‰«æã€‚
    "MANUAL_STOCK_LIST": []
}


# ============================================================
# æ¨¡å— 2ï¼šæ—¥å¿—é‡å®šå‘ç±»
# ============================================================
class LogRedirector:
    """
    å°† sys.stdout çš„è¾“å‡ºåŒæ—¶é‡å®šå‘åˆ°ç»ˆç«¯å’Œæ—¥å¿—æ–‡ä»¶ï¼Œå¹¶å®ç°æŒ‰å¤§å°è½®æ¢ï¼ˆä¿ç•™æ—§æ–‡ä»¶ï¼‰ã€‚
    """
    # 20MB è½®æ¢é™åˆ¶
    MAX_BYTES = 20 * 1024 * 1024

    def __init__(self, folder="stocks"):
        # æ—¥å¿—è·¯å¾„: stocks/logs/YYYYMMDD/
        self.today_str = datetime.datetime.now().strftime('%Y%m%d')
        self.log_dir = os.path.join(folder, "logs", self.today_str)
        os.makedirs(self.log_dir, exist_ok=True)

        self.terminal = sys.stdout
        self.log_file = None
        self.current_log_path = None
        self.is_active = False

    def _get_new_log_path(self):
        """ç”Ÿæˆæ–°çš„æ—¥å¿—æ–‡ä»¶åï¼šYYYYMMDD_HHMMSS.log"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"{timestamp}.log"
        return os.path.join(self.log_dir, log_filename)

    def _check_and_rotate(self):
        """æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœè¶…è¿‡é™åˆ¶åˆ™å…³é—­æ—§æ–‡ä»¶å¹¶åˆ›å»ºæ–°æ–‡ä»¶ã€‚"""
        if self.log_file is None:
            # é¦–æ¬¡åˆ›å»º
            self.current_log_path = self._get_new_log_path()
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')
            return True

        # æ£€æŸ¥å¤§å°
        if os.path.getsize(self.current_log_path) > self.MAX_BYTES:
            self.log_file.write(f"\n[è½®æ¢] æ—¥å¿—è¾¾åˆ° {self.MAX_BYTES / 1024 / 1024:.0f}MB é™åˆ¶ï¼Œæ­£åœ¨åˆ‡æ¢æ–‡ä»¶...\n")
            self.log_file.close()  # <--- å…³é—­æ—§æ–‡ä»¶ï¼Œä¿ç•™åœ¨ç£ç›˜ä¸Š

            # åˆ›å»ºæ–°æ–‡ä»¶
            self.current_log_path = self._get_new_log_path()  # <--- ç”Ÿæˆå…¨æ–°çš„ã€ä¸é‡å¤çš„æ–‡ä»¶å
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')  # <--- æ‰“å¼€æ–°æ–‡ä»¶ç»§ç»­å†™å…¥
            return True

        return False

    def __enter__(self):
        try:
            self._check_and_rotate()  # é¦–æ¬¡åˆ›å»ºæ—¥å¿—æ–‡ä»¶
            sys.stdout = self
            self.is_active = True
            self.write(
                f"\n{'=' * 70}\n[ä¼šè¯å¼€å§‹] {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n[æ—¥å¿—æ–‡ä»¶] {self.current_log_path}\n{'=' * 70}\n")
            return self
        except Exception as e:
            print(f"[é”™è¯¯] æ—¥å¿—ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}", file=self.terminal)
            sys.stdout = self.terminal
            return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.is_active:
            self.write(f"\n{'=' * 70}\n[ä¼šè¯ç»“æŸ] {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'=' * 70}\n")
            sys.stdout = self.terminal
            if self.log_file:
                self.log_file.close()
            print(f"æ—¥å¿—æ–‡ä»¶å·²ä¿å­˜è‡³: {self.current_log_path}")

    def write(self, message):
        self.terminal.write(message)
        self.terminal.flush()

        if self.log_file:
            self._check_and_rotate()  # å†™å…¥å‰æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢
            if not message.startswith('\r'):
                self.log_file.write(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] {message}")
            else:
                self.log_file.write(message)
            self.log_file.flush()

    def flush(self):
        self.terminal.flush()
        if self.log_file:
            self.log_file.flush()

# ============================================================
# æ¨¡å—3ï¼šäº¤æ˜“æ—¥å†ç›¸å…³çš„å…¨å±€å˜é‡å’Œå‡½æ•°
# ============================================================
_TRADE_CALENDAR = set()

def is_trade_day(date_obj):
    """æ£€æŸ¥ç»™å®šçš„æ—¥æœŸæ˜¯å¦åœ¨å·²åŠ è½½çš„äº¤æ˜“æ—¥å†ä¸­"""
    # å¦‚æœæ—¥å†åŠ è½½å¤±è´¥ï¼Œ_TRADE_CALENDAR ä¸ºç©ºï¼Œåˆ™è®¤ä¸ºä¸æ˜¯äº¤æ˜“æ—¥ï¼Œä»¥é˜²æ­¢é”™è¯¯è¿½åŠ 
    return date_obj in _TRADE_CALENDAR

def load_trade_calendar():
    """
    åŠ è½½æœ€è¿‘å‡ å¹´çš„äº¤æ˜“æ—¥å†åˆ°å…¨å±€é›†åˆä¸­ã€‚
    ä¿®æ­£äº† akshare æ¥å£è¿”å› datetime.date å¯¹è±¡å¯¼è‡´çš„ç±»å‹é”™è¯¯ã€‚
    """
    global _TRADE_CALENDAR
    _TRADE_CALENDAR.clear()  # æ¸…ç©ºæ—§çš„æ—¥å†é›†åˆ

    try:
        print("[ç³»ç»Ÿ] æ­£åœ¨åŠ è½½äº¤æ˜“æ—¥å†...")

        # 1. è°ƒç”¨ AkShare æ¥å£
        calendar_df = ak.tool_trade_date_hist_sina()

        if calendar_df.empty or 'trade_date' not in calendar_df.columns:
            raise ValueError("äº¤æ˜“æ—¥å†æ•°æ®ç»“æ„ä¸æ­£ç¡®æˆ–ä¸ºç©ºã€‚")

        trade_dates = calendar_df['trade_date'].tolist()

        # 2. é˜²å¾¡æ€§å¤„ç†æ—¥æœŸç±»å‹
        for d in trade_dates:
            if isinstance(d, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸² (ä¾‹å¦‚ '20230101')ï¼Œè¿›è¡Œè§£æ
                date_obj = datetime.datetime.strptime(d, '%Y%m%d').date()
            # æ˜ç¡®å¼•ç”¨ datetime æ¨¡å—ä¸­çš„ datetime.datetime ç±»ï¼Œè§£å†³ isinstance æŠ¥é”™
            elif isinstance(d, datetime.datetime):
                # å¦‚æœæ˜¯ datetime.datetime å¯¹è±¡
                date_obj = d.date()
            # æ˜ç¡®å¼•ç”¨ datetime æ¨¡å—ä¸­çš„ datetime.date ç±»
            elif isinstance(d, datetime.date):
                # è¿™æ˜¯ AkShare æ¥å£å®é™…è¿”å›çš„ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨
                date_obj = d
            else:
                # é‡åˆ°å…¶ä»–ç±»å‹ï¼Œè·³è¿‡
                continue

            _TRADE_CALENDAR.add(date_obj)

        print(f"[ç³»ç»Ÿ] äº¤æ˜“æ—¥å†åŠ è½½å®Œæˆï¼Œå…± {len(_TRADE_CALENDAR)} ä¸ªäº¤æ˜“æ—¥ã€‚")

    except Exception as e:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè®¾ç½®ä¸ºç©ºé›†ï¼Œis_trade_day å°†è¿”å› Falseï¼Œä»è€Œç¦ç”¨å®æ—¶è¿½åŠ 
        print(f"[è­¦å‘Š] æ— æ³•åŠ è½½äº¤æ˜“æ—¥å†ï¼Œå®æ—¶æ•°æ®è¿½åŠ åŠŸèƒ½å°†å¤±æ•ˆï¼ˆå°†å§‹ç»ˆè·³è¿‡ï¼‰: {e}")

# ============================================================
# å·¥å…·ï¼šé‡è¯•è£…é¥°å™¨ (Retry Decorator)
# ============================================================
def retry(max_retries=10, delay=15):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == max_retries - 1:
                        raise
                    print(f"[è­¦å‘Š] {func.__name__} å¤±è´¥ ({type(e).__name__})ï¼Œ{delay}s åé‡è¯•...")
                    time.sleep(delay)

        return wrapper

    return decorator


# ============================================================
# æ¨¡å— 4ï¼šè·å–/ç¼“å­˜ å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
# ============================================================
@retry(max_retries=2, delay=1)
def fetch_stock_list_safe():
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_info_a_code_name æ¥å£")
            return df[["code", "name"]]
    except Exception as e:
        print(f"[è­¦å‘Š] è½»é‡æ¥å£å¤±è´¥ ({e})ï¼Œå°è¯•å¤‡ç”¨æ¥å£...")
    try:
        df = ak.stock_zh_a_spot_em()
        print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_zh_a_spot_em æ¥å£")
        if 'ä»£ç ' in df.columns:
            df = df.rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
        return df[["code", "name"]]
    except Exception as e:
        raise Exception(f"æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨æ¥å£å‡ä¸å¯ç”¨: {e}")


def get_stock_list_manager():
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                print(f"[ç³»ç»Ÿ] åŠ è½½æœ¬æ—¥ç¼“å­˜ï¼Œå…± {len(cache['data'])} æ”¯è‚¡ç¥¨")
                return pd.DataFrame(cache["data"])
        except Exception:
            pass

    df = fetch_stock_list_safe()
    if not df.empty:
        with open(cache_file, "w", encoding="utf-8") as f:
            data = {
                "time": today_str,
                "data": df.to_dict(orient="records")
            }
            json.dump(data, f, ensure_ascii=False, indent=2)

    return df


def filter_stock_list(df):
    if df is None or df.empty:
        return []
    df["code"] = df["code"].astype(str).str.zfill(6)
    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]:
        mask |= df["code"].str.startswith(("300", "301"))
    if CONFIG["EXCLUDE_KCB"]:
        mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]:
        mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns:
        mask |= df["name"].str.contains("ST|é€€", na=False)
    return df[~mask]["code"].tolist()


# ============================================================
# æ¨¡å— 5ï¼šæŠ€æœ¯æŒ‡æ ‡ï¼ˆSQZMOM / linreg / true_range / color / sqz_idï¼‰
# ============================================================
def tv_linreg(y, length):
    if pd.isna(y).any(): return np.nan
    x = np.arange(length)
    y = y.values
    if len(y) < 2: return np.nan
    A = np.vstack([x, np.ones(length)]).T
    try:
        m, b = np.linalg.lstsq(A, y, rcond=None)[0]
    except np.linalg.LinAlgError:
        return np.nan
    return m * (length - 1) + b


def true_range(df):
    prev_close = df['close'].shift(1)
    tr1 = df['high'] - df['low']
    tr2 = (df['high'] - prev_close).abs()
    tr3 = (df['low'] - prev_close).abs()
    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)


def get_color_cn(v, v_prev):
    if pd.isna(v) or pd.isna(v_prev): return None
    if v == 0: return "ä¸­æ€§"
    if v > 0: return "å¼ºå¤š" if v > v_prev else "å¼±å¤š"
    return "å¼ºç©º" if v < v_prev else "å¼±ç©º"


def add_squeeze_counter(df):
    counter = 0
    current_state = None
    sqz_id_list = []
    for status in df["sqz_status"]:
        if status in ["æŒ¤å‹", "é‡Šæ”¾"]:
            if status == current_state:
                counter += 1
            else:
                current_state = status
                counter = 1
            sqz_id_list.append(counter)
        else:
            current_state = None
            counter = 0
            sqz_id_list.append(0)
    df["sqz_id"] = sqz_id_list
    return df


def squeeze_momentum(df, length=None, mult=None, lengthKC=None, multKC=None, useTrueRange=True):
    if length is None: length = CONFIG["SQZ"]["length"]
    if mult is None: mult = CONFIG["SQZ"]["mult"]
    if lengthKC is None: lengthKC = CONFIG["SQZ"]["lengthKC"]
    if multKC is None: multKC = CONFIG["SQZ"]["multKC"]

    close, high, low = df['close'], df['high'], df['low']

    basis = close.rolling(length).mean()
    dev = multKC * close.rolling(length).std(ddof=0)
    upperBB, lowerBB = basis + dev, basis - dev
    bb_width = (upperBB - lowerBB) / basis.replace(0, np.nan)

    ma = close.rolling(lengthKC).mean()
    r = true_range(df) if useTrueRange else (high - low)
    rangema = r.rolling(lengthKC).mean()
    upperKC, lowerKC = ma + rangema * multKC, ma - rangema * multKC

    sqzOn = (lowerBB > lowerKC) & (upperBB < upperKC)
    sqzOff = (lowerBB < lowerKC) & (upperBB > upperKC)
    df["sqz_status"] = np.select([sqzOn, sqzOff], ["æŒ¤å‹", "é‡Šæ”¾"], default="æ— ")

    highest_h = high.rolling(lengthKC).max()
    lowest_l = low.rolling(lengthKC).min()
    avg_hl = (highest_h + lowest_l) / 2
    sma_close = close.rolling(lengthKC).mean()
    mid = (avg_hl + sma_close) / 2
    source_mid = close - mid

    val = source_mid.rolling(lengthKC).apply(lambda x: tv_linreg(pd.Series(x), lengthKC), raw=False)
    df["val"] = val
    df["val_prev"] = val.shift(1)
    df["val_color"] = df.apply(lambda r: get_color_cn(r["val"], r["val_prev"]), axis=1)

    df["BB_pct"] = bb_width
    df = add_squeeze_counter(df)
    return df


# ============================================================
# æ¨¡å— 6ï¼šPivot é«˜ç‚¹ï¼ˆå‰é˜»åŠ›ä½ï¼‰
# ============================================================
def calculate_pivot_high_vectorized(df, left=None, right=None):
    if left is None: left = CONFIG["PIVOT_LEFT"]
    if right is None: right = CONFIG["PIVOT_RIGHT"]

    highs = df['high'].values
    n = len(highs)
    pivots = np.full(n, np.nan)

    for i in range(left, n - right):
        left_max = np.max(highs[i - left:i])
        right_max = np.max(highs[i + 1:i + 1 + right])
        if highs[i] > left_max and highs[i] > right_max:
            pivots[i] = highs[i]

    return pd.Series(pivots, index=df.index).ffill()


# ============================================================
# æ¨¡å— 7ï¼šä»Šæ—¥å®æ—¶Kè¡¥å……
# ============================================================
@retry(max_retries=10, delay=15)
def fetch_realtime_snapshot():
    """
    è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§ (akshare)ï¼Œå¹¶æ ‡å‡†åŒ–åˆ—åå’Œä»£ç æ ¼å¼ã€‚
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å‰¥ç¦»æ‰€æœ‰éæ•°å­—å‰ç¼€ã€‚
    """
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§...")
    try:
        df = ak.stock_zh_a_spot()
    except Exception as e:
        print(f"[é”™è¯¯] è·å–å®æ—¶å¿«ç…§å¤±è´¥: {e}")
        return pd.DataFrame()

    # --- å…³é”®ä¿®æ­£ 1ï¼šç»Ÿä¸€åˆ—åæ˜ å°„ ---
    df = df.rename(columns={
        'ä»£ç ': 'code',
        'æœ€æ–°ä»·': 'close',
        'æˆäº¤é‡': 'volume',
        'ä»Šå¼€': 'open',
        'æœ€é«˜': 'high',
        'æœ€ä½': 'low',
        'æˆäº¤é¢': 'amount',
        'é‡‘é¢': 'amount',
    })

    # å¼ºåˆ¶è½¬æ¢ä¸ºçº¯æ•°å­— code
    if 'code' in df.columns:
        # ----------------------------------------------------
        # ğŸ†• æœ€ç»ˆä¿®æ­£ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ‰€æœ‰éæ•°å­—å­—ç¬¦
        # ----------------------------------------------------
        # 1. è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ›¿æ¢æ‰€æœ‰éæ•°å­—å­—ç¬¦ä¸ºç©ºç™½ ('\D' ä»£è¡¨éæ•°å­—)
        df['code'] = df['code'].astype(str).str.replace(r'\D', '', regex=True)

        # 2. ç¡®ä¿ä»£ç æ˜¯ 6 ä½å­—ç¬¦ä¸²
        df['code'] = df['code'].str.zfill(6)
    else:
        print("[è­¦å‘Š] å®æ—¶å¿«ç…§æ•°æ®ä¸­ç¼ºå°‘ 'ä»£ç '/'code' åˆ—ã€‚")
        return pd.DataFrame()

    # --- åç»­åˆ—ç­›é€‰ã€å¡«å……å’Œç±»å‹è½¬æ¢ä¸å˜ ---
    required_cols = ['code', 'open', 'high', 'low', 'close', 'volume', 'amount']

    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    df = df[required_cols]

    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    print(f"[ç³»ç»Ÿ] æˆåŠŸè·å– {len(df)} æ¡å®æ—¶å¿«ç…§æ•°æ®ã€‚")
    return df

def append_today_realtime_snapshot(code: str, df_daily: pd.DataFrame, df_spot: pd.DataFrame) -> pd.DataFrame:
    """
    å°†å®æ—¶è¡Œæƒ…å¿«ç…§ä½œä¸ºå½“æ—¥ K çº¿è¿½åŠ åˆ°å†å²æ—¥çº¿æ•°æ®ä¸­ã€‚
    å¢åŠ äº†è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ã€‚
    """

    latest_date = datetime.datetime.now().date()

    # ----------------------------------------------------
    # è°ƒè¯•ç‚¹ 1ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
    # ----------------------------------------------------
    if not is_trade_day(latest_date):
        print(f"[{code}] è°ƒè¯• 1: {latest_date} ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè·³è¿‡æ‹¼æ¥ã€‚")
        return df_daily

    # ----------------------------------------------------
    # è°ƒè¯•ç‚¹ 2ï¼šæ£€æŸ¥å®æ—¶å¿«ç…§ä¸­æ˜¯å¦å­˜åœ¨è¯¥è‚¡ç¥¨ (æœ€å¸¸è§å¤±è´¥åŸå› )
    # ----------------------------------------------------
    spot_row = df_spot[df_spot['code'] == code]
    if spot_row.empty:
        print(f"[{code}] è°ƒè¯• 2: å®æ—¶å¿«ç…§ df_spot ä¸­æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨æ•°æ®ã€‚")
        return df_daily

    latest_data = spot_row.iloc[0]  # latest_data æ˜¯ä¸€ä¸ª Series

    # ----------------------------------------------------
    # è°ƒè¯•ç‚¹ 3ï¼šæ£€æŸ¥å†å²æ•°æ®æ˜¯å¦å·²åŒ…å«ä»Šæ—¥æ•°æ®
    # ----------------------------------------------------
    if not df_daily.empty:
        df_daily_dates = pd.to_datetime(df_daily['date']).dt.date
        last_history_date = df_daily_dates.iloc[-1]

        if last_history_date == latest_date:
            print(f"[{code}] è°ƒè¯• 3: å†å²æ•°æ®ä¸­å·²åŒ…å« {latest_date} çš„æ•°æ®ï¼Œè·³è¿‡å®æ—¶æ‹¼æ¥ã€‚")
            return df_daily

        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿å†å²æ•°æ®æ—¥æœŸæ˜¯å‡åºä¸”è¿ç»­çš„ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´é”™è¯¯åˆ¤æ–­
        if last_history_date > latest_date:
            print(f"[{code}] è­¦å‘Š: å†å²æ•°æ®æ—¥æœŸ {last_history_date} æ™šäºå½“å‰æ—¥æœŸ {latest_date}ï¼Œè·³è¿‡æ‹¼æ¥ã€‚")
            return df_daily

    new_row_data = {
        'date': latest_date,
        # ç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆ—å
        'open': latest_data.get('open'),
        'high': latest_data.get('high'),
        'low': latest_data.get('low'),
        'close': latest_data.get('close'),
        'volume': latest_data.get('volume'),
        'amount': latest_data.get('amount'),

        # å¡«å……å†å²æ•°æ®ä¸­å…¶ä»–éœ€è¦çš„åˆ—
        'outstanding_share': None,
        'turnover': None,
        'adjust': CONFIG.get("ADJUST", ""),  # å‡è®¾ CONFIG æ˜¯å¯ç”¨çš„å…¨å±€å˜é‡
        'code': code,
    }

    # æ„é€ æ–°çš„ DataFrame è¡Œï¼Œå¹¶ç¡®ä¿åˆ—åä¸å†å²æ•°æ®ä¸€è‡´
    # âš ï¸ å¿…é¡»ç¡®ä¿ df_daily.columns åŒ…å« new_row_data ä¸­æ‰€æœ‰çš„é”®
    df_new_day = pd.DataFrame([new_row_data], columns=df_daily.columns)

    # ç›´æ¥è¿½åŠ åˆ°å†å²æ•°æ®æœ«å°¾
    df_final = pd.concat([df_daily, df_new_day], ignore_index=True)

    return df_final


def fetch_data_with_timeout(symbol, start_date, end_date, adjust, timeout):
    def _fetch():
        if CONFIG["USE_LOCAL_MYSQL"]:
            try:
                return stock_zh_a_daily_mysql(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)
            except NameError:
                print(f"\n[é”™è¯¯] {symbol} å°è¯•ä½¿ç”¨ MySQL æ¥å£å¤±è´¥ (NameError)ï¼Œè‡ªåŠ¨é™çº§åˆ° AkShareã€‚\n")
                pass

        return ak.stock_zh_a_daily(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)

    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(_fetch)
        try:
            done, not_done = wait([future], timeout=timeout)
            if future in done:
                return future.result()
            elif future in not_done:
                future.cancel()
                raise ThreadingTimeoutError(f"è¯·æ±‚è¶…æ—¶ ({timeout}s)")
        except Exception as e:
            raise e


# ============================================================
# æ¨¡å— 8ï¼šå•åªè‚¡ç¥¨ç­–ç•¥ï¼ˆæ•´åˆï¼‰ - å¢åŠ  "çªç ´è¶‹åŠ¿" å’Œ "å¾—åˆ†"
# ============================================================
def strategy_single_stock(code, start_date, end_date, df_spot):
    symbol = f"sh{code}" if code.startswith("6") else f"sz{code}"

    try:
        df = fetch_data_with_timeout(symbol=symbol, start_date=start_date, end_date=end_date, adjust=CONFIG["ADJUST"],
                                     timeout=CONFIG["REQUEST_TIMEOUT"])

        if df is None or df.empty or len(df) < 220: return None

        # è°ƒç”¨å®æ—¶è‚¡ç¥¨è¡Œæƒ…æ‹¼æ¥æ¥å£
        if CONFIG["USE_REAL_TIME_DATA"]:
            df = append_today_realtime_snapshot(code, df, df_spot)

        # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿè®¡ç®—å‰3å¤©
        if len(df) < 5: return None

        current_close = float(df['close'].iloc[-1])
        prev_close = float(df['close'].iloc[-2])
        pct_chg = (current_close - prev_close) / prev_close * 100

        ma200_series = df['close'].rolling(200).mean()
        if pd.isna(ma200_series.iloc[-1]): return None
        ma200 = ma200_series.iloc[-1]

        pivot_series = calculate_pivot_high_vectorized(df)
        if pd.isna(pivot_series.iloc[-1]): return None
        last_pivot = pivot_series.iloc[-1]

        condition_trend = current_close > ma200
        condition_break = current_close > last_pivot
        condition_up = pct_chg > 0

        if not (condition_trend and condition_break and condition_up): return None

        df = squeeze_momentum(df, useTrueRange=CONFIG["SQZ"]["useTrueRange"])
        last = df.iloc[-1]
        prev = df.iloc[-2]

        break_strength = (current_close - last_pivot) / last_pivot * 100
        signal = "æ— "

        cond_now_strong_release = (
                last.get("val_color") == "å¼ºå¤š" and
                last.get("sqz_status") == "é‡Šæ”¾" and
                int(last.get("sqz_id", 0)) == 1
        )
        cond_prev_squeeze_long = (
                prev.get("sqz_status") == "æŒ¤å‹" and
                int(prev.get("sqz_id", 0)) >= 6
        )

        if cond_now_strong_release and cond_prev_squeeze_long:
            signal = "ä¹°å…¥"

        last_val = last.get("val")

        # ===================================================
        # ğŸ†• å¢åŠ  "çªç ´è¶‹åŠ¿" å’Œ "å¾—åˆ†" çš„è®¡ç®—é€»è¾‘
        # ===================================================

        # è·å–å€’æ•°ç¬¬ 4, 3, 2 å¤©çš„æ”¶ç›˜ä»·
        close_D1 = df['close'].iloc[-4]  # å€’æ•°ç¬¬4å¤©
        close_D2 = df['close'].iloc[-3]  # å€’æ•°ç¬¬3å¤©
        close_D3 = df['close'].iloc[-2]  # å€’æ•°ç¬¬2å¤©

        # è§„åˆ™ï¼šæ”¶ç›˜ä»· > å‰é˜»åŠ›ä½ä»·æ ¼ (last_pivot) -> "é«˜"ï¼›å¦åˆ™ "ä½"
        # é¡ºåºï¼šå€’æ•°ç¬¬4å¤©-å€’æ•°ç¬¬3å¤©-å€’æ•°ç¬¬2å¤©

        # è®¡ç®—æ¯ä¸ªæ—¥æœŸçš„çªç ´çŠ¶æ€ (0: é«˜, 1: ä½)
        status_D1 = 0 if close_D1 > last_pivot else 1
        status_D2 = 0 if close_D2 > last_pivot else 1
        status_D3 = 0 if close_D3 > last_pivot else 1

        # æ„é€  "çªç ´è¶‹åŠ¿" å­—ç¬¦ä¸²
        trend_D1 = "é«˜" if status_D1 == 0 else "ä½"
        trend_D2 = "é«˜" if status_D2 == 0 else "ä½"
        trend_D3 = "é«˜" if status_D3 == 0 else "ä½"

        break_trend = f"{trend_D1}-{trend_D2}-{trend_D3}"

        # è®¡ç®— "å¾—åˆ†" (ä½ä¸º 1 åˆ†ï¼Œé«˜ä¸º 0 åˆ†)
        score = status_D1 + status_D2 + status_D3
        # ===================================================

        return {
            "ä»£ç ": code,
            "å¾—åˆ†": score,
            # ğŸ†• æ–°å¢ç»“æœ
            "çªç ´è¶‹åŠ¿": break_trend,
            "å½“å‰ä»·": round(current_close, 2),
            "æ¶¨å¹…%": round(pct_chg, 2),
            "MA200": round(ma200, 2),
            "å‰é˜»åŠ›ä½": round(float(last_pivot), 2),
            "çªç ´åŠ›åº¦%": round(break_strength, 2),
            "BBå€¼": None if pd.isna(last_val) else round(float(last_val), 2),
            "BBä¸­æ–‡": last.get("val_color"),
            "ä¿¡å·": signal
        }

    except ThreadingTimeoutError:
        print(f"[è¶…æ—¶] {code} è¯·æ±‚å†å²æ•°æ®è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return None

    except Exception as e:
        print(f"[é”™è¯¯] {code} å¤„ç†å¤±è´¥: {e}")
        return None

# ============================================================
# æ¨¡å— 9ï¼šå¹¶å‘æ‰«æ (Async Scheduler)
# ============================================================
async def main_scanner_async(stock_codes, df_spot):
    end_date = datetime.datetime.now().strftime("%Y%m%d")
    start_date = (datetime.datetime.now() - datetime.timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")

    results = []
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:

        tasks = [
            loop.run_in_executor(pool, strategy_single_stock, code, start_date, end_date, df_spot)
            for code in stock_codes
        ]

        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")
        for coro in pbar:
            res = await coro
            if res:
                results.append(res)
                pbar.set_postfix({"å‘½ä¸­": len(results)})

    return results


async def batch_scan_manager_async(target_codes, df_spot):
    all_results = []
    batch_size = CONFIG.get("BATCH_SIZE", 120)
    interval = CONFIG.get("BATCH_INTERVAL_SEC", 8)
    total_stocks = len(target_codes)
    num_batches = math.ceil(total_stocks / batch_size)

    print(f"\n[è°ƒåº¦å™¨] æ€»è®¡ {total_stocks} æ”¯è‚¡ç¥¨ï¼Œå°†åˆ†ä¸º {num_batches} æ‰¹æ¬¡ ({batch_size} æ”¯/æ‰¹)ã€‚")

    for i in range(num_batches):
        start_index = i * batch_size
        end_index = min((i + 1) * batch_size, total_stocks)
        batch_codes = target_codes[start_index:end_index]

        print("\n" + "=" * 60)
        print(f"--- ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i + 1}/{num_batches} ({len(batch_codes)} æ”¯) ---")

        batch_results = await main_scanner_async(batch_codes, df_spot)
        all_results.extend(batch_results)

        if i < num_batches - 1:
            print(f"--- ğŸ˜´ æ‰¹æ¬¡ {i + 1} å®Œæˆï¼Œå½“å‰æ€»å‘½ä¸­: {len(all_results)}ï¼Œä¼‘æ¯ {interval} ç§’... ---")
            time.sleep(interval)
        else:
            print("--- ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚---")

    return all_results


# ============================================================
# æ¨¡å— 10ï¼šä¸»å…¥å£
# ============================================================
def main():
    start_time = time.time()

    # --- ğŸ†• åŠ è½½äº¤æ˜“æ—¥å† (æ–°å¢æ­¥éª¤) ---
    if CONFIG["USE_REAL_TIME_DATA"]:
        load_trade_calendar()

    # ä½¿ç”¨ LogRedirector å¯åŠ¨æ—¥å¿—ç®¡ç†
    with LogRedirector(folder=CONFIG['OUTPUT_FOLDER_BASE']) as log_redirector:

        end_date = datetime.datetime.now().strftime("%Y%m%d")
        start_date = (datetime.datetime.now() - datetime.timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
        print(f"\n[ä»»åŠ¡å¯åŠ¨] æ‰«æèŒƒå›´: {start_date} ~ {end_date}")
        print(f"[é…ç½®] ç›®æ ‡çº¿ç¨‹: {CONFIG['MAX_WORKERS']} | è¶…æ—¶: {CONFIG['REQUEST_TIMEOUT']}s")

        # 1. ä¸²è¡Œè·å–å®æ—¶å¿«ç…§ (å—å¼€å…³æ§åˆ¶)
        df_spot = pd.DataFrame()

        if CONFIG["USE_REAL_TIME_DATA"]:
            try:
                # åªæœ‰åœ¨äº¤æ˜“æ—¥æ‰å°è¯•è·å–å®æ—¶å¿«ç…§
                if is_trade_day:
                    df_spot = fetch_realtime_snapshot()
                    if df_spot.empty:
                        # âš ï¸ å…³é”®ä¿®æ”¹ï¼šå½“ df_spot ä¸ºç©ºæ—¶ï¼Œç¨‹åºé€€å‡º
                        print("[ç»ˆæ­¢] æ— æ³•è·å–å®æ—¶è¡Œæƒ…å¿«ç…§ï¼Œç»ˆæ­¢æ‰«æã€‚")
                        sys.exit(1)
                else:
                    print("[é…ç½®] å½“å‰ä¸ºéäº¤æ˜“æ—¥ï¼Œè·³è¿‡å®æ—¶å¿«ç…§è·å–ã€‚")
            except Exception as e:
                # âš ï¸ å…³é”®ä¿®æ”¹ï¼šå½“ df_spot ä¸ºç©ºæ—¶ï¼Œç¨‹åºé€€å‡º
                print(f"[è‡´å‘½ç»ˆæ­¢] è·å–å®æ—¶è¡Œæƒ…å¿«ç…§å¤±è´¥: {e}")
                sys.exit(1)
        else:
            print("[é…ç½®] å®æ—¶æ•°æ®è·å–å¼€å…³å…³é—­ (USE_REAL_TIME_DATA=False)ï¼Œè·³è¿‡å…¨å¸‚åœºå¿«ç…§è·å–ã€‚")

        # 2. è·å–è‚¡ç¥¨åˆ—è¡¨å’Œè¿‡æ»¤
        manual_list = CONFIG["MANUAL_STOCK_LIST"]
        df_base = pd.DataFrame()
        target_codes = []

        if manual_list and len(manual_list) > 0:
            target_codes = [str(c).zfill(6) for c in manual_list]
            print(f"[æ‰‹åŠ¨æ¨¡å¼] ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥åˆ—è¡¨ï¼Œå…± {len(target_codes)} æ”¯è‚¡ç¥¨ã€‚")
            try:
                df_base = get_stock_list_manager()
            except Exception:
                df_base = pd.DataFrame({"code": target_codes, "name": ["æœªçŸ¥"] * len(target_codes)})
        else:
            try:
                df_base = get_stock_list_manager()
            except Exception as e:
                print(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}")
                return

            valid_codes = filter_stock_list(df_base)

            sample_size = CONFIG["SAMPLE_SIZE"]
            if isinstance(sample_size, int) and sample_size > 0 and len(valid_codes) > sample_size:
                print(f"[æŠ½æ ·æ¨¡å¼] éšæœºæŠ½å– {sample_size} æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•...")
                target_codes = random.sample(valid_codes, sample_size)
            else:
                print(f"[å…¨é‡æ¨¡å¼] æ‰«ææ‰€æœ‰ {len(valid_codes)} æ”¯æœ‰æ•ˆè‚¡ç¥¨...")
                target_codes = valid_codes

        # 3. å¹¶å‘æ‰«æ
        if CONFIG["SAMPLE_SIZE"] > 0 or len(target_codes) <= CONFIG["BATCH_SIZE"]:
            final_data = asyncio.run(main_scanner_async(target_codes, df_spot))
        else:
            final_data = asyncio.run(batch_scan_manager_async(target_codes, df_spot))

        # 4. ç»“æœæ•´ç†ä¸å¯¼å‡º
        if final_data:
            res_df = pd.DataFrame(final_data)
            res_df = res_df[res_df["ä¿¡å·"] == "ä¹°å…¥"].copy()

            if res_df.empty:
                print("\n[ç»“æœ] è¿‡æ»¤åæ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
                return

            if not df_base.empty:
                name_map = dict(zip(df_base["code"].astype(str), df_base["name"]))
                res_df.insert(1, "åç§°", res_df["ä»£ç "].map(name_map).fillna("æœªçŸ¥"))
            else:
                res_df.insert(1, "åç§°", "æœªçŸ¥")

            # =============================================================
            # ğŸ†• æ’åºé€»è¾‘ä¿®æ”¹ï¼š
            # 1. ä¸»è¦æ’åºé”®ï¼š'å¾—åˆ†' (è¶Šé«˜è¶Šå¥½ï¼Œascending=False)
            # 2. æ¬¡è¦æ’åºé”®ï¼š'æ¶¨å¹…%' (è¶Šé«˜è¶Šå¥½ï¼Œascending=False)
            # =============================================================
            res_df = res_df.sort_values(["å¾—åˆ†", "æ¶¨å¹…%"], ascending=[False, False]).reset_index(drop=True)

            # å¯¼å‡º CSV
            today_date_str = datetime.datetime.now().strftime('%Y-%m-%d')
            folder_path = os.path.join(CONFIG["OUTPUT_FOLDER_BASE"], today_date_str)
            os.makedirs(folder_path, exist_ok=True)
            timestamp = datetime.datetime.now().strftime('%H%M%S')
            file_name = f"{CONFIG['OUTPUT_FILENAME_BASE']}_{timestamp}.csv"
            full_file_path = os.path.join(folder_path, file_name)
            res_df.to_csv(full_file_path, index=False, encoding=CONFIG["EXPORT_ENCODING"])

            print("\n" + "=" * 60)
            print(f"âœ… æ‰«æå®Œæˆ | è€—æ—¶: {time.time() - start_time:.1f}s")
            print(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {full_file_path}")
            # æ³¨æ„ï¼šæ—¥å¿—æ–‡ä»¶çš„è·¯å¾„ç”± LogRedirector è‡ªèº«çš„ __exit__ æ–¹æ³•åœ¨é€€å‡ºæ—¶æ‰“å°
            print(f"ğŸ“ˆ å‘½ä¸­æ•°é‡: {len(res_df)}")
            print("=" * 60)
            print("--- å‘½ä¸­è‚¡ç¥¨ Top 10 ---")
            print(res_df.head(10).to_string(index=False))
            return res_df

        else:
            print("\n[ç»“æœ] æ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
            return pd.DataFrame()


if __name__ == "__main__":
    main()