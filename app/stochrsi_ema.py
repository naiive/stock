#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡çªç ´æ‰«æç³»ç»Ÿï¼ˆStochRSI + EMA è¶‹åŠ¿è¿‡æ»¤ï¼‰
ç‰ˆæœ¬ï¼šv4.0 (å¢åŠ  ATR æ­¢ç›ˆæ­¢æŸä½è®¡ç®—)

ã€æ ¸å¿ƒç­–ç•¥ã€‘
1. StochRSI ä¿¡å·: K çº¿ä¸Šç©¿è¶…å–æ°´å¹³ (é»˜è®¤ 20)ï¼Œä¸” K > D (é‡‘å‰)ã€‚
2. è¶‹åŠ¿è¿‡æ»¤: å½“å‰æ”¶ç›˜ä»· > EMA50 > EMA200 (å¼ºåŠ¿ä¸Šæ¶¨è¶‹åŠ¿)ã€‚
3. ATR åŠ¨æ€æ­¢æŸ: ä¿¡å·ä»· - N * ATRã€‚
============================================================
"""
import os
import sys
import json
import time
import random
import math
import datetime
from concurrent.futures import ThreadPoolExecutor, wait, TimeoutError as ThreadingTimeoutError

import pandas as pd
import numpy as np
import akshare as ak
import asyncio
from tqdm import tqdm

from api.stock_query import stock_zh_a_daily_mysql

# ============================================================
# æ¨¡å— 1ï¼šé…ç½® (Configuration) (å¢åŠ  ATR å‚æ•°)
# ============================================================
CONFIG = {
    # --- ğŸ†• æ—¶é—´èŒƒå›´ ---
    "DAYS": 365,  # æ‰«æå›æº¯å¤©æ•° (ç”¨äºè®¡ç®— MA200/EMA200)

    # --- ğŸ†• è¿‡æ»¤æ¡ä»¶ (ä¿æŒä¸å˜) ---
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ã€301ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ã€689ï¼‰
    "EXCLUDE_BJ": True,  # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ã€92ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",  # å¤æƒæ–¹å¼

    # --- ğŸ†• StochRSI å‚æ•° (ä¿æŒä¸å˜) ---
    "STOCH_RSI": {
        "lengthRSI": 14,
        "lengthStoch": 14,
        "smoothK": 3,
        "smoothD": 3,
        "oversoldLevel": 20
    },

    # --- ğŸ†• ATR æ­¢ç›ˆæ­¢æŸå‚æ•° ---
    "ATR_SETTING": {
        "lengthATR": 7,
        "stop_loss_multiplier": 1.5,  # æ­¢æŸå€æ•° M
        "take_profit_multiplier": 1.2  # æ­¢ç›ˆå€æ•°
    },

    # --- ğŸ†• æ–‡ä»¶è·¯å¾„/åç§° (ä¿æŒä¸å˜) ---
    "CACHE_FILE": "../conf/stock_list_cache.json",
    "EXPORT_ENCODING": "utf-8-sig",  # CSVæ–‡ä»¶å¯¼å‡ºç¼–ç 
    "OUTPUT_FILENAME_BASE": "Buy_Stocks_StochRSI_EMA_ATR",  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
    "OUTPUT_FOLDER_BASE": "../stocks",  # csvè¾“å‡º æ–‡ä»¶å¤¹
    "OUTPUT_LOG": "../logs",  # LogRedirector æ—¥å¿—è¾“å‡ºæ–‡ä»¶å¤¹

    # --- ğŸ†• å¹¶å‘ (ä¿æŒä¸å˜) ---
    "MAX_WORKERS": 10,  # é™ä½çº¿ç¨‹æ•°åˆ° 10
    "REQUEST_TIMEOUT": 20,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 20s

    # --- ğŸ†• æ•°æ®æºæ§åˆ¶ (ä¿æŒä¸å˜) ---
    "USE_LOCAL_MYSQL": True,
    "USE_REAL_TIME_DATA": False,
    "SAMPLE_SIZE": 0,
    "BATCH_SIZE": 1000,
    "BATCH_INTERVAL_SEC": 1,

    # --- ğŸ†• æ‰‹åŠ¨è¾“å…¥ (ä¿æŒä¸å˜) ---
    "MANUAL_STOCK_LIST": []
}


# ============================================================
# æ¨¡å— Aï¼šPine Script æ ¸å¿ƒå¹³æ»‘å‡½æ•° (StochRSI/EMA è®¡ç®—åŸºç¡€) (ä¿æŒä¸å˜)
# ============================================================
def pine_rma(series, length):
    """ RMA (Wilder's Smoothing) - ç”¨äºç²¾ç¡® RSI/ATR è®¡ç®— """
    # å¼ºåˆ¶è½¬æ¢ä¸º Series ä»¥ç¡®ä¿ .ewm() å¯ç”¨
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    alpha = 1 / length
    return series.ewm(alpha=alpha, adjust=False).mean()


def pine_sma(series, length):
    """ Simple Moving Average (SMA) - ç”¨äºç²¾ç¡® StochRSI K/D å¹³æ»‘ """
    # å¼ºåˆ¶è½¬æ¢ä¸º Series ä»¥ç¡®ä¿ .rolling() å¯ç”¨
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    return series.rolling(length).mean()


def pine_ema(series, length):
    """ EMA (Exponential Moving Average) - ç”¨äºç²¾ç¡® EMA 50/200 è¶‹åŠ¿è¿‡æ»¤ """
    # å¼ºåˆ¶è½¬æ¢ä¸º Series ä»¥ç¡®ä¿ .ewm() å¯ç”¨
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    alpha = 2 / (length + 1)
    return series.ewm(alpha=alpha, adjust=False).mean()


# ============================================================
# æ¨¡å— Bï¼šStochRSI æ ¸å¿ƒè®¡ç®— & ATR è®¡ç®— (æ–°å¢ ATR)
# ============================================================
def calculate_stoch_rsi_values(series, length_rsi, length_stoch):
    """è®¡ç®— StochRSI çš„åŸå§‹ K å€¼ (å·²ä¿®å¤ç±»å‹é”™è¯¯)"""
    if not isinstance(series, pd.Series):
        series = pd.Series(series)

    delta = series.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)

    up_avg = pine_rma(up, length_rsi)
    down_avg = pine_rma(down, length_rsi)

    rs_arr = np.where(down_avg != 0, up_avg / down_avg, np.inf)
    rsi_arr = 100 - (100 / (1 + rs_arr))

    # å…³é”®ä¿®æ­£ï¼šå°† NumPy æ•°ç»„è½¬æ¢å› Pandas Series
    rsi = pd.Series(rsi_arr, index=series.index)

    lowest_rsi = rsi.rolling(length_stoch).min()
    highest_rsi = rsi.rolling(length_stoch).max()

    denominator = highest_rsi - lowest_rsi

    stoch_rsi_raw = np.where(denominator != 0, (rsi - lowest_rsi) / denominator, 0)
    stoch_rsi_raw = pd.Series(stoch_rsi_raw, index=series.index) * 100

    return stoch_rsi_raw


def calculate_stoch_rsi_signal_and_values(df, length_rsi=14, length_stoch=14, smooth_k=3, smooth_d=3,
                                          oversold_level=20):
    """è®¡ç®— StochRSI K, D å€¼åŠè¶…å–çªç ´ä¹°å…¥ä¿¡å·ã€‚"""
    stoch_rsi_raw = calculate_stoch_rsi_values(df['close'], length_rsi, length_stoch)

    k = pine_sma(stoch_rsi_raw, smooth_k)
    d = pine_sma(k, smooth_d)

    k_crossover_level = (k.shift(1) <= oversold_level) & (k > oversold_level)
    k_gt_d = (k > d)
    buy_signal_raw = k_crossover_level & k_gt_d

    return k.iloc[-1], d.iloc[-1], buy_signal_raw.iloc[-1]


def calculate_atr(df, length=14):
    """
    è®¡ç®— Average True Range (ATR)ï¼Œä½¿ç”¨ RMA (Wilder's Smoothing) å¹³æ»‘ã€‚
    TR = Max[ (H - L), Abs(H - C[1]), Abs(L - C[1]) ]
    ATR = RMA(TR, length)
    """
    df = df.copy()
    high = df['high']
    low = df['low']
    close_prev = df['close'].shift(1)

    # True Range (TR)
    tr1 = high - low
    tr2 = (high - close_prev).abs()
    tr3 = (low - close_prev).abs()

    # max() æ“ä½œä¼šè¿”å› Series
    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

    # ä½¿ç”¨ pine_rma è¿›è¡Œå¹³æ»‘è®¡ç®— ATR
    atr_series = pine_rma(true_range, length)

    return atr_series.iloc[-1]


# ============================================================
# æ¨¡å— 2 - 7 (LogRedirector, äº¤æ˜“æ—¥å†, é‡è¯•è£…é¥°å™¨, è‚¡ç¥¨åˆ—è¡¨, å®æ—¶æ•°æ®)
# æ­¤å¤„çœç•¥ï¼Œä¿æŒä¸ V3.0 å®Œå…¨ä¸€è‡´ï¼Œä»¥èŠ‚çœç¯‡å¹…ã€‚
# (æ³¨æ„ï¼šåœ¨å®é™…ä»£ç æ–‡ä»¶ä¸­ï¼Œæ‚¨åº”è¯¥ä¿ç•™è¿™äº›æ¨¡å—çš„å®Œæ•´ä»£ç )
# ============================================================

class LogRedirector:
    # ... (ä¸ V3.0 ä¿æŒä¸€è‡´)
    MAX_BYTES = 20 * 1024 * 1024

    def __init__(self, folder="stocks"):
        self.today_str = datetime.datetime.now().strftime('%Y%m%d')
        self.log_dir = os.path.join(folder, self.today_str)
        os.makedirs(self.log_dir, exist_ok=True)
        self.terminal = sys.stdout
        self.log_file = None
        self.current_log_path = None
        self.is_active = False

    def _get_new_log_path(self):
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"{timestamp}.log"
        return os.path.join(self.log_dir, log_filename)

    def _check_and_rotate(self):
        if self.log_file is None:
            self.current_log_path = self._get_new_log_path()
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')
            return True
        if os.path.getsize(self.current_log_path) > self.MAX_BYTES:
            self.log_file.write(f"\n[è½®æ¢] æ—¥å¿—è¾¾åˆ° {self.MAX_BYTES / 1024 / 1024:.0f}MB é™åˆ¶ï¼Œæ­£åœ¨åˆ‡æ¢æ–‡ä»¶...\n")
            self.log_file.close()
            self.current_log_path = self._get_new_log_path()
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')
            return True
        return False

    def __enter__(self):
        try:
            self._check_and_rotate()
            sys.stdout = self
            self.is_active = True
            self.write(
                f"\n{'=' * 70}\n[ä¼šè¯å¼€å§‹] {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n[æ—¥å¿—æ–‡ä»¶] {self.current_log_path}\n{'=' * 70}\n")
            return self
        except Exception as e:
            print(f"[é”™è¯¯] æ—¥å¿—ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}", file=self.terminal)
            sys.stdout = self.terminal
            return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.is_active:
            self.write(
                f"\n{'=' * 70}\n[ä¼šè¯ç»“æŸ] {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'=' * 70}\n")
            sys.stdout = self.terminal
            if self.log_file:
                self.log_file.close()
            print(f"æ—¥å¿—æ–‡ä»¶å·²ä¿å­˜è‡³: {self.current_log_path}")

    def write(self, message):
        self.terminal.write(message)
        self.terminal.flush()
        if self.log_file:
            self._check_and_rotate()
            if not message.startswith('\r'):
                self.log_file.write(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] {message}")
            else:
                self.log_file.write(message)
            self.log_file.flush()

    def flush(self):
        self.terminal.flush()
        if self.log_file:
            self.log_file.flush()


_TRADE_CALENDAR = set()


def is_trade_day(date_obj): return date_obj in _TRADE_CALENDAR


def load_trade_calendar():
    global _TRADE_CALENDAR
    _TRADE_CALENDAR.clear()
    try:
        print("[ç³»ç»Ÿ] æ­£åœ¨åŠ è½½äº¤æ˜“æ—¥å†...")
        calendar_df = ak.tool_trade_date_hist_sina()
        if calendar_df.empty or 'trade_date' not in calendar_df.columns:
            raise ValueError("äº¤æ˜“æ—¥å†æ•°æ®ç»“æ„ä¸æ­£ç¡®æˆ–ä¸ºç©ºã€‚")
        trade_dates = calendar_df['trade_date'].tolist()
        for d in trade_dates:
            if isinstance(d, str):
                date_obj = datetime.datetime.strptime(d, '%Y%m%d').date()
            elif isinstance(d, datetime.datetime):
                date_obj = d.date()
            elif isinstance(d, datetime.date):
                date_obj = d
            else:
                continue
            _TRADE_CALENDAR.add(date_obj)
        print(f"[ç³»ç»Ÿ] äº¤æ˜“æ—¥å†åŠ è½½å®Œæˆï¼Œå…± {len(_TRADE_CALENDAR)} ä¸ªäº¤æ˜“æ—¥ã€‚")
    except Exception as e:
        print(f"[è­¦å‘Š] æ— æ³•åŠ è½½äº¤æ˜“æ—¥å†ï¼Œå®æ—¶æ•°æ®è¿½åŠ åŠŸèƒ½å¯èƒ½å¤±æ•ˆ: {e}")


def retry(max_retries=10, delay=15):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == max_retries - 1: raise
                    print(f"[è­¦å‘Š] {func.__name__} å¤±è´¥ ({type(e).__name__})ï¼Œ{delay}s åé‡è¯•...")
                    time.sleep(delay)

        return wrapper

    return decorator


@retry(max_retries=2, delay=1)
def fetch_stock_list_safe():
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_info_a_code_name æ¥å£")
            return df[["code", "name"]]
    except Exception as e:
        print(f"[è­¦å‘Š] è½»é‡æ¥å£å¤±è´¥ ({e})ï¼Œå°è¯•å¤‡ç”¨æ¥å£...")
    try:
        df = ak.stock_zh_a_spot_em()
        print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_zh_a_spot_em æ¥å£")
        if 'ä»£ç ' in df.columns: df = df.rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
        return df[["code", "name"]]
    except Exception as e:
        raise Exception(f"æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨æ¥å£å‡ä¸å¯ç”¨: {e}")


def get_stock_list_manager():
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                print(f"[ç³»ç»Ÿ] åŠ è½½æœ¬æ—¥ç¼“å­˜ï¼Œå…± {len(cache['data'])} æ”¯è‚¡ç¥¨")
                return pd.DataFrame(cache["data"])
        except Exception:
            pass
    df = fetch_stock_list_safe()
    if not df.empty:
        with open(cache_file, "w", encoding="utf-8") as f:
            data = {"time": today_str, "data": df.to_dict(orient="records")}
            json.dump(data, f, ensure_ascii=False, indent=2)
    return df


def filter_stock_list(df):
    if df is None or df.empty: return []
    df["code"] = df["code"].astype(str).str.zfill(6)
    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]: mask |= df["code"].str.startswith(("300", "301"))
    if CONFIG["EXCLUDE_KCB"]: mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]: mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns: mask |= df["name"].str.contains("ST|é€€", na=False)
    return df[~mask]["code"].tolist()


@retry(max_retries=10, delay=15)
def fetch_realtime_snapshot():
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§...")
    try:
        df = ak.stock_zh_a_spot()
    except Exception as e:
        print(f"[é”™è¯¯] è·å–å®æ—¶å¿«ç…§å¤±è´¥: {e}")
        return pd.DataFrame()
    df = df.rename(
        columns={'ä»£ç ': 'code', 'æœ€æ–°ä»·': 'close', 'æˆäº¤é‡': 'volume', 'ä»Šå¼€': 'open', 'æœ€é«˜': 'high', 'æœ€ä½': 'low',
                 'æˆäº¤é¢': 'amount', 'é‡‘é¢': 'amount', })
    if 'code' in df.columns:
        df['code'] = df['code'].astype(str).str.replace(r'\D', '', regex=True)
        df['code'] = df['code'].str.zfill(6)
    else:
        return pd.DataFrame()
    required_cols = ['code', 'open', 'high', 'low', 'close', 'volume', 'amount']
    for col in required_cols:
        if col not in df.columns: df[col] = np.nan
    df = df[required_cols]
    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
    for col in numeric_cols: df[col] = pd.to_numeric(df[col], errors='coerce')
    print(f"[ç³»ç»Ÿ] æˆåŠŸè·å– {len(df)} æ¡å®æ—¶å¿«ç…§æ•°æ®ã€‚")
    return df


def append_today_realtime_snapshot(code: str, df_daily: pd.DataFrame, df_spot: pd.DataFrame) -> pd.DataFrame:
    latest_date = datetime.datetime.now().date()
    if not is_trade_day(latest_date): return df_daily
    spot_row = df_spot[df_spot['code'] == code]
    if spot_row.empty: return df_daily
    latest_data = spot_row.iloc[0]
    if not df_daily.empty:
        df_daily_dates = pd.to_datetime(df_daily['date']).dt.date
        last_history_date = df_daily_dates.iloc[-1]
        if last_history_date == latest_date: return df_daily
        if last_history_date > latest_date: return df_daily

    new_row_data = {
        'date': latest_date, 'open': latest_data.get('open'), 'high': latest_data.get('high'),
        'low': latest_data.get('low'), 'close': latest_data.get('close'),
        'volume': latest_data.get('volume'), 'amount': latest_data.get('amount'),
        'outstanding_share': None, 'turnover': None, 'adjust': CONFIG.get("ADJUST", ""), 'code': code,
    }
    try:
        df_new_day = pd.DataFrame([new_row_data], columns=df_daily.columns)
    except ValueError:
        df_new_day = pd.DataFrame([new_row_data])
    df_final = pd.concat([df_daily, df_new_day], ignore_index=True)
    return df_final


def fetch_data_with_timeout(symbol, start_date, end_date, adjust, timeout):
    def _fetch():
        if CONFIG["USE_LOCAL_MYSQL"]:
            try:
                return stock_zh_a_daily_mysql(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)
            except NameError:
                print(f"\n[é”™è¯¯] {symbol} å°è¯•ä½¿ç”¨ MySQL æ¥å£å¤±è´¥ (NameError)ï¼Œè‡ªåŠ¨é™çº§åˆ° AkShareã€‚\n")
            except Exception as e:
                print(f"\n[é”™è¯¯] {symbol} å°è¯•ä½¿ç”¨ MySQL æ¥å£å¤±è´¥ ({type(e).__name__})ï¼Œè‡ªåŠ¨é™çº§åˆ° AkShareã€‚\n")
        return ak.stock_zh_a_daily(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)

    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(_fetch)
        try:
            done, not_done = wait([future], timeout=timeout)
            if future in done:
                return future.result()
            elif future in not_done:
                future.cancel()
                raise ThreadingTimeoutError(f"è¯·æ±‚è¶…æ—¶ ({timeout}s)")
        except Exception as e:
            raise e


# ============================================================
# æ¨¡å— 8ï¼šå•åªè‚¡ç¥¨ç­–ç•¥ (å¢åŠ  ATR æ­¢ç›ˆæ­¢æŸè®¡ç®—)
# ============================================================
def strategy_single_stock(code, start_date, end_date, df_spot):
    symbol = f"sh{code}" if code.startswith("6") else f"sz{code}"

    try:
        df = fetch_data_with_timeout(symbol=symbol, start_date=start_date, end_date=end_date, adjust=CONFIG["ADJUST"],
                                     timeout=CONFIG["REQUEST_TIMEOUT"])

        # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿè®¡ç®— EMA200/ATR (è‡³å°‘ 200 + 1)
        # ATR 14 éœ€è¦ 14 ä¸ªæ•°æ®ç‚¹æ¥å¹³æ»‘ï¼Œä½†å®‰å…¨èµ·è§ä»ä½¿ç”¨ 220
        if df is None or df.empty or len(df) < 220: return None

        # è°ƒç”¨å®æ—¶è‚¡ç¥¨è¡Œæƒ…æ‹¼æ¥æ¥å£
        if CONFIG["USE_REAL_TIME_DATA"]:
            df = append_today_realtime_snapshot(code, df, df_spot)

        # ç¡®ä¿æ•°æ®é•¿åº¦åœ¨æ‹¼æ¥åä»è¶³å¤Ÿ
        if len(df) < 220: return None

        df['date'] = pd.to_datetime(df['date']).dt.date
        df = df.sort_values('date').reset_index(drop=True)
        current_close = df['close'].iloc[-1]

        # --- ç­–ç•¥æ ¸å¿ƒè®¡ç®— ---

        # 1. è®¡ç®— StochRSI ä¿¡å·
        k_val, d_val, stoch_rsi_buy_signal = calculate_stoch_rsi_signal_and_values(
            df,
            length_rsi=CONFIG["STOCH_RSI"]["lengthRSI"],
            length_stoch=CONFIG["STOCH_RSI"]["lengthStoch"],
            smooth_k=CONFIG["STOCH_RSI"]["smoothK"],
            smooth_d=CONFIG["STOCH_RSI"]["smoothD"],
            oversold_level=CONFIG["STOCH_RSI"]["oversoldLevel"]
        )

        if not stoch_rsi_buy_signal: return None

        # 2. è®¡ç®— EMA è¶‹åŠ¿
        ema50 = pine_ema(df['close'], 50).iloc[-1]
        ema200 = pine_ema(df['close'], 200).iloc[-1]

        # 3. è¶‹åŠ¿è¿‡æ»¤æ¡ä»¶: close > EMA50 > EMA200
        trend_filter = (current_close > ema50) and (ema50 > ema200)

        if not trend_filter: return None

        # 4. ğŸ†• è®¡ç®— ATR åŠæ­¢ç›ˆæ­¢æŸä½
        atr_length = CONFIG["ATR_SETTING"]["lengthATR"]
        sl_mult = CONFIG["ATR_SETTING"]["stop_loss_multiplier"]
        tp_mult = CONFIG["ATR_SETTING"]["take_profit_multiplier"]

        current_atr = calculate_atr(df, length=atr_length)

        # ä»·æ ¼å¤§äº 5 å…ƒæ‰è®¡ç®— ATR æ­¢æŸï¼Œé˜²æ­¢åˆ†æ¯å¤ªå°
        if current_close < 5.0 and current_close > 0:
            current_atr = 0  # å°äº 5 å…ƒçš„è‚¡ç¥¨æš‚ä¸è¿›è¡Œ ATR æ­¢æŸè®¡ç®—

        # å‡è®¾ä¿¡å·å‘ç”Ÿæ—¶çš„ä¹°å…¥ä»·å°±æ˜¯å½“å‰ä»· current_close
        stop_loss_price = current_close - (sl_mult * current_atr)
        take_profit_price = current_close + (tp_mult * current_atr)

        # --- æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼Œæ„å»ºè¿”å›ç»“æœ ---

        pct_chg = (current_close / df['close'].iloc[-2] - 1) * 100 if len(df) >= 2 else 0

        return {
            "ä»£ç ": code,
            "æ—¥æœŸ": df['date'].iloc[-1].strftime('%Y-%m-%d'),
            "ä¿¡å·": 'StochRSI/EMA Buy',
            "å½“å‰ä»·": round(current_close, 2),
            "æ¶¨å¹…%": round(pct_chg, 2),
            "StochK": round(float(k_val), 2),
            "StochD": round(float(d_val), 2),
            "EMA50": round(float(ema50), 2),
            "EMA200": round(float(ema200), 2),
            "ATR_14": round(float(current_atr), 3),
            "æ­¢æŸä»·": round(stop_loss_price, 2),
            "æ­¢ç›ˆä»·": round(take_profit_price, 2),
            "è¶‹åŠ¿è¿‡æ»¤": "æ»¡è¶³ (C>E50>E200)",
        }

    except ThreadingTimeoutError:
        print(f"[è¶…æ—¶] {code} è¯·æ±‚å†å²æ•°æ®è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return None

    except Exception as e:
        print(f"[é”™è¯¯] {code} å¤„ç†å¤±è´¥: {type(e).__name__}: {e}")
        return None


# ============================================================
# æ¨¡å— 9 & 10ï¼šå¹¶å‘æ‰«æ & ä¸»å…¥å£ (ä»…æ›´æ–°è¾“å‡ºåˆ—)
# ============================================================
async def main_scanner_async(stock_codes, df_spot):
    # ... (ä¿æŒä¸å˜)
    end_date = datetime.datetime.now().strftime("%Y%m%d")
    start_date = (datetime.datetime.now() - datetime.timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
    results = []
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:
        tasks = [
            loop.run_in_executor(pool, strategy_single_stock, code, start_date, end_date, df_spot)
            for code in stock_codes
        ]
        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")
        for coro in pbar:
            res = await coro
            if res:
                results.append(res)
                pbar.set_postfix({"å‘½ä¸­": len(results)})
    return results


async def batch_scan_manager_async(target_codes, df_spot):
    # ... (ä¿æŒä¸å˜)
    all_results = []
    batch_size = CONFIG.get("BATCH_SIZE", 500)
    interval = CONFIG.get("BATCH_INTERVAL_SEC", 2)
    total_stocks = len(target_codes)
    num_batches = math.ceil(total_stocks / batch_size)

    print(f"\n[è°ƒåº¦å™¨] æ€»è®¡ {total_stocks} æ”¯è‚¡ç¥¨ï¼Œå°†åˆ†ä¸º {num_batches} æ‰¹æ¬¡ ({batch_size} æ”¯/æ‰¹)ã€‚")
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = min((i + 1) * batch_size, total_stocks)
        batch_codes = target_codes[start_index:end_index]
        print("\n" + "=" * 60)
        print(f"--- ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i + 1}/{num_batches} ({len(batch_codes)} æ”¯) ---")
        batch_results = await main_scanner_async(batch_codes, df_spot)
        all_results.extend(batch_results)
        if i < num_batches - 1:
            print(f"--- ğŸ˜´ æ‰¹æ¬¡ {i + 1} å®Œæˆï¼Œå½“å‰æ€»å‘½ä¸­: {len(all_results)}ï¼Œä¼‘æ¯ {interval} ç§’... ---")
            time.sleep(interval)
        else:
            print("--- ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚---")
    return all_results


def main():
    start_time = time.time()
    if CONFIG["USE_REAL_TIME_DATA"]: load_trade_calendar()

    with LogRedirector(folder=CONFIG['OUTPUT_LOG']) as log_redirector:
        end_date = datetime.datetime.now().strftime("%Y%m%d")
        start_date = (datetime.datetime.now() - datetime.timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
        print(f"\n[ä»»åŠ¡å¯åŠ¨] æ‰«æèŒƒå›´: {start_date} ~ {end_date}")
        print(f"[é…ç½®] ç›®æ ‡çº¿ç¨‹: {CONFIG['MAX_WORKERS']} | è¶…æ—¶: {CONFIG['REQUEST_TIMEOUT']}s")

        df_spot = pd.DataFrame()
        if CONFIG["USE_REAL_TIME_DATA"]:
            try:
                today_date = datetime.datetime.now().date()
                if is_trade_day(today_date):
                    df_spot = fetch_realtime_snapshot()
                    if df_spot.empty:
                        print("[ç»ˆæ­¢] æ— æ³•è·å–å®æ—¶è¡Œæƒ…å¿«ç…§ï¼Œç»ˆæ­¢æ‰«æã€‚")
                        sys.exit(1)
                else:
                    print("[é…ç½®] å½“å‰ä¸ºéäº¤æ˜“æ—¥ï¼Œè·³è¿‡å®æ—¶å¿«ç…§è·å–ã€‚")
            except Exception as e:
                print(f"[è‡´å‘½ç»ˆæ­¢] è·å–å®æ—¶è¡Œæƒ…å¿«ç…§å¤±è´¥: {e}")
                sys.exit(1)
        else:
            print("[é…ç½®] å®æ—¶æ•°æ®è·å–å¼€å…³å…³é—­ (USE_REAL_TIME_DATA=False)ï¼Œè·³è¿‡å…¨å¸‚åœºå¿«ç…§è·å–ã€‚")

        manual_list = CONFIG["MANUAL_STOCK_LIST"]
        df_base = pd.DataFrame()
        target_codes = []

        if manual_list and len(manual_list) > 0:
            target_codes = [str(c).zfill(6) for c in manual_list]
            print(f"[æ‰‹åŠ¨æ¨¡å¼] ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥åˆ—è¡¨ï¼Œå…± {len(target_codes)} æ”¯è‚¡ç¥¨ã€‚")
            try:
                df_base = get_stock_list_manager()
            except Exception:
                df_base = pd.DataFrame({"code": target_codes, "name": ["æœªçŸ¥"] * len(target_codes)})
        else:
            try:
                df_base = get_stock_list_manager()
            except Exception as e:
                print(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}")
                return
            valid_codes = filter_stock_list(df_base)
            sample_size = CONFIG["SAMPLE_SIZE"]
            if isinstance(sample_size, int) and sample_size > 0 and len(valid_codes) > sample_size:
                print(f"[æŠ½æ ·æ¨¡å¼] éšæœºæŠ½å– {sample_size} æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•...")
                target_codes = random.sample(valid_codes, sample_size)
            else:
                print(f"[å…¨é‡æ¨¡å¼] æ‰«ææ‰€æœ‰ {len(valid_codes)} æ”¯æœ‰æ•ˆè‚¡ç¥¨...")
                target_codes = valid_codes

        if CONFIG["SAMPLE_SIZE"] > 0 or len(target_codes) <= CONFIG["BATCH_SIZE"]:
            final_data = asyncio.run(main_scanner_async(target_codes, df_spot))
        else:
            final_data = asyncio.run(batch_scan_manager_async(target_codes, df_spot))

        # 4. ç»“æœæ•´ç†ä¸å¯¼å‡º
        if final_data:
            res_df = pd.DataFrame(final_data)

            name_map = df_base.set_index('code')['name'].to_dict()
            res_df['åç§°'] = res_df['ä»£ç '].map(name_map).fillna('æœªçŸ¥')

            today_date_str = datetime.datetime.now().strftime('%Y-%m-%d')
            folder_path = os.path.join(CONFIG["OUTPUT_FOLDER_BASE"], today_date_str)
            os.makedirs(folder_path, exist_ok=True)
            timestamp = datetime.datetime.now().strftime('%H%M%S')
            file_name = f"{CONFIG['OUTPUT_FILENAME_BASE']}_{timestamp}.csv"
            full_file_path = os.path.join(folder_path, file_name)

            # ğŸ†• é‡æ–°æ’åºç»“æœåˆ— (æ–°å¢ ATR, æ­¢æŸä»·, æ­¢ç›ˆä»·)
            ordered_cols = ["æ—¥æœŸ", "ä»£ç ", "åç§°", "å½“å‰ä»·", "æ¶¨å¹…%", "æ­¢æŸä»·", "æ­¢ç›ˆä»·", "ATR_14"]
            res_df = res_df[ordered_cols]

            res_df = res_df.sort_values(["æ¶¨å¹…%"], ascending=[False]).reset_index(drop=True)


            res_df.to_csv(full_file_path, index=False, encoding=CONFIG["EXPORT_ENCODING"])

            print("\n" + "=" * 60)
            print(f"âœ… æ‰«æå®Œæˆ | è€—æ—¶: {time.time() - start_time:.1f}s")
            print(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {full_file_path}")
            print(f"ğŸ“ˆ å‘½ä¸­æ•°é‡: {len(res_df)}")
            print("=" * 60)
            print("--- å‘½ä¸­è‚¡ç¥¨ Top 10 ---")
            print(res_df.head(10).to_string(index=False))
            return res_df

        else:
            print("\n[ç»“æœ] æ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
            return pd.DataFrame()


if __name__ == "__main__":
    main()