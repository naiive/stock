#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡çªç ´æ‰«æç³»ç»Ÿï¼ˆStochRSI + EMA è¶‹åŠ¿è¿‡æ»¤ï¼‰
ç‰ˆæœ¬ï¼šv3.0 (ä¿®å¤ numpy.ndarray é”™è¯¯)

ã€æ ¸å¿ƒç­–ç•¥ã€‘
1. StochRSI ä¿¡å·: K çº¿ä¸Šç©¿è¶…å–æ°´å¹³ (é»˜è®¤ 20)ï¼Œä¸” K > D (é‡‘å‰)ã€‚
2. è¶‹åŠ¿è¿‡æ»¤: å½“å‰æ”¶ç›˜ä»· > EMA50 > EMA200 (å¼ºåŠ¿ä¸Šæ¶¨è¶‹åŠ¿)ã€‚
============================================================
"""
import os
import sys
import json
import time
import random
import math
import datetime
from concurrent.futures import ThreadPoolExecutor, wait, TimeoutError as ThreadingTimeoutError

import pandas as pd
import numpy as np
import akshare as ak
import asyncio
from tqdm import tqdm

# --- å¯¼å…¥äº¤æ˜“æ•°æ®æ¥å£ (ä¿æŒä¸å˜) ---
try:
    # å°è¯•ä»æœ¬åœ°å¯¼å…¥è‡ªå®šä¹‰æ•°æ®æ¥å£
    from api.stock_query import stock_zh_a_daily_mysql
except ImportError:
    # è­¦å‘Šï¼šå¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ™å®šä¹‰ä¸€ä¸ªæ›¿ä»£å‡½æ•°ï¼Œå¹¶å…è®¸é™çº§åˆ° AkShare
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ stock_zh_a_daily_mysqlã€‚è¯·ç¡®ä¿æ‚¨çš„ stock_query.py æ–‡ä»¶å­˜åœ¨ã€‚")


    def stock_zh_a_daily_mysql(*args, **kwargs):
        raise NameError("stock_zh_a_daily_mysql å°šæœªå®šä¹‰æˆ–å¯¼å…¥å¤±è´¥ã€‚")


# ============================================================
# æ¨¡å— Aï¼šPine Script æ ¸å¿ƒå¹³æ»‘å‡½æ•° (StochRSI/EMA è®¡ç®—åŸºç¡€)
# **ã€ä¿®å¤ 1ï¼šå¢åŠ  Series ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ã€‘**
# ============================================================
def pine_rma(series, length):
    """ RMA (Wilder's Smoothing) - ç”¨äºç²¾ç¡® RSI è®¡ç®— """
    # å¼ºåˆ¶è½¬æ¢ä¸º Series ä»¥ç¡®ä¿ .ewm() å¯ç”¨
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    alpha = 1 / length
    return series.ewm(alpha=alpha, adjust=False).mean()


def pine_sma(series, length):
    """ Simple Moving Average (SMA) - ç”¨äºç²¾ç¡® StochRSI K/D å¹³æ»‘ """
    # å¼ºåˆ¶è½¬æ¢ä¸º Series ä»¥ç¡®ä¿ .rolling() å¯ç”¨
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    return series.rolling(length).mean()


def pine_ema(series, length):
    """ EMA (Exponential Moving Average) - ç”¨äºç²¾ç¡® EMA 50/200 è¶‹åŠ¿è¿‡æ»¤ """
    # å¼ºåˆ¶è½¬æ¢ä¸º Series ä»¥ç¡®ä¿ .ewm() å¯ç”¨
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    alpha = 2 / (length + 1)
    return series.ewm(alpha=alpha, adjust=False).mean()


# ============================================================
# æ¨¡å— Bï¼šStochRSI æ ¸å¿ƒè®¡ç®—
# **ã€ä¿®å¤ 2ï¼šå°† NumPy æ•°ç»„è®¡ç®—ç»“æœè½¬æ¢å› Pandas Seriesã€‘**
# ============================================================
def calculate_stoch_rsi_values(series, length_rsi, length_stoch):
    """è®¡ç®— StochRSI çš„åŸå§‹ K å€¼ (æœªå¹³æ»‘)"""
    # å¼ºåˆ¶è¾“å…¥ä¸º Seriesï¼Œå¹¶ä¿æŒç´¢å¼•
    if not isinstance(series, pd.Series):
        series = pd.Series(series)

    # 1. è®¡ç®— RSI: ä½¿ç”¨ RMA æ¨¡æ‹Ÿ ta.rsi å†…éƒ¨çš„å¹³æ»‘é€»è¾‘
    delta = series.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)

    up_avg = pine_rma(up, length_rsi)
    down_avg = pine_rma(down, length_rsi)

    # é¿å…é™¤ä»¥é›¶å¯¼è‡´è­¦å‘Š
    rs_arr = np.where(down_avg != 0, up_avg / down_avg, np.inf)
    rsi_arr = 100 - (100 / (1 + rs_arr))

    # ï¼ï¼ï¼å…³é”®ä¿®æ­£ï¼ï¼ï¼ï¼šå°† NumPy æ•°ç»„è½¬æ¢å› Pandas Seriesï¼Œå¹¶æºå¸¦åŸå§‹ç´¢å¼•
    rsi = pd.Series(rsi_arr, index=series.index)

    # 2. è®¡ç®— StochRSI
    lowest_rsi = rsi.rolling(length_stoch).min()
    highest_rsi = rsi.rolling(length_stoch).max()

    denominator = highest_rsi - lowest_rsi

    # è¿™é‡Œçš„ stoch_rsi_raw å·²ç»æ˜¯ Series
    stoch_rsi_raw = np.where(denominator != 0, (rsi - lowest_rsi) / denominator, 0)
    stoch_rsi_raw = pd.Series(stoch_rsi_raw, index=series.index) * 100

    return stoch_rsi_raw


def calculate_stoch_rsi_signal_and_values(df, length_rsi=14, length_stoch=14, smooth_k=3, smooth_d=3,
                                          oversold_level=20):
    """
    è®¡ç®— StochRSI K, D å€¼åŠè¶…å–çªç ´ä¹°å…¥ä¿¡å·ã€‚
    è¿”å›å½“å‰æœ€æ–°çš„ K å€¼, D å€¼, ä»¥åŠå¸ƒå°”å‹ä¿¡å·ã€‚
    """
    stoch_rsi_raw = calculate_stoch_rsi_values(df['close'], length_rsi, length_stoch)

    k = pine_sma(stoch_rsi_raw, smooth_k)
    d = pine_sma(k, smooth_d)

    # æ ¸å¿ƒä¿¡å·é€»è¾‘ (é‡‘å‰å’Œè¶…å–åŒºçªç ´)
    k_crossover_level = (k.shift(1) <= oversold_level) & (k > oversold_level)
    k_gt_d = (k > d)
    buy_signal_raw = k_crossover_level & k_gt_d

    # è¿”å›æœ€æ–°çš„ K å€¼, D å€¼å’Œä¿¡å· (å¸ƒå°”å€¼)
    # iloc[-1] ç¡®ä¿è¿”å›çš„æ˜¯æ•°å€¼è€Œé Series
    return k.iloc[-1], d.iloc[-1], buy_signal_raw.iloc[-1]


# ============================================================
# æ¨¡å— 1ï¼šé…ç½® (Configuration) (ä¿æŒä¸å˜)
# ============================================================
CONFIG = {
    # --- ğŸ†• æ—¶é—´èŒƒå›´ ---
    "DAYS": 365,  # æ‰«æå›æº¯å¤©æ•° (ç”¨äºè®¡ç®— MA200/EMA200)

    # --- ğŸ†• è¿‡æ»¤æ¡ä»¶ ---
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ã€301ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ã€689ï¼‰
    "EXCLUDE_BJ": True,  # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ã€92ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",  # å¤æƒæ–¹å¼

    # --- ğŸ†• StochRSI å‚æ•° ---
    "STOCH_RSI": {
        "lengthRSI": 14,
        "lengthStoch": 14,
        "smoothK": 3,
        "smoothD": 3,
        "oversoldLevel": 20
    },

    # --- ğŸ†• æ–‡ä»¶è·¯å¾„/åç§° ---
    "CACHE_FILE": "../conf/stock_list_cache.json",
    "EXPORT_ENCODING": "utf-8-sig",  # CSVæ–‡ä»¶å¯¼å‡ºç¼–ç 
    "OUTPUT_FILENAME_BASE": "Buy_Stocks_StochRSI_EMA",  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
    "OUTPUT_FOLDER_BASE": "../stocks",  # csvè¾“å‡º æ–‡ä»¶å¤¹
    "OUTPUT_LOG": "../logs",  # LogRedirector æ—¥å¿—è¾“å‡ºæ–‡ä»¶å¤¹

    # --- ğŸ†• å¹¶å‘ ---
    "MAX_WORKERS": 10,  # é™ä½çº¿ç¨‹æ•°åˆ° 10
    "REQUEST_TIMEOUT": 20,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 20s

    # --- ğŸ†• æ•°æ®æºæ§åˆ¶ ---
    "USE_LOCAL_MYSQL": True,

    # --- ğŸ†• å®æ—¶æ•°æ®å¼€å…³ ---
    "USE_REAL_TIME_DATA": False,

    # --- ğŸ†• æ˜¯å¦å…¨é‡/åˆ†æ‰¹æ§åˆ¶ ---
    "SAMPLE_SIZE": 0,  # 0 æˆ– None è¡¨ç¤ºå…¨é‡
    "BATCH_SIZE": 1000,  # SAMPLE_SIZE å…¨é‡æ‰å¼€å¯åˆ†æ‰¹åŠŸèƒ½ï¼Œæ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
    "BATCH_INTERVAL_SEC": 1,  # æ‰¹æ¬¡é—´éš”ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰

    # --- ğŸ†• æ‰‹åŠ¨è¾“å…¥ ---
    "MANUAL_STOCK_LIST": []
}


# ============================================================
# æ¨¡å— 2ï¼šæ—¥å¿—é‡å®šå‘ç±» (LogRedirector) (ä¿æŒä¸å˜)
# ============================================================
class LogRedirector:
    """
    å°† sys.stdout çš„è¾“å‡ºåŒæ—¶é‡å®šå‘åˆ°ç»ˆç«¯å’Œæ—¥å¿—æ–‡ä»¶ï¼Œå¹¶å®ç°æŒ‰å¤§å°è½®æ¢ï¼ˆä¿ç•™æ—§æ–‡ä»¶ï¼‰ã€‚
    """
    # 20MB è½®æ¢é™åˆ¶
    MAX_BYTES = 20 * 1024 * 1024

    def __init__(self, folder="stocks"):
        # æ—¥å¿—è·¯å¾„: stocks/logs/YYYYMMDD/
        self.today_str = datetime.datetime.now().strftime('%Y%m%d')
        self.log_dir = os.path.join(folder, self.today_str)
        os.makedirs(self.log_dir, exist_ok=True)

        self.terminal = sys.stdout
        self.log_file = None
        self.current_log_path = None
        self.is_active = False

    def _get_new_log_path(self):
        """ç”Ÿæˆæ–°çš„æ—¥å¿—æ–‡ä»¶åï¼šYYYYMMDD_HHMMSS.log"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"{timestamp}.log"
        return os.path.join(self.log_dir, log_filename)

    def _check_and_rotate(self):
        """æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœè¶…è¿‡é™åˆ¶åˆ™å…³é—­æ—§æ–‡ä»¶å¹¶åˆ›å»ºæ–°æ–‡ä»¶ã€‚"""
        if self.log_file is None:
            # é¦–æ¬¡åˆ›å»º
            self.current_log_path = self._get_new_log_path()
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')
            return True

        # æ£€æŸ¥å¤§å°
        if os.path.getsize(self.current_log_path) > self.MAX_BYTES:
            self.log_file.write(f"\n[è½®æ¢] æ—¥å¿—è¾¾åˆ° {self.MAX_BYTES / 1024 / 1024:.0f}MB é™åˆ¶ï¼Œæ­£åœ¨åˆ‡æ¢æ–‡ä»¶...\n")
            self.log_file.close()  # <--- å…³é—­æ—§æ–‡ä»¶ï¼Œä¿ç•™åœ¨ç£ç›˜ä¸Š

            # åˆ›å»ºæ–°æ–‡ä»¶
            self.current_log_path = self._get_new_log_path()  # <--- ç”Ÿæˆå…¨æ–°çš„ã€ä¸é‡å¤çš„æ–‡ä»¶å
            self.log_file = open(self.current_log_path, 'a', encoding='utf-8')  # <--- æ‰“å¼€æ–°æ–‡ä»¶ç»§ç»­å†™å…¥
            return True

        return False

    def __enter__(self):
        try:
            self._check_and_rotate()  # é¦–æ¬¡åˆ›å»ºæ—¥å¿—æ–‡ä»¶
            sys.stdout = self
            self.is_active = True
            self.write(
                f"\n{'=' * 70}\n[ä¼šè¯å¼€å§‹] {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n[æ—¥å¿—æ–‡ä»¶] {self.current_log_path}\n{'=' * 70}\n")
            return self
        except Exception as e:
            print(f"[é”™è¯¯] æ—¥å¿—ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}", file=self.terminal)
            sys.stdout = self.terminal
            return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.is_active:
            self.write(
                f"\n{'=' * 70}\n[ä¼šè¯ç»“æŸ] {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'=' * 70}\n")
            sys.stdout = self.terminal
            if self.log_file:
                self.log_file.close()
            print(f"æ—¥å¿—æ–‡ä»¶å·²ä¿å­˜è‡³: {self.current_log_path}")

    def write(self, message):
        self.terminal.write(message)
        self.terminal.flush()

        if self.log_file:
            self._check_and_rotate()  # å†™å…¥å‰æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢
            if not message.startswith('\r'):
                self.log_file.write(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] {message}")
            else:
                self.log_file.write(message)
            self.log_file.flush()

    def flush(self):
        self.terminal.flush()
        if self.log_file:
            self.log_file.flush()


# ============================================================
# æ¨¡å— 3ï¼šäº¤æ˜“æ—¥å†ç›¸å…³çš„å…¨å±€å˜é‡å’Œå‡½æ•° (ä¿æŒä¸å˜)
# ============================================================
_TRADE_CALENDAR = set()


def is_trade_day(date_obj):
    """æ£€æŸ¥ç»™å®šçš„æ—¥æœŸæ˜¯å¦åœ¨å·²åŠ è½½çš„äº¤æ˜“æ—¥å†ä¸­"""
    return date_obj in _TRADE_CALENDAR


def load_trade_calendar():
    """
    åŠ è½½æœ€è¿‘å‡ å¹´çš„äº¤æ˜“æ—¥å†åˆ°å…¨å±€é›†åˆä¸­ã€‚
    """
    global _TRADE_CALENDAR
    _TRADE_CALENDAR.clear()  # æ¸…ç©ºæ—§çš„æ—¥å†é›†åˆ

    try:
        print("[ç³»ç»Ÿ] æ­£åœ¨åŠ è½½äº¤æ˜“æ—¥å†...")

        calendar_df = ak.tool_trade_date_hist_sina()

        if calendar_df.empty or 'trade_date' not in calendar_df.columns:
            raise ValueError("äº¤æ˜“æ—¥å†æ•°æ®ç»“æ„ä¸æ­£ç¡®æˆ–ä¸ºç©ºã€‚")

        trade_dates = calendar_df['trade_date'].tolist()

        # é˜²å¾¡æ€§å¤„ç†æ—¥æœŸç±»å‹
        for d in trade_dates:
            if isinstance(d, str):
                date_obj = datetime.datetime.strptime(d, '%Y%m%d').date()
            elif isinstance(d, datetime.datetime):
                date_obj = d.date()
            elif isinstance(d, datetime.date):
                date_obj = d
            else:
                continue

            _TRADE_CALENDAR.add(date_obj)

        print(f"[ç³»ç»Ÿ] äº¤æ˜“æ—¥å†åŠ è½½å®Œæˆï¼Œå…± {len(_TRADE_CALENDAR)} ä¸ªäº¤æ˜“æ—¥ã€‚")

    except Exception as e:
        print(f"[è­¦å‘Š] æ— æ³•åŠ è½½äº¤æ˜“æ—¥å†ï¼Œå®æ—¶æ•°æ®è¿½åŠ åŠŸèƒ½å¯èƒ½å¤±æ•ˆ: {e}")


# ============================================================
# å·¥å…·ï¼šé‡è¯•è£…é¥°å™¨ (Retry Decorator) (ä¿æŒä¸å˜)
# ============================================================
def retry(max_retries=10, delay=15):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == max_retries - 1:
                        raise
                    print(f"[è­¦å‘Š] {func.__name__} å¤±è´¥ ({type(e).__name__})ï¼Œ{delay}s åé‡è¯•...")
                    time.sleep(delay)

        return wrapper

    return decorator


# ============================================================
# æ¨¡å— 4ï¼šè·å–/ç¼“å­˜ å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ (ä¿æŒä¸å˜)
# ============================================================
@retry(max_retries=2, delay=1)
def fetch_stock_list_safe():
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_info_a_code_name æ¥å£")
            return df[["code", "name"]]
    except Exception as e:
        print(f"[è­¦å‘Š] è½»é‡æ¥å£å¤±è´¥ ({e})ï¼Œå°è¯•å¤‡ç”¨æ¥å£...")
    try:
        df = ak.stock_zh_a_spot_em()
        print("[ç³»ç»Ÿ] æˆåŠŸ: ä½¿ç”¨ stock_zh_a_spot_em æ¥å£")
        if 'ä»£ç ' in df.columns:
            df = df.rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
        return df[["code", "name"]]
    except Exception as e:
        raise Exception(f"æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨æ¥å£å‡ä¸å¯ç”¨: {e}")


def get_stock_list_manager():
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                print(f"[ç³»ç»Ÿ] åŠ è½½æœ¬æ—¥ç¼“å­˜ï¼Œå…± {len(cache['data'])} æ”¯è‚¡ç¥¨")
                return pd.DataFrame(cache["data"])
        except Exception:
            pass

    df = fetch_stock_list_safe()
    if not df.empty:
        with open(cache_file, "w", encoding="utf-8") as f:
            data = {
                "time": today_str,
                "data": df.to_dict(orient="records")
            }
            json.dump(data, f, ensure_ascii=False, indent=2)

    return df


def filter_stock_list(df):
    if df is None or df.empty:
        return []
    df["code"] = df["code"].astype(str).str.zfill(6)
    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]:
        mask |= df["code"].str.startswith(("300", "301"))
    if CONFIG["EXCLUDE_KCB"]:
        mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]:
        mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns:
        mask |= df["name"].str.contains("ST|é€€", na=False)
    return df[~mask]["code"].tolist()


# ============================================================
# æ¨¡å— 7ï¼šä»Šæ—¥å®æ—¶Kè¡¥å…… (ä¿æŒä¸å˜)
# ============================================================
@retry(max_retries=10, delay=15)
def fetch_realtime_snapshot():
    """
    è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§ (akshare)ï¼Œå¹¶æ ‡å‡†åŒ–åˆ—åå’Œä»£ç æ ¼å¼ã€‚
    """
    print("[ç³»ç»Ÿ] æ­£åœ¨å°è¯•è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§...")
    try:
        df = ak.stock_zh_a_spot()
    except Exception as e:
        print(f"[é”™è¯¯] è·å–å®æ—¶å¿«ç…§å¤±è´¥: {e}")
        return pd.DataFrame()

    # --- å…³é”®ä¿®æ­£ 1ï¼šç»Ÿä¸€åˆ—åæ˜ å°„ ---
    df = df.rename(columns={
        'ä»£ç ': 'code',
        'æœ€æ–°ä»·': 'close',
        'æˆäº¤é‡': 'volume',
        'ä»Šå¼€': 'open',
        'æœ€é«˜': 'high',
        'æœ€ä½': 'low',
        'æˆäº¤é¢': 'amount',
        'é‡‘é¢': 'amount',
    })

    # å¼ºåˆ¶è½¬æ¢ä¸ºçº¯æ•°å­— code
    if 'code' in df.columns:
        df['code'] = df['code'].astype(str).str.replace(r'\D', '', regex=True)
        df['code'] = df['code'].str.zfill(6)
    else:
        print("[è­¦å‘Š] å®æ—¶å¿«ç…§æ•°æ®ä¸­ç¼ºå°‘ 'ä»£ç '/'code' åˆ—ã€‚")
        return pd.DataFrame()

    # --- åç»­åˆ—ç­›é€‰ã€å¡«å……å’Œç±»å‹è½¬æ¢ä¸å˜ ---
    required_cols = ['code', 'open', 'high', 'low', 'close', 'volume', 'amount']

    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    df = df[required_cols]

    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    print(f"[ç³»ç»Ÿ] æˆåŠŸè·å– {len(df)} æ¡å®æ—¶å¿«ç…§æ•°æ®ã€‚")
    return df


def append_today_realtime_snapshot(code: str, df_daily: pd.DataFrame, df_spot: pd.DataFrame) -> pd.DataFrame:
    """
    å°†å®æ—¶è¡Œæƒ…å¿«ç…§ä½œä¸ºå½“æ—¥ K çº¿è¿½åŠ åˆ°å†å²æ—¥çº¿æ•°æ®ä¸­ã€‚
    """

    latest_date = datetime.datetime.now().date()

    if not is_trade_day(latest_date):
        # print(f"[{code}] è°ƒè¯•: {latest_date} ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè·³è¿‡æ‹¼æ¥ã€‚")
        return df_daily

    spot_row = df_spot[df_spot['code'] == code]
    if spot_row.empty:
        # print(f"[{code}] è°ƒè¯•: å®æ—¶å¿«ç…§ df_spot ä¸­æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨æ•°æ®ã€‚")
        return df_daily

    latest_data = spot_row.iloc[0]

    if not df_daily.empty:
        df_daily_dates = pd.to_datetime(df_daily['date']).dt.date
        last_history_date = df_daily_dates.iloc[-1]

        if last_history_date == latest_date:
            # print(f"[{code}] è°ƒè¯•: å†å²æ•°æ®ä¸­å·²åŒ…å« {latest_date} çš„æ•°æ®ï¼Œè·³è¿‡å®æ—¶æ‹¼æ¥ã€‚")
            return df_daily

        if last_history_date > latest_date:
            # print(f"[{code}] è­¦å‘Š: å†å²æ•°æ®æ—¥æœŸ {last_history_date} æ™šäºå½“å‰æ—¥æœŸ {latest_date}ï¼Œè·³è¿‡æ‹¼æ¥ã€‚")
            return df_daily

    # æ„é€ æ–°çš„ DataFrame è¡Œï¼Œç¡®ä¿åˆ—åä¸å†å²æ•°æ®ä¸€è‡´
    new_row_data = {
        'date': latest_date,
        'open': latest_data.get('open'),
        'high': latest_data.get('high'),
        'low': latest_data.get('low'),
        'close': latest_data.get('close'),
        'volume': latest_data.get('volume'),
        'amount': latest_data.get('amount'),

        # å¡«å……å†å²æ•°æ®ä¸­å…¶ä»–éœ€è¦çš„åˆ—
        'outstanding_share': None,
        'turnover': None,
        'adjust': CONFIG.get("ADJUST", ""),
        'code': code,
    }

    # âš ï¸ å¿…é¡»ç¡®ä¿ df_daily.columns åŒ…å« new_row_data ä¸­æ‰€æœ‰çš„é”®
    try:
        df_new_day = pd.DataFrame([new_row_data], columns=df_daily.columns)
    except ValueError:
        # å¦‚æœåˆ—åä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯å†å²æ•°æ® df_daily ç»“æ„ä¸æ ‡å‡†ï¼Œå°è¯•ä½¿ç”¨å¿«ç…§æ•°æ®æä¾›çš„åˆ—
        df_new_day = pd.DataFrame([new_row_data])

    df_final = pd.concat([df_daily, df_new_day], ignore_index=True)

    return df_final


def fetch_data_with_timeout(symbol, start_date, end_date, adjust, timeout):
    def _fetch():
        if CONFIG["USE_LOCAL_MYSQL"]:
            try:
                # å°è¯•ä½¿ç”¨æœ¬åœ° MySQL æ¥å£
                return stock_zh_a_daily_mysql(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)
            except NameError:
                # å¦‚æœ NameError è¯´æ˜æœ¬åœ°æ¥å£æœªå®šä¹‰æˆ–å¯¼å…¥å¤±è´¥ï¼Œé™çº§
                print(f"\n[é”™è¯¯] {symbol} å°è¯•ä½¿ç”¨ MySQL æ¥å£å¤±è´¥ (NameError)ï¼Œè‡ªåŠ¨é™çº§åˆ° AkShareã€‚\n")
                pass
            except Exception as e:
                # å…¶ä»–è¿æ¥æˆ–æŸ¥è¯¢é”™è¯¯ï¼Œé™çº§
                print(f"\n[é”™è¯¯] {symbol} å°è¯•ä½¿ç”¨ MySQL æ¥å£å¤±è´¥ ({type(e).__name__})ï¼Œè‡ªåŠ¨é™çº§åˆ° AkShareã€‚\n")
                pass

        # ä½¿ç”¨ AkShare æ¥å£
        return ak.stock_zh_a_daily(symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust)

    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(_fetch)
        try:
            done, not_done = wait([future], timeout=timeout)
            if future in done:
                return future.result()
            elif future in not_done:
                future.cancel()
                raise ThreadingTimeoutError(f"è¯·æ±‚è¶…æ—¶ ({timeout}s)")
        except Exception as e:
            raise e


# ============================================================
# æ¨¡å— 8ï¼šå•åªè‚¡ç¥¨ç­–ç•¥ï¼ˆæ•´åˆ StochRSI + EMA è¿‡æ»¤ï¼‰(ä¿æŒä¸å˜ï¼Œä½†ä¾èµ–äºä¿®å¤åçš„æ¨¡å— A/B)
# ============================================================
def strategy_single_stock(code, start_date, end_date, df_spot):
    symbol = f"sh{code}" if code.startswith("6") else f"sz{code}"

    try:
        df = fetch_data_with_timeout(symbol=symbol, start_date=start_date, end_date=end_date, adjust=CONFIG["ADJUST"],
                                     timeout=CONFIG["REQUEST_TIMEOUT"])

        # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿè®¡ç®— EMA200 (è‡³å°‘ 200 + 1)
        if df is None or df.empty or len(df) < 220: return None

        # è°ƒç”¨å®æ—¶è‚¡ç¥¨è¡Œæƒ…æ‹¼æ¥æ¥å£
        if CONFIG["USE_REAL_TIME_DATA"]:
            df = append_today_realtime_snapshot(code, df, df_spot)

        # ç¡®ä¿æ•°æ®é•¿åº¦åœ¨æ‹¼æ¥åä»è¶³å¤Ÿ
        if len(df) < 220: return None

        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯ datetime.date æˆ–å¯æ’åºç±»å‹
        df['date'] = pd.to_datetime(df['date']).dt.date
        df = df.sort_values('date').reset_index(drop=True)

        # --- ç­–ç•¥æ ¸å¿ƒè®¡ç®—ï¼šStochRSI + EMA è¶‹åŠ¿è¿‡æ»¤ ---

        # 1. è®¡ç®— StochRSI ä¿¡å·
        # è¿™ä¼šè°ƒç”¨å·²ä¿®å¤çš„ calculate_stoch_rsi_signal_and_values
        k_val, d_val, stoch_rsi_buy_signal = calculate_stoch_rsi_signal_and_values(
            df,
            length_rsi=CONFIG["STOCH_RSI"]["lengthRSI"],
            length_stoch=CONFIG["STOCH_RSI"]["lengthStoch"],
            smooth_k=CONFIG["STOCH_RSI"]["smoothK"],
            smooth_d=CONFIG["STOCH_RSI"]["smoothD"],
            oversold_level=CONFIG["STOCH_RSI"]["oversoldLevel"]
        )

        # å¦‚æœæ²¡æœ‰ StochRSI ä¿¡å·ï¼Œåˆ™ç›´æ¥è¿”å› None
        if not stoch_rsi_buy_signal:
            return None

        # 2. è®¡ç®— EMA è¶‹åŠ¿
        # è¿™ä¼šè°ƒç”¨å·²ä¿®å¤çš„ pine_ema
        ema50 = pine_ema(df['close'], 50).iloc[-1]
        ema200 = pine_ema(df['close'], 200).iloc[-1]
        current_close = df['close'].iloc[-1]

        # 3. è¶‹åŠ¿è¿‡æ»¤æ¡ä»¶: close > EMA50 > EMA200
        # âš ï¸ æ³¨æ„ï¼šæ­¤å¤„è¦æ±‚ä¸¥æ ¼çš„ C > E50 > E200 é¡ºåº
        trend_filter = (current_close > ema50) and (ema50 > ema200)

        # 4. æœ€ç»ˆä¿¡å·åˆ¤å®š
        if not trend_filter:
            # StochRSI ä¿¡å·å‘½ä¸­ï¼Œä½†è¶‹åŠ¿ä¸æ»¡è¶³ï¼Œè·³è¿‡
            return None

        # --- æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼Œæ„å»ºè¿”å›ç»“æœ ---

        # è®¡ç®—æ¶¨å¹… (ä¸å‰ä¸€æ—¥ç›¸æ¯”)
        pct_chg = (current_close / df['close'].iloc[-2] - 1) * 100 if len(df) >= 2 else 0

        return {
            "ä»£ç ": code,
            "æ—¥æœŸ": df['date'].iloc[-1].strftime('%Y-%m-%d'),
            "ä¿¡å·": 'StochRSI/EMA Buy',
            "å½“å‰ä»·": round(current_close, 2),
            "æ¶¨å¹…%": round(pct_chg, 2),
            "StochK": round(float(k_val), 2),
            "StochD": round(float(d_val), 2),
            "EMA50": round(float(ema50), 2),
            "EMA200": round(float(ema200), 2),
            "è¶‹åŠ¿è¿‡æ»¤": "æ»¡è¶³ (C>E50>E200)",
        }

    except ThreadingTimeoutError:
        print(f"[è¶…æ—¶] {code} è¯·æ±‚å†å²æ•°æ®è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return None

    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©è°ƒè¯•
        print(f"[é”™è¯¯] {code} å¤„ç†å¤±è´¥: {type(e).__name__}: {e}")
        return None


# ============================================================
# æ¨¡å— 9ï¼šå¹¶å‘æ‰«æ (Async Scheduler) (ä¿æŒä¸å˜)
# ============================================================
async def main_scanner_async(stock_codes, df_spot):
    end_date = datetime.datetime.now().strftime("%Y%m%d")
    start_date = (datetime.datetime.now() - datetime.timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")

    results = []
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:

        tasks = [
            loop.run_in_executor(pool, strategy_single_stock, code, start_date, end_date, df_spot)
            for code in stock_codes
        ]

        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")
        for coro in pbar:
            res = await coro
            if res:
                results.append(res)
                pbar.set_postfix({"å‘½ä¸­": len(results)})

    return results


async def batch_scan_manager_async(target_codes, df_spot):
    all_results = []
    batch_size = CONFIG.get("BATCH_SIZE", 500)
    interval = CONFIG.get("BATCH_INTERVAL_SEC", 2)
    total_stocks = len(target_codes)
    num_batches = math.ceil(total_stocks / batch_size)

    print(f"\n[è°ƒåº¦å™¨] æ€»è®¡ {total_stocks} æ”¯è‚¡ç¥¨ï¼Œå°†åˆ†ä¸º {num_batches} æ‰¹æ¬¡ ({batch_size} æ”¯/æ‰¹)ã€‚")

    for i in range(num_batches):
        start_index = i * batch_size
        end_index = min((i + 1) * batch_size, total_stocks)
        batch_codes = target_codes[start_index:end_index]

        print("\n" + "=" * 60)
        print(f"--- ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i + 1}/{num_batches} ({len(batch_codes)} æ”¯) ---")

        batch_results = await main_scanner_async(batch_codes, df_spot)
        all_results.extend(batch_results)

        if i < num_batches - 1:
            print(f"--- ğŸ˜´ æ‰¹æ¬¡ {i + 1} å®Œæˆï¼Œå½“å‰æ€»å‘½ä¸­: {len(all_results)}ï¼Œä¼‘æ¯ {interval} ç§’... ---")
            time.sleep(interval)
        else:
            print("--- ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚---")

    return all_results


# ============================================================
# æ¨¡å— 10ï¼šä¸»å…¥å£ (ä¿æŒä¸å˜)
# ============================================================
def main():
    start_time = time.time()

    # --- ğŸ†• åŠ è½½äº¤æ˜“æ—¥å† (æ–°å¢æ­¥éª¤) ---
    if CONFIG["USE_REAL_TIME_DATA"]:
        load_trade_calendar()

    # ä½¿ç”¨ LogRedirector å¯åŠ¨æ—¥å¿—ç®¡ç†
    with LogRedirector(folder=CONFIG['OUTPUT_LOG']) as log_redirector:

        end_date = datetime.datetime.now().strftime("%Y%m%d")
        start_date = (datetime.datetime.now() - datetime.timedelta(days=CONFIG["DAYS"])).strftime("%Y%m%d")
        print(f"\n[ä»»åŠ¡å¯åŠ¨] æ‰«æèŒƒå›´: {start_date} ~ {end_date}")
        print(f"[é…ç½®] ç›®æ ‡çº¿ç¨‹: {CONFIG['MAX_WORKERS']} | è¶…æ—¶: {CONFIG['REQUEST_TIMEOUT']}s")

        # 1. ä¸²è¡Œè·å–å®æ—¶å¿«ç…§ (å—å¼€å…³æ§åˆ¶)
        df_spot = pd.DataFrame()

        if CONFIG["USE_REAL_TIME_DATA"]:
            try:
                # æ£€æŸ¥å½“å‰æ—¥æœŸæ˜¯å¦ä¸ºäº¤æ˜“æ—¥ (is_trade_day å‡½æ•°éœ€è¦ä¸€ä¸ª date å¯¹è±¡)
                today_date = datetime.datetime.now().date()
                if is_trade_day(today_date):
                    df_spot = fetch_realtime_snapshot()
                    if df_spot.empty:
                        print("[ç»ˆæ­¢] æ— æ³•è·å–å®æ—¶è¡Œæƒ…å¿«ç…§ï¼Œç»ˆæ­¢æ‰«æã€‚")
                        sys.exit(1)
                else:
                    print("[é…ç½®] å½“å‰ä¸ºéäº¤æ˜“æ—¥ï¼Œè·³è¿‡å®æ—¶å¿«ç…§è·å–ã€‚")
            except Exception as e:
                print(f"[è‡´å‘½ç»ˆæ­¢] è·å–å®æ—¶è¡Œæƒ…å¿«ç…§å¤±è´¥: {e}")
                sys.exit(1)
        else:
            print("[é…ç½®] å®æ—¶æ•°æ®è·å–å¼€å…³å…³é—­ (USE_REAL_TIME_DATA=False)ï¼Œè·³è¿‡å…¨å¸‚åœºå¿«ç…§è·å–ã€‚")

        # 2. è·å–è‚¡ç¥¨åˆ—è¡¨å’Œè¿‡æ»¤
        manual_list = CONFIG["MANUAL_STOCK_LIST"]
        df_base = pd.DataFrame()
        target_codes = []

        if manual_list and len(manual_list) > 0:
            target_codes = [str(c).zfill(6) for c in manual_list]
            print(f"[æ‰‹åŠ¨æ¨¡å¼] ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥åˆ—è¡¨ï¼Œå…± {len(target_codes)} æ”¯è‚¡ç¥¨ã€‚")
            try:
                df_base = get_stock_list_manager()
            except Exception:
                df_base = pd.DataFrame({"code": target_codes, "name": ["æœªçŸ¥"] * len(target_codes)})
        else:
            try:
                df_base = get_stock_list_manager()
            except Exception as e:
                print(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}")
                return

            valid_codes = filter_stock_list(df_base)

            sample_size = CONFIG["SAMPLE_SIZE"]
            if isinstance(sample_size, int) and sample_size > 0 and len(valid_codes) > sample_size:
                print(f"[æŠ½æ ·æ¨¡å¼] éšæœºæŠ½å– {sample_size} æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•...")
                target_codes = random.sample(valid_codes, sample_size)
            else:
                print(f"[å…¨é‡æ¨¡å¼] æ‰«ææ‰€æœ‰ {len(valid_codes)} æ”¯æœ‰æ•ˆè‚¡ç¥¨...")
                target_codes = valid_codes

        # 3. å¹¶å‘æ‰«æ
        # å¦‚æœæ˜¯æŠ½æ ·æ¨¡å¼æˆ–æ€»æ•°å°äºæ‰¹æ¬¡å¤§å°ï¼Œåˆ™ç›´æ¥è¿è¡Œï¼Œå¦åˆ™ä½¿ç”¨æ‰¹æ¬¡ç®¡ç†
        if CONFIG["SAMPLE_SIZE"] > 0 or len(target_codes) <= CONFIG["BATCH_SIZE"]:
            final_data = asyncio.run(main_scanner_async(target_codes, df_spot))
        else:
            final_data = asyncio.run(batch_scan_manager_async(target_codes, df_spot))

        # 4. ç»“æœæ•´ç†ä¸å¯¼å‡º
        if final_data:
            res_df = pd.DataFrame(final_data)

            # åˆå¹¶è‚¡ç¥¨åç§°
            name_map = df_base.set_index('code')['name'].to_dict()
            res_df['åç§°'] = res_df['ä»£ç '].map(name_map).fillna('æœªçŸ¥')

            # å¯¼å‡º CSV
            today_date_str = datetime.datetime.now().strftime('%Y-%m-%d')
            folder_path = os.path.join(CONFIG["OUTPUT_FOLDER_BASE"], today_date_str)
            os.makedirs(folder_path, exist_ok=True)
            timestamp = datetime.datetime.now().strftime('%H%M%S')
            file_name = f"{CONFIG['OUTPUT_FILENAME_BASE']}_{timestamp}.csv"
            full_file_path = os.path.join(folder_path, file_name)

            # é‡æ–°æ’åºç»“æœåˆ—
            ordered_cols = ["ä»£ç ", "åç§°", "æ—¥æœŸ", "å½“å‰ä»·", "æ¶¨å¹…%", "StochK", "StochD", "EMA50", "EMA200"]
            res_df = res_df[ordered_cols]

            res_df = res_df.sort_values(["æ¶¨å¹…%"], ascending=[False]).reset_index(drop=True)


            res_df.to_csv(full_file_path, index=False, encoding=CONFIG["EXPORT_ENCODING"])

            print("\n" + "=" * 60)
            print(f"âœ… æ‰«æå®Œæˆ | è€—æ—¶: {time.time() - start_time:.1f}s")
            print(f"ğŸ“„ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {full_file_path}")
            print(f"ğŸ“ˆ å‘½ä¸­æ•°é‡: {len(res_df)}")
            print("=" * 60)
            print("--- å‘½ä¸­è‚¡ç¥¨ Top 10 ---")
            print(res_df.head(10).to_string(index=False))
            return res_df

        else:
            print("\n[ç»“æœ] æ²¡æœ‰å‘ç°ç¬¦åˆç­–ç•¥çš„è‚¡ç¥¨ã€‚")
            return pd.DataFrame()


if __name__ == "__main__":
    main()