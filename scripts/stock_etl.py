#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
import random
import logging
import asyncio
import traceback
import re
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
import akshare as ak
from tqdm import tqdm
from sqlalchemy import create_engine, text

# é…ç½®æ–‡ä»¶å­˜åœ¨
try:
    import conf.config as conf
except ImportError:
    # å…¼å®¹æµ‹è¯•ç¯å¢ƒ
    class MockConf:
        DB_CONFIG = {
            "USER": "root", "PASS": "password", "HOST": "127.0.0.1", "PORT": 3306, "DB_NAME": "asian_quant"
        }
    conf = MockConf()

# ---------------------------------------------------------
# 0. ç»ˆç«¯è§†è§‰å¸¸é‡ (ANSI é¢œè‰²)
# ---------------------------------------------------------
C_END, C_BOLD, C_RED, C_GREEN, C_YELLOW, C_BLUE, C_CYAN = "\033[0m", "\033[1m", "\033[31m", "\033[32m", "\033[33m", "\033[34m", "\033[36m"

# ---------------------------------------------------------
# 1. å…¨å±€é…ç½®å­—å…¸ (å­—å…¸å±•å¼€æ’ç‰ˆï¼Œæ‹’ç»å•è¡Œ)
# ---------------------------------------------------------
CONFIG = {
    # æ•°æ®å‘¨æœŸï¼šdaily(æ—¥çº¿), weekly(å‘¨çº¿), monthly(æœˆçº¿)
    "PERIOD": "daily",

    # ã€ä¼˜å…ˆçº§æœ€é«˜ã€‘ç›¸å¯¹å¤©æ•°æ¨¡å¼ï¼š0-ä»Šå¤©, 1-æ˜¨å¤©, 2-å‰å¤©... 3-None-ä½¿ç”¨å›ºå®šæ—¥æœŸ
    "LOOKBACK_DAYS": 0,

    # å›ºå®šæ—¥æœŸæ¨¡å¼ (å½“ LOOKBACK_DAYS ä¸º None æ—¶ç”Ÿæ•ˆ) æ ¼å¼ï¼š20260106
    "START_DATE": "20260106",
    "END_DATE": "20260106",

    # å®šå‘åŒæ­¥æ¸…å•ï¼šè‹¥å¡«å…¥ä»£ç å¦‚ ["600519"]ï¼Œåˆ™åªåŒæ­¥è¿™äº›ï¼Œå¿½ç•¥è¿‡æ»¤é€»è¾‘
    "TARGET_STOCKS": [],

    # å¹¶å‘ä¸é‡è¯•è®¾ç½®
    "MAX_WORKERS": 8,  # æœ€å¤§å¹¶å‘æ•°
    "MAX_RETRIES": 2,  # å•åªè‚¡ç¥¨å¤±è´¥åçš„é‡è¯•æ¬¡æ•°

    # æ•°æ®åº“è¡¨æ˜ å°„å…³ç³»
    "TABLE_MAP": {
        "daily": "asian_quant_stock_daily",
        "weekly": "asian_quant_stock_weekly",
        "monthly": "asian_quant_stock_monthly"
    },

    # å¤æƒæ–¹å¼ï¼šqfq(å‰å¤æƒ), hfq(åå¤æƒ), None(ä¸å¤æƒ)
    "ADJUST": "qfq",

    # è¿‡æ»¤å™¨é…ç½®
    "EXCLUDE_GEM": True,     # åˆ›ä¸šæ¿è¿‡æ»¤
    "EXCLUDE_KCB": True,     # ç§‘åˆ›æ¿è¿‡æ»¤
    "EXCLUDE_BJ": True,      # åŒ—äº¤æ‰€åŠæ–°ä¸‰æ¿è¿‡æ»¤
    "EXCLUDE_ST": False,     # æ’é™¤ ST/*ST
    "EXCLUDE_DELIST": True,  # æ’é™¤é€€å¸‚è‚¡
}

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = conf.DB_CONFIG

# è·¯å¾„ç®¡ç†
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CURRENT_DATE_STR = datetime.now().strftime("%Y%m%d")
LOG_DIR = os.path.join(BASE_DIR, "data/logs", CURRENT_DATE_STR)
CACHE_DIR = os.path.join(BASE_DIR, "data/cache")
CACHE_PATH = os.path.join(CACHE_DIR, "stock_list_cache.json")

os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)


# ---------------------------------------------------------
# 2. æ·±åº¦å®šåˆ¶æ—¥å¿—ç³»ç»Ÿ (æ”¯æŒé¢œè‰² & å…¼å®¹ Tqdm)
# ---------------------------------------------------------
class ColoredFormatter(logging.Formatter):
    """è‡ªå®šä¹‰å½©è‰²æ ¼å¼åŒ–ç±»"""
    MAPPING = {
        logging.INFO: f"{C_BLUE}%(asctime)s [INFO]{C_END} %(message)s",
        logging.WARNING: f"{C_YELLOW}%(asctime)s [WARN]{C_END} %(message)s",
        logging.ERROR: f"{C_RED}%(asctime)s [ERROR]{C_END} %(message)s",
        logging.CRITICAL: f"{C_BOLD}{C_RED}%(asctime)s [CRIT]{C_END} %(message)s",
    }

    def format(self, record):
        fmt = self.MAPPING.get(record.levelno)
        return logging.Formatter(fmt, datefmt='%H:%M:%S').format(record)


class TqdmLoggingHandler(logging.Handler):
    """ç¡®ä¿æ—¥å¿—è¾“å‡ºä¸ç ´åè¿›åº¦æ¡è¡Œçš„å¤„ç†å™¨"""

    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.write(msg, file=sys.stdout)
            self.flush()
        except Exception:
            self.handleError(record)


def setup_logger():
    """åˆå§‹åŒ–å…¨å±€æ—¥å¿—è®°å½•å™¨"""
    l = logging.getLogger('StockETL')
    l.setLevel(logging.INFO)
    if l.handlers:
        l.handlers.clear()
    l.propagate = False

    # 1. ç‰©ç†æ–‡ä»¶è®°å½•
    file_fmt = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')
    fh = logging.FileHandler(os.path.join(LOG_DIR, "etl.log"), encoding='utf-8')
    fh.setFormatter(file_fmt)
    l.addHandler(fh)

    # 2. ç»ˆç«¯å½©è‰²è¾“å‡º
    th = TqdmLoggingHandler()
    th.setFormatter(ColoredFormatter())
    l.addHandler(th)
    return l


logger = setup_logger()


# ---------------------------------------------------------
# 3. åŸºç¡€å·¥å…·å‡½æ•° (è¿‡æ»¤ã€æ•°æ®åº“ã€ç¼“å­˜)
# ---------------------------------------------------------
def get_engine():
    """åˆ›å»º SQLAlchemy å¼•æ“"""
    c = DB_CONFIG
    conn_url = (
        f"mysql+pymysql://{c['USER']}:{c['PASS']}@"
        f"{c['HOST']}:{c['PORT']}/{c['DB_NAME']}?charset=utf8mb4"
    )
    return create_engine(conn_url, pool_recycle=3600)


def apply_filters(df):
    """
    å¯¹è‚¡ç¥¨åˆ—è¡¨è¿›è¡Œæ¿å—å’Œé£é™©è¿‡æ»¤
    """
    before_count = len(df)

    # 1. é£é™©è­¦ç¤ºè‚¡è¿‡æ»¤ (ST/*ST)
    if CONFIG.get("EXCLUDE_ST", True):
        df = df[~df['name'].str.contains(r'ST|\*ST', flags=re.IGNORECASE)]

    # 2. ç»ˆæ­¢ä¸Šå¸‚è‚¡è¿‡æ»¤ (é€€å¸‚æ•´ç†æœŸ)
    if CONFIG.get("EXCLUDE_DELIST", True):
        df = df[~df['name'].str.contains(r'é€€å¸‚|é€€')]

    # 3. é A è‚¡ä¸šåŠ¡è¿‡æ»¤ (B è‚¡æ’é™¤)
    df = df[~df['code'].str.startswith(('900', '200'))]

    # 4. åˆ›ä¸šæ¿è¿‡æ»¤
    if CONFIG.get("EXCLUDE_GEM", True):
        df = df[~df['code'].str.startswith(('300', '301'))]

    # 5. ç§‘åˆ›æ¿è¿‡æ»¤
    if CONFIG.get("EXCLUDE_KCB", True):
        df = df[~df['code'].str.startswith(('688', '689'))]

    # 6. åŒ—äº¤æ‰€åŠæ–°ä¸‰æ¿è¿‡æ»¤
    if CONFIG.get("EXCLUDE_BJ", True):
        df = df[~df['code'].str.startswith(('4', '8', '92'))]

    after_count = len(df)
    logger.info(
        f"ğŸ” {C_BOLD}{C_YELLOW}è¿‡æ»¤æŠ¥è¡¨:{C_END} æ€»æ ·æœ¬: {C_CYAN}{before_count}{C_END}æ”¯ | "
        f"å‰”é™¤: {C_RED}{before_count - after_count}{C_END}æ”¯ | "
        f"æœ‰æ•ˆ: {C_GREEN}{after_count}{C_END}æ”¯"
    )
    return df


def get_stock_list_with_cache():
    """è·å–å…¨å¸‚åœºè‚¡ç¥¨æ¸…å•"""
    today = datetime.now().strftime("%Y-%m-%d")
    if os.path.exists(CACHE_PATH):
        try:
            with open(CACHE_PATH, "r", encoding="utf-8") as f:
                c = json.load(f)
                if c.get("time") == today:
                    logger.info(f"{C_GREEN}âœ… ç¼“å­˜å‘½ä¸­:{C_END} ä½¿ç”¨ä»Šæ—¥ä»£ç æ¸…å•")
                    return pd.DataFrame(c['data'])
        except Exception:
            pass

    logger.info("ğŸ“¡ æ¥å£æ›´æ–°: æŠ“å–æœ€æ–°ä»£ç åˆ—è¡¨...")
    df = ak.stock_zh_a_spot_em()[['ä»£ç ', 'åç§°']].rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
    df['code'] = df['code'].astype(str)
    with open(CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump({"time": today, "data": df.to_dict(orient="records")}, f, ensure_ascii=False, indent=2)
    return df


def get_downloaded_codes():
    """æ£€æŸ¥æ•°æ®åº“å†…å·²åŒæ­¥çš„ä»£ç """
    table = CONFIG["TABLE_MAP"].get(CONFIG["PERIOD"])
    adj = CONFIG["ADJUST"] or 'none'
    sql = f"SELECT DISTINCT code FROM {table} WHERE date BETWEEN :s AND :e AND adjust = :adj"
    try:
        with get_engine().connect() as conn:
            res = conn.execute(text(sql), {"s": CONFIG["START_DATE"], "e": CONFIG["END_DATE"], "adj": adj})
            return {row[0] for row in res}
    except Exception:
        return set()


# ---------------------------------------------------------
# 4. æŠ“å–ä¸ Upsert é€»è¾‘
# ---------------------------------------------------------
def fetch_stock_data(item, s_date, e_date):
    """æŠ“å–å•åªè‚¡ç¥¨è¯¦æƒ…"""
    code = item['code']
    time.sleep(random.uniform(0.3, 0.6))
    last_err = "æœªçŸ¥é”™è¯¯"

    for attempt in range(1, CONFIG["MAX_RETRIES"] + 1):
        try:
            # æ”¯æŒå†å²åŠ å®æ—¶æ•°æ®
            df = ak.stock_zh_a_hist(
                symbol=code,
                period=CONFIG['PERIOD'],
                start_date=s_date,
                end_date=e_date,
                adjust=CONFIG['ADJUST']
            )
            if df is None or df.empty:
                raise ValueError("ç©ºæ•°æ®")

            # å­—æ®µé‡å‘½åä¸å¯¹é½
            mapping = {
                "æ—¥æœŸ": "date",
                "å¼€ç›˜": "open",
                "æœ€é«˜": "high",
                "æœ€ä½": "low",
                "æ”¶ç›˜": "close",
                "æˆäº¤é‡": "volume",
                "æˆäº¤é¢": "amount",
                "æŒ¯å¹…": "amplitude",
                "æ¶¨è·Œé¢": "chg",
                "æ¶¨è·Œå¹…": "pct_chg",
                "æ¢æ‰‹ç‡": "turnover_rate"
            }
            df = df.rename(columns=mapping)
            df['date'] = pd.to_datetime(df['date']).dt.date
            df['code'] = code
            df['adjust'] = CONFIG['ADJUST'] if CONFIG['ADJUST'] else 'none'

            # ä¸¥æ ¼ç­›é€‰æ•°æ®åº“åˆ—
            db_columns = [
                'code', 'date', 'open', 'high', 'low', 'close',
                'volume', 'amount', 'amplitude', 'pct_chg',
                'chg', 'turnover_rate', 'adjust'
            ]
            return {"code": code, "df": df[db_columns], "error": None}

        except Exception as e:
            last_err = str(e)
            if attempt < CONFIG["MAX_RETRIES"]:
                time.sleep(1.5)
    return {"code": code, "df": None, "error": last_err}


def mysql_upsert_logic(table, conn, keys, data_iter):
    """MySQL æ‰¹é‡ Upsert è¯­å¥"""
    data_list = [dict(zip(keys, row)) for row in data_iter]
    cols = ", ".join([f"`{k}`" for k in keys])
    plh = ", ".join([f":{k}" for k in keys])
    upd = ", ".join([f"`{k}`=VALUES(`{k}`)" for k in keys if k not in ['date', 'code', 'adjust']])
    sql = f"INSERT INTO {table.name} ({cols}) VALUES ({plh}) ON DUPLICATE KEY UPDATE {upd}"
    conn.execute(text(sql), data_list)


# ---------------------------------------------------------
# 5. æ ¸å¿ƒå¼•æ“ (å¼‚æ­¥è°ƒåº¦)
# ---------------------------------------------------------
async def start_engine(todo_jobs, total_query, already_exist):
    sem = asyncio.Semaphore(CONFIG["MAX_WORKERS"])
    loop = asyncio.get_running_loop()
    success_dfs, failed_logs = [], []

    async def worker(item):
        async with sem:
            with ThreadPoolExecutor() as pool:
                return await loop.run_in_executor(pool, fetch_stock_data, item, CONFIG["START_DATE"],
                                                  CONFIG["END_DATE"])

    pbar = tqdm(
        total=len(todo_jobs),
        desc=f"{C_BOLD}ğŸ“Š åŒæ­¥è¿›åº¦{C_END}",
        bar_format="{l_bar}%s{bar:25}%s{r_bar}" % (C_GREEN, C_END),
        dynamic_ncols=True
    )

    tasks = [worker(it) for it in todo_jobs]
    for future in asyncio.as_completed(tasks):
        res = await future
        if res["df"] is not None:
            success_dfs.append(res["df"])
        else:
            failed_logs.append(res)
        pbar.update(1)
        pbar.set_postfix({
            "ğŸ¯æ€»éœ€": total_query,
            "ğŸ“¦å·²æœ‰": f"{C_CYAN}{already_exist}{C_END}",
            "âœ…æˆåŠŸ": f"{C_GREEN}{len(success_dfs)}{C_END}",
            "âŒå¤±è´¥": f"{C_RED}{len(failed_logs)}{C_END}"
        })
    pbar.close()

    if failed_logs:
        logger.error(f"{C_BOLD}{' å¤±è´¥æ˜ç»† ':=^40}{C_END}")
        for log in failed_logs:
            logger.error(f"ä»£ç : {log['code']} | åŸå› : {log['error']}")

    if success_dfs:
        target = CONFIG["TABLE_MAP"].get(CONFIG["PERIOD"])
        logger.info(f"{C_GREEN}ğŸ’¾ æ­£åœ¨å…¥åº“ {len(success_dfs)} æ¡æ•°æ®åŒ…...{C_END}")
        try:
            pd.concat(success_dfs).to_sql(
                name=target, con=get_engine(), if_exists='append',
                index=False, chunksize=2000, method=mysql_upsert_logic
            )
            logger.info(f"{C_BOLD}{C_GREEN}âœ¨ ETL åŒæ­¥åœ†æ»¡å®Œæˆï¼{C_END}")
        except Exception as e:
            logger.error(f"ğŸ’” å…¥åº“å´©æºƒ: {e}")


# ---------------------------------------------------------
# 6. ä¸»å…¥å£ (å‚æ•°ä¼˜å…ˆçº§ä¸æ—¥æœŸè®¡ç®—)
# ---------------------------------------------------------
def main():
    start_ts = time.time()
    logger.info(f"{C_BOLD}{'=' * 75}{C_END}")
    logger.info(f"{C_BOLD}AsianQuant ETL ç³»ç»Ÿå¯åŠ¨{C_END}")

    # --- æ—¥æœŸä¼˜å…ˆçº§å¤„ç†é€»è¾‘ ---
    # --- æ—¥æœŸä¼˜å…ˆçº§å¤„ç†é€»è¾‘ (ä¿®æ­£åçš„åŒºé—´é€»è¾‘) ---
    lookback = CONFIG.get("LOOKBACK_DAYS")
    if lookback is not None:
        today_dt = datetime.now()

        if lookback == 0:
            # æ¨¡å¼ 0: ä»Šå¤© -> ä»Šå¤©
            start_dt = today_dt
            end_dt = today_dt
        else:
            # æ¨¡å¼ N: (æ˜¨å¤© - N + 1) -> æ˜¨å¤©
            # ä¾‹å¦‚ lookback = 3: (æ˜¨å¤©-2å¤©) åˆ° (æ˜¨å¤©)
            yesterday_dt = today_dt - timedelta(days=1)
            start_dt = yesterday_dt - timedelta(days=int(lookback) - 1)
            end_dt = yesterday_dt

        CONFIG["START_DATE"] = start_dt.strftime("%Y%m%d")
        CONFIG["END_DATE"] = end_dt.strftime("%Y%m%d")

        mode_desc = f"{C_YELLOW}å›æº¯åŒºé—´æ¨¡å¼ (N={lookback}){C_END} -> {C_RED}{CONFIG['START_DATE']} è‡³ {CONFIG['END_DATE']}{C_END}"
    elif CONFIG.get("START_DATE") and CONFIG.get("END_DATE"):
        # ä¼˜å…ˆçº§2ï¼šå›ºå®šæ—¥æœŸæ¨¡å¼
        mode_desc = f"{C_BLUE}å›ºå®šæ—¥æœŸæ¨¡å¼{C_END} -> èŒƒå›´: {C_RED}{CONFIG['START_DATE']} è‡³ {CONFIG['END_DATE']}{C_END}"
    else:
        # å…œåº•ï¼šæœªé…ç½®æŠ¥é”™
        logger.error(f"{C_RED}è‡´å‘½é”™è¯¯: LOOKBACK_DAYS å’Œ START/END_DATE å‡æœªé…ç½®ï¼{C_END}")
        raise ValueError("Configuration Error: No date range specified.")

    logger.info(f"ğŸ“… å½“å‰è¿è¡Œæ¨¡å¼: {mode_desc}")
    logger.info(f"â±ï¸ ä»»åŠ¡å‘¨æœŸ: {CONFIG['PERIOD']} | å¤æƒ: {CONFIG['ADJUST']}")

    try:
        # 1. ä»»åŠ¡å‡†å¤‡
        df_all = get_stock_list_with_cache()
        if CONFIG["TARGET_STOCKS"]:
            full_jobs = [{"code": str(c), "name": "å®šå‘åŒæ­¥"} for c in CONFIG["TARGET_STOCKS"]]
        else:
            df_filtered = apply_filters(df_all)
            full_jobs = df_filtered.to_dict(orient="records")

        # 2. æŸ¥æ¼è¡¥ç¼º (æ–­ç‚¹ç»­ä¼ )
        downloaded = get_downloaded_codes()
        todo_jobs = [j for j in full_jobs if j['code'] not in downloaded]
        already_exist = len(full_jobs) - len(todo_jobs)

        logger.info(
            f"ğŸ“‹ {C_BOLD}ä»»åŠ¡ç»Ÿè®¡:{C_END} æ€»é‡={len(full_jobs)} | "
            f"è·³è¿‡å·²å­˜åœ¨={already_exist} | å®é™…å¾…åŒæ­¥={len(todo_jobs)}"
        )

        # 3. å¼‚æ­¥å¯åŠ¨
        if todo_jobs:
            asyncio.run(start_engine(todo_jobs, len(full_jobs), already_exist))
        else:
            logger.info(f"{C_GREEN}âœ… ç›®æ ‡æ—¶é—´æ®µæ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€æ“ä½œã€‚{C_END}")

    except Exception as e:
        logger.critical(f"ğŸ›‘ ç¨‹åºå´©æºƒ: {e}\n{traceback.format_exc()}")

    logger.info(f"{C_BOLD}ğŸ æ€»è€—æ—¶: {time.time() - start_ts:.2f}s{C_END}")
    logger.info(f"{C_BOLD}{'=' * 75}{C_END}")


if __name__ == "__main__":
    main()