#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
import random
import logging
import asyncio
import traceback
import re
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
import akshare as ak
from tqdm import tqdm
from sqlalchemy import create_engine, text

import conf.config as conf

C_END = "\033[0m"
C_BOLD = "\033[1m"
C_RED = "\033[31m"
C_GREEN = "\033[32m"
C_YELLOW = "\033[33m"
C_BLUE = "\033[34m"
C_CYAN = "\033[36m"

CONFIG = {
    # å®šå‘åŒæ­¥æ¸…å•ï¼šè‹¥å¡«å…¥ä»£ç å¦‚ ["600519"]ï¼Œåˆ™åªåŒæ­¥è¿™äº›ï¼Œå¿½ç•¥è¿‡æ»¤é€»è¾‘
    "TARGET_STOCKS": [],

    # æ•°æ®å‘¨æœŸï¼šdaily(æ—¥çº¿), weekly(å‘¨çº¿), monthly(æœˆçº¿)
    "PERIOD": "daily",

    # æ—¶é—´èŒƒå›´ (YYYYMMDD)
    "START_DATE": "20260101",
    "END_DATE": "20260105",

    # å¹¶å‘ä¸é‡è¯•è®¾ç½®
    "MAX_WORKERS": 8,  # æœ€å¤§å¹¶å‘æ•°
    "MAX_RETRIES": 2,  # å•åªè‚¡ç¥¨å¤±è´¥åçš„é‡è¯•æ¬¡æ•°

    # æ•°æ®åº“è¡¨æ˜ å°„å…³ç³»
    "TABLE_MAP": {
        "daily": "asian_quant_stock_daily",
        "weekly": "asian_quant_stock_weekly",
        "monthly": "asian_quant_stock_monthly"
    },

    # å¤æƒæ–¹å¼ï¼šqfq(å‰å¤æƒ), hfq(åå¤æƒ), None(ä¸å¤æƒ)
    "ADJUST": "qfq",

    # è¿‡æ»¤å™¨é…ç½®
    "EXCLUDE_GEM": True,    # åˆ›ä¸šæ¿è¿‡æ»¤
    "EXCLUDE_KCB": True,    # ç§‘åˆ›æ¿è¿‡æ»¤
    "EXCLUDE_BJ": True,     # åŒ—äº¤æ‰€åŠæ–°ä¸‰æ¿è¿‡æ»¤
    "EXCLUDE_ST": False,    # æ’é™¤ ST/*ST (ç‰¹åˆ«å¤„ç†/é€€å¸‚é£é™©è­¦ç¤º)
    "EXCLUDE_DELIST": True, # æ’é™¤é€€å¸‚è‚¡ (åŒ…å«åç§°å«"é€€"æˆ–"é€€å¸‚"çš„æ•´ç†æœŸè‚¡ç¥¨)
}

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = conf.DB_CONFIG

# è·¯å¾„ç®¡ç†
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CURRENT_DATE_STR = datetime.now().strftime("%Y%m%d")
LOG_DIR = os.path.join(BASE_DIR, "data/logs", CURRENT_DATE_STR)
CACHE_DIR = os.path.join(BASE_DIR, "data/cache")
CACHE_PATH = os.path.join(CACHE_DIR, "stock_list_cache.json")

os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)


# ---------------------------------------------------------
# æ·±åº¦å®šåˆ¶æ—¥å¿—ç³»ç»Ÿ (æ”¯æŒé¢œè‰² & å…¼å®¹ Tqdm)
# ---------------------------------------------------------
class ColoredFormatter(logging.Formatter):
    """è‡ªå®šä¹‰å½©è‰²æ ¼å¼åŒ–ç±»"""
    MAPPING = {
        logging.INFO: f"{C_BLUE}%(asctime)s [INFO]{C_END} %(message)s",
        logging.WARNING: f"{C_YELLOW}%(asctime)s [WARN]{C_END} %(message)s",
        logging.ERROR: f"{C_RED}%(asctime)s [ERROR]{C_END} %(message)s",
        logging.CRITICAL: f"{C_BOLD}{C_RED}%(asctime)s [CRIT]{C_END} %(message)s",
    }

    def format(self, record):
        fmt = self.MAPPING.get(record.levelno)
        return logging.Formatter(fmt, datefmt='%H:%M:%S').format(record)


class TqdmLoggingHandler(logging.Handler):
    """ç¡®ä¿æ—¥å¿—è¾“å‡ºä¸ç ´åè¿›åº¦æ¡è¡Œçš„å¤„ç†å™¨"""

    def emit(self, record):
        try:
            msg = self.format(record)
            # å…³é”®ï¼šä½¿ç”¨ tqdm.write æ›¿ä»£ sys.stdout.write
            tqdm.write(msg, file=sys.stdout)
            self.flush()
        except Exception:
            self.handleError(record)


def setup_logger():
    """åˆå§‹åŒ–å…¨å±€æ—¥å¿—è®°å½•å™¨"""
    l = logging.getLogger('StockETL')
    l.setLevel(logging.INFO)

    # æ¸…ç†å†å² Handlerï¼Œé˜²æ­¢é‡å¤æ‰“å°
    if l.handlers:
        l.handlers.clear()
    l.propagate = False

    # 1. ç‰©ç†æ–‡ä»¶ Handler (è®°å½•åŸå§‹æ–‡æœ¬ï¼Œæ—  ANSI é¢œè‰²ç )
    file_fmt = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')
    fh = logging.FileHandler(os.path.join(LOG_DIR, "etl.log"), encoding='utf-8')
    fh.setFormatter(file_fmt)
    l.addHandler(fh)

    # 2. ç»ˆç«¯ Tqdm Handler (å½©è‰²)
    th = TqdmLoggingHandler()
    th.setFormatter(ColoredFormatter())
    l.addHandler(th)
    return l


logger = setup_logger()


# ---------------------------------------------------------
# åŸºç¡€å·¥å…·å‡½æ•° (è¿‡æ»¤ã€æ•°æ®åº“ã€ç¼“å­˜)
# ---------------------------------------------------------
def get_engine():
    """åˆ›å»º SQLAlchemy å¼•æ“"""
    c = DB_CONFIG
    conn_url = (
        f"mysql+pymysql://{c['USER']}:{c['PASS']}@"
        f"{c['HOST']}:{c['PORT']}/{c['DB_NAME']}?charset=utf8mb4"
    )
    return create_engine(conn_url, pool_recycle=3600)


def apply_filters(df):
    """
    å¯¹ A è‚¡å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨è¿›è¡Œå¤šç»´åº¦æ¸…æ´—ã€‚
    ä¸»è¦ä»»åŠ¡ï¼šå‰”é™¤ ST é£é™©è‚¡ã€é A è‚¡æ¿å—ï¼ˆB è‚¡ï¼‰ã€ä»¥åŠæ ¹æ®é…ç½®æ’é™¤ç‰¹å®šäº¤æ˜“è§„åˆ™çš„æ¿å—ã€‚
    """
    before_count = len(df)
    # ---------------------------------------------------------
    # 1. é£é™©è­¦ç¤ºè‚¡è¿‡æ»¤ (ST/*ST)
    # é€»è¾‘ï¼šåŒ¹é…åç§°ä¸­åŒ…å« 'ST' ä½†ä¸å« 'é€€' çš„è‚¡ç¥¨
    # ---------------------------------------------------------
    if CONFIG.get("EXCLUDE_ST", True):
        # ä»…åŒ¹é… ST å’Œ *ST
        df = df[~df['name'].str.contains(r'ST|\*ST', flags=re.IGNORECASE)]

    # ---------------------------------------------------------
    # 2. ç»ˆæ­¢ä¸Šå¸‚è‚¡è¿‡æ»¤ (é€€å¸‚æ•´ç†æœŸ)
    # é€»è¾‘ï¼šåŒ¹é…åç§°ä¸­åŒ…å« 'é€€' æˆ– 'é€€å¸‚' çš„è‚¡ç¥¨
    # è¿™äº›è‚¡ç¥¨é€šå¸¸å¤„äº 15 ä¸ªäº¤æ˜“æ—¥çš„é€€å¸‚æ•´ç†æœŸï¼ŒæµåŠ¨æ€§æå·®ä¸”é¢ä¸´å½’é›¶é£é™©
    # ---------------------------------------------------------
    if CONFIG.get("EXCLUDE_DELIST", True):
        # åŒ¹é…åŒ…å« "é€€" å­—æˆ– "é€€å¸‚" å­—æ ·çš„è‚¡ç¥¨
        df = df[~df['name'].str.contains(r'é€€å¸‚|é€€')]

    # ---------------------------------------------------------
    # 3. é A è‚¡ä¸šåŠ¡è¿‡æ»¤ (B è‚¡æ’é™¤)
    # é€»è¾‘ï¼šé˜²æ­¢æ¥å£æ··å…¥ä»¥ç¾å…ƒæˆ–æ¸¯å¸è®¡ä»·çš„ B è‚¡ã€‚
    # æ²ªå¸‚ B è‚¡å¼€å¤´ä¸º 900ï¼›æ·±å¸‚ B è‚¡å¼€å¤´ä¸º 200ã€‚
    # ---------------------------------------------------------
    df = df[~df['code'].str.startswith(('900', '200'))]

    # ---------------------------------------------------------
    # 4. åˆ›ä¸šæ¿è¿‡æ»¤ (Growth Enterprise Market)
    # é€»è¾‘ï¼šæ’é™¤ 300 å’Œ 301 å¼€å¤´çš„è‚¡ç¥¨ã€‚
    # äº¤æ˜“è§„åˆ™ï¼š20% æ¶¨è·Œå¹…ï¼Œä¸”éœ€è¦ä¸“é—¨çš„å¼€é€šæƒé™ã€‚
    # ---------------------------------------------------------
    if CONFIG.get("EXCLUDE_GEM", True):
        df = df[~df['code'].str.startswith(('300', '301'))]

    # ---------------------------------------------------------
    # 5. ç§‘åˆ›æ¿è¿‡æ»¤ (STAR Market)
    # é€»è¾‘ï¼šæ’é™¤ 688 å’Œ 689 å¼€å¤´çš„è‚¡ç¥¨ã€‚
    # ç‰¹åˆ«è¯´æ˜ï¼š688 ä¸ºæ™®é€šç§‘åˆ›æ¿ï¼Œ689 ä¸ºç§‘åˆ›æ¿ CDR (å­˜æ‰˜å‡­è¯ï¼Œå¦‚ä¹å·å…¬å¸)ã€‚
    # äº¤æ˜“è§„åˆ™ï¼š20% æ¶¨è·Œå¹…ï¼Œ50ä¸‡èµ„é‡‘é—¨æ§›ã€‚
    # ---------------------------------------------------------
    if CONFIG.get("EXCLUDE_KCB", True):
        df = df[~df['code'].str.startswith(('688', '689'))]

    # ---------------------------------------------------------
    # 6. åŒ—äº¤æ‰€åŠæ–°ä¸‰æ¿è¿‡æ»¤ (Beijing Stock Exchange)
    # é€»è¾‘ï¼šæ’é™¤ 4, 8, 92 å¼€å¤´çš„è‚¡ç¥¨ã€‚
    # 43/83/87/88ï¼šåŒ—äº¤æ‰€åŠç²¾é€‰å±‚ï¼›920ï¼šåŒ—äº¤æ‰€ä¸“ç”¨æ®µï¼›400ï¼šè€ä¸‰æ¿ã€‚
    # äº¤æ˜“è§„åˆ™ï¼š30% æ¶¨è·Œå¹…ã€‚
    # ---------------------------------------------------------
    if CONFIG.get("EXCLUDE_BJ", True):
        df = df[~df['code'].str.startswith(('4', '8', '92'))]

    after_count = len(df)

    # æ‰“å°è¯¦ç»†çš„æ¸…æ´—æŠ¥å‘Šï¼Œæ–¹ä¾¿æ ¸å¯¹æ•°æ®è§„æ¨¡
    logger.info(
        f"ğŸ” {C_BOLD}{C_YELLOW}è¿‡æ»¤æŠ¥è¡¨:{C_END} æ€»æ ·æœ¬: {C_CYAN}{before_count}{C_END}æ”¯ | å‰”é™¤: {C_RED}{before_count - after_count}{C_END}æ”¯ | æœ‰æ•ˆ: {C_GREEN}{after_count}{C_END}æ”¯"
    )

    return df


def get_stock_list_with_cache():
    """è·å– A è‚¡åˆ—è¡¨ï¼Œå¸¦ç‰©ç†ç¼“å­˜æœºåˆ¶"""
    today = datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(CACHE_PATH):
        try:
            with open(CACHE_PATH, "r", encoding="utf-8") as f:
                c = json.load(f)
                if c.get("update_at") == today:
                    logger.info(f"{C_GREEN}âœ… ç¼“å­˜å‘½ä¸­:{C_END} ä½¿ç”¨ä»Šæ—¥å·²å­˜åˆ—è¡¨")
                    return pd.DataFrame(c['data'])
        except Exception:
            pass

    logger.info("ğŸ“¡ æ¥å£æ›´æ–°: æŠ“å–å…¨å¸‚åœºæœ€æ–°ä»£ç ...")
    df = ak.stock_zh_a_spot_em()[['ä»£ç ', 'åç§°']].rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
    df['code'] = df['code'].astype(str)

    with open(CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(
            {
                "update_at": today,
                "data": df.to_dict(orient="records")
            },
            f,
            ensure_ascii=False,
            indent=2
        )
    return df


def get_downloaded_codes():
    """æ‰«ææ•°æ®åº“ï¼Œè®¡ç®—å“ªäº›æ•°æ®å·²ç»åœ¨ç›®æ ‡åŒºé—´å†…å­˜åœ¨"""
    table = CONFIG["TABLE_MAP"].get(CONFIG["PERIOD"])
    adj = CONFIG["ADJUST"] or 'none'

    # æ„é€ æ¡ä»¶æŸ¥è¯¢ SQL
    sql = f"""
        SELECT DISTINCT code 
        FROM {table} 
        WHERE date BETWEEN :s AND :e AND adjust = :adj
    """

    try:
        with get_engine().connect() as conn:
            res = conn.execute(
                text(sql),
                {
                    "s": CONFIG["START_DATE"],
                    "e": CONFIG["END_DATE"],
                    "adj": adj
                }
            )
            return {row[0] for row in res}
    except Exception:
        return set()


# ---------------------------------------------------------
# æŠ“å–ä¸ Upsert é€»è¾‘
# ---------------------------------------------------------
def fetch_stock_data(item, s_date, e_date):
    """æ‰§è¡Œå•åªè‚¡ç¥¨æŠ“å– (å·¥ä½œçº¿ç¨‹å†…è¿è¡Œ)"""
    code = item['code']
    time.sleep(random.uniform(0.3, 0.6))

    last_err = "æœªçŸ¥é”™è¯¯"
    for attempt in range(1, CONFIG["MAX_RETRIES"] + 1):
        try:
            df = ak.stock_zh_a_hist(
                symbol=code,
                period=CONFIG['PERIOD'],
                start_date=s_date,
                end_date=e_date,
                adjust=CONFIG['ADJUST']
            )

            if df is None or df.empty:
                raise ValueError("æ¥å£è¿”å›ç©ºæ•°æ®")

            # âœ… 1. å­—æ®µæ˜ å°„
            mapping = {
                "æ—¥æœŸ": "date",
                "è‚¡ç¥¨ä»£ç ": "code",
                "å¼€ç›˜": "open",
                "æœ€é«˜": "high",
                "æœ€ä½": "low",
                "æ”¶ç›˜": "close",
                "æˆäº¤é‡": "volume",
                "æˆäº¤é¢": "amount",
                "æŒ¯å¹…": "amplitude",
                "æ¶¨è·Œé¢": "chg",
                "æ¶¨è·Œå¹…": "pct_chg",
                "æ¢æ‰‹ç‡": "turnover_rate"

            }
            df = df.rename(columns=mapping)

            # âœ… 2. å¤„ç†æ—¥æœŸæ ¼å¼
            df['date'] = pd.to_datetime(df['date']).dt.date

            # âœ… 3. è¡¥å…¨/è¦†ç›–å…ƒæ•°æ® (ç¡®ä¿ code å’Œ adjust å­˜åœ¨)
            df['code'] = code
            df['adjust'] = CONFIG['ADJUST'] if CONFIG['ADJUST'] else 'none'

            # âœ… 4. å…³é”®æ­¥éª¤ï¼šè¿‡æ»¤æ‰æ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„åˆ—
            # ç¡®ä¿ DataFrame çš„åˆ—é¡ºåºæˆ–å­˜åœ¨æ€§ä¸æ•°æ®åº“è¡¨ç»“æ„å®Œå…¨åŒ¹é…
            db_columns = [
                'code', 'date', 'open', 'high', 'low', 'close',
                'volume', 'amount', 'amplitude', 'pct_chg',
                'chg', 'turnover_rate', 'adjust'
            ]
            # åªä¿ç•™æ•°æ®åº“éœ€è¦çš„åˆ—ï¼Œé˜²æ­¢å¤šä½™çš„ä¸­æ–‡åˆ—å¹²æ‰°
            df = df[db_columns]

            return {
                "code": code,
                "df": df,
                "error": None
            }
        except Exception as e:
            last_err = str(e)
            logger.warning(f"ğŸ”„ {code:<9} | å°è¯• {attempt}/{CONFIG['MAX_RETRIES']} | {last_err[:30]}")
            if attempt < CONFIG["MAX_RETRIES"]:
                time.sleep(1.5)

    return {"code": code, "df": None, "error": last_err}


def mysql_upsert_logic(table, conn, keys, data_iter):
    """
    æ‰§è¡Œ MySQL ON DUPLICATE KEY UPDATE é€»è¾‘
    """
    data_list = [dict(zip(keys, row)) for row in data_iter]

    # æ„é€  SQL
    cols = ", ".join([f"`{k}`" for k in keys])
    plh = ", ".join([f":{k}" for k in keys])
    upd = ", ".join([
        f"`{k}`=VALUES(`{k}`)"
        for k in keys
        if k not in ['date', 'code', 'adjust']
    ])

    sql = f"""
        INSERT INTO {table.name} ({cols}) 
        VALUES ({plh}) 
        ON DUPLICATE KEY UPDATE {upd}
    """
    conn.execute(text(sql), data_list)


# ---------------------------------------------------------
# æ ¸å¿ƒåŒæ­¥å¼•æ“
# ---------------------------------------------------------
async def start_engine(todo_jobs, total_query, already_exist):
    """å¤šçº¿ç¨‹åç¨‹è°ƒåº¦å™¨"""
    sem = asyncio.Semaphore(CONFIG["MAX_WORKERS"])
    loop = asyncio.get_running_loop()

    success_dfs = []
    failed_logs = []

    async def worker(item):
        async with sem:
            with ThreadPoolExecutor() as pool:
                return await loop.run_in_executor(
                    pool,
                    fetch_stock_data,
                    item,
                    CONFIG["START_DATE"],
                    CONFIG["END_DATE"]
                )

    # åˆå§‹åŒ–è¿›åº¦æ¡ UI
    pbar = tqdm(
        total=len(todo_jobs),
        desc=f"{C_BOLD}ğŸ“Š åŒæ­¥è¿›åº¦{C_END}",
        bar_format="{l_bar}%s{bar:25}%s{r_bar}" % (C_GREEN, C_END),
        file=sys.stdout,
        dynamic_ncols=True
    )

    # æäº¤æ‰€æœ‰ä»»åŠ¡å¹¶ç›‘å¬å®Œæˆ
    tasks = [worker(it) for it in todo_jobs]
    for future in asyncio.as_completed(tasks):
        res = await future

        if res["df"] is not None:
            success_dfs.append(res["df"])
        else:
            failed_logs.append(res)

        pbar.update(1)
        # å®æ—¶åˆ·æ–°å››ç»´åº¦ postfix
        pbar.set_postfix({
            "ğŸ¯æŸ¥è¯¢": f"{total_query}",
            "ğŸ“¦å·²æœ‰": f"{C_CYAN}{already_exist}{C_END}",
            "âœ…æˆåŠŸ": f"{C_GREEN}{len(success_dfs)}{C_END}",
            "âŒå¤±è´¥": f"{C_RED}{len(failed_logs)}{C_END}"
        })

    pbar.close()

    # å¤„ç†æœ€ç»ˆç»“æœ
    if failed_logs:
        logger.error(f"{C_BOLD}{' æœ€ç»ˆå¤±è´¥æ±‡æ€» ':=^50}{C_END}")
        for log in failed_logs:
            logger.error(f"ä»£ç : {log['code']:<10} | åŸå› : {log['error']}")

    if success_dfs:
        target = CONFIG["TABLE_MAP"].get(CONFIG["PERIOD"])
        logger.info(f"{C_GREEN}ğŸ’¾ æ­£åœ¨å…¥åº“ {len(success_dfs)} æ”¯è‚¡ç¥¨æ•°æ®...{C_END}")
        try:
            pd.concat(success_dfs).to_sql(
                name=target,
                con=get_engine(),
                if_exists='append',
                index=False,
                chunksize=2000,
                method=mysql_upsert_logic
            )
            logger.info(f"{C_BOLD}{C_GREEN}âœ¨ æ•°æ®åŒæ­¥åœ†æ»¡ç»“æŸï¼{C_END}")
        except Exception as e:
            logger.error(f"ğŸ’” å…¥åº“å¼‚å¸¸: {e}")


def main():
    start_ts = time.time()
    logger.info(f"{C_BOLD}{'=' * 75}{C_END}")
    logger.info(f"{C_BOLD}AsianQuant ETL å¯åŠ¨{C_END} | {C_CYAN}å‘¨æœŸ: {CONFIG['PERIOD']}{C_END} | {C_RED}èŒƒå›´: {CONFIG['START_DATE']}-{CONFIG['END_DATE']}{C_END}")

    try:
        # 1. ç¡®å®šåˆå§‹ä»»åŠ¡é›†
        df_all = get_stock_list_with_cache()

        if CONFIG["TARGET_STOCKS"]:
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡ï¼Œåˆ™è·³è¿‡è¿‡æ»¤é€»è¾‘
            full_jobs = [{"code": str(c)} for c in CONFIG["TARGET_STOCKS"]]
        else:
            # å¦åˆ™æ‰§è¡Œæ¿å—è¿‡æ»¤
            df_filtered = apply_filters(df_all)
            full_jobs = df_filtered.to_dict(orient="records")

        query_total = len(full_jobs)

        # 2. å·®é›†è®¡ç®—ï¼šæ’é™¤åº“ä¸­å·²å­˜åœ¨çš„ (æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒ)
        downloaded = get_downloaded_codes()
        todo_jobs = [j for j in full_jobs if j['code'] not in downloaded]
        already_exist = query_total - len(todo_jobs)

        logger.info(
            f"ğŸ“‹ {C_BOLD}ä»»åŠ¡åˆå§‹: {C_END}æ€»é‡={query_total} | "
            f"å·²æœ‰={already_exist} | å¾…æŠ“={len(todo_jobs)}"
        )

        # 3. å¼‚æ­¥å¯åŠ¨
        if todo_jobs:
            asyncio.run(start_engine(todo_jobs, query_total, already_exist))
        else:
            logger.info(f"{C_GREEN}âœ… åº“ä¸­æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€åŒæ­¥ã€‚{C_END}")

    except Exception as e:
        logger.critical(f"ğŸ›‘ å´©æºƒ: {e}\n{traceback.format_exc()}")

    logger.info(f"{C_BOLD}ğŸ æ€»è€—æ—¶: {time.time() - start_ts:.1f}s{C_END}")
    logger.info(f"{C_BOLD}{'=' * 75}{C_END}")


if __name__ == "__main__":
    main()