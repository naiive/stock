#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
============================================================
A è‚¡å†å²æ•°æ®æ‰¹é‡å¯¼å…¥ MySQL è„šæœ¬ï¼ˆUpsert ä¼˜åŒ–ç‰ˆ - é™„å¸¦ Logging å’ŒåŠ¨æ€ç›®å½•ï¼‰
è¯´æ˜ï¼š
- æ—¥å¿—æ–‡ä»¶å°†ä¿å­˜åˆ°å½“å‰ç›®å½•ä¸‹çš„ stocks/YYYYMMDD æ–‡ä»¶å¤¹ä¸­ã€‚
- æ ¸å¿ƒåŠŸèƒ½æ”¹ä¸ºåŸºäºä¸»é”®çš„ **æ›´æ–°æˆ–æ’å…¥ (Upsert)**ã€‚
============================================================
"""

import os
import json
import time
import random
import math
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import asyncio

import pandas as pd
import akshare as ak
from tqdm import tqdm

from sqlalchemy import create_engine, text

import conf.config as conf

# ------------------- åŠ¨æ€æ—¥å¿—ç›®å½•é…ç½® -------------------

# 1. ç¡®å®šæ—¥æœŸå’ŒåŸºç¡€ç›®å½•
CURRENT_DATE_STR = datetime.now().strftime("%Y%m%d")
LOG_BASE_DIR = "../logs"
LOG_DAILY_DIR = os.path.join(LOG_BASE_DIR, CURRENT_DATE_STR)

# 2. åˆ›å»ºç›®å½• (å¦‚æœä¸å­˜åœ¨)
# exist_ok=True ç¡®ä¿å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œä¸ä¼šæŠ¥é”™
try:
    os.makedirs(LOG_DAILY_DIR, exist_ok=True)
except Exception as e:
    # å¦‚æœåˆ›å»ºç›®å½•å¤±è´¥ï¼ˆæƒé™ç­‰é—®é¢˜ï¼‰ï¼Œåˆ™é€€å›åˆ°å½“å‰ç›®å½•
    print(f"è­¦å‘Šï¼šæ— æ³•åˆ›å»ºæ—¥å¿—ç›®å½• {LOG_DAILY_DIR}ã€‚æ—¥å¿—å°†ä¿å­˜åˆ°å½“å‰ç›®å½•ã€‚é”™è¯¯: {e}")
    LOG_DAILY_DIR = ".."

LOG_FILE = os.path.join(LOG_DAILY_DIR, "stock_data_pull.log")
# ------------------- /åŠ¨æ€æ—¥å¿—ç›®å½•é…ç½® -------------------


# ------------------- æ—¥å¿—é…ç½® -------------------
# è·å–å¹¶é…ç½® Logger
logger = logging.getLogger('StockPullLogger')
logger.setLevel(logging.INFO)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨ (File Handler)
fh = logging.FileHandler(LOG_FILE, encoding='utf-8')
fh.setLevel(logging.INFO)

# åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨ (Stream Handler)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)

# å®šä¹‰è¾“å‡ºæ ¼å¼
formatter = logging.Formatter(
    '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
)
fh.setFormatter(formatter)
ch.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨åˆ° Logger
# ä½¿ç”¨ len(logger.handlers) æ£€æŸ¥ï¼Œé˜²æ­¢é‡å¤æ·»åŠ  Handler
if not logger.handlers:
    logger.addHandler(fh)
    logger.addHandler(ch)
# ------------------- /æ—¥å¿—é…ç½® -------------------


# ------------------- é…ç½® -------------------
CONFIG = {
    # MySQL
    "DB": conf.DB_CONFIG,

    "MYSQL_TABLE": conf.MYSQL_TABLE,

    # !!! æ•°æ®åº“è¦æ±‚ï¼šç›®æ ‡è¡¨ a_stock_daily å¿…é¡»è®¾ç½® (date, code, adjust) ä¸ºè”åˆä¸»é”®ã€‚

    # æŠ“å–èŒƒå›´æ§åˆ¶ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
    "TARGET_STOCKS": [],      # ä¼˜å…ˆçº§æœ€é«˜ï¼šæŒ‡å®šéœ€è¦æ›´æ–°çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨["600519", "600520"]ã€‚ç©ºåˆ—è¡¨ [] è¡¨ç¤ºå…¨é‡ã€‚
    "TARGET_START_DATE": "",  # ä¼˜å…ˆçº§æ¬¡ä¹‹ï¼šæŒ‡å®šå¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ "YYYYMMDD"ã€‚
    "TARGET_END_DATE": "",    # ä¼˜å…ˆçº§æ¬¡ä¹‹ï¼šæŒ‡å®šç»“æŸæ—¥æœŸï¼Œæ ¼å¼ "YYYYMMDD"ã€‚
    "DAYS": 1,                # ä¼˜å…ˆçº§æœ€ä½ï¼šå¦‚æœ TARGET_START_DATE ä¸ºç©ºï¼Œåˆ™æŠ“å–æœ€è¿‘ DAYS å¤©çš„æ•°æ®ã€‚

    # è¿‡æ»¤
    "EXCLUDE_GEM": True,  # æ’é™¤åˆ›ä¸šæ¿ï¼ˆ300ã€301ï¼‰
    "EXCLUDE_KCB": True,  # æ’é™¤ç§‘åˆ›æ¿ï¼ˆ688ï¼‰
    "EXCLUDE_BJ": True,  # æ’é™¤åŒ—äº¤æ‰€ï¼ˆ8ã€4ã€92ï¼‰
    "EXCLUDE_ST": False,  # æ’é™¤ ST/é€€
    "ADJUST": "qfq",  # 'qfq' / 'hfq' / None

    # å¹¶å‘ä¸è¶…æ—¶
    "MAX_WORKERS": 6,       # å»ºè®® 2~4 æ›´ç¨³
    "REQUEST_TIMEOUT": 28,  # å•æ¬¡ akshare è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
    "CACHE_FILE": "../conf/stock_list_cache.json",

    # é‡è¯•ç­–ç•¥ï¼ˆfetch_data_only å†…éƒ¨ï¼‰
    "RETRY_TIMES": 2,
    "RETRY_BACKOFF_BASE": 1.6,  # æŒ‡æ•°é€€é¿åŸºæ•°
}


# ------------------- /é…ç½® -------------------


# ------------------- æ•°æ®åº“è¿æ¥ -------------------
def get_db_engine():
    db_conf = CONFIG["DB"]
    url = f"mysql+pymysql://{db_conf['user']}:{db_conf['password']}@{db_conf['host']}:{db_conf['port']}/{db_conf['database']}?charset=utf8mb4&local_infile=1"
    engine = create_engine(url, pool_recycle=3600)
    return engine


# ------------------- MySQL Upsert æ–¹æ³•å®šä¹‰ (æœ€ç»ˆä¸”å…¼å®¹å‘½åå‚æ•°ç‰ˆ) -------------------
def mysql_upsert_method(table, conn, keys, data_iter):
    """
    Pandas to_sql è‡ªå®šä¹‰æ–¹æ³•ï¼Œå®ç° MySQL çš„ ON DUPLICATE KEY UPDATE (Upsert)
    å¼ºåˆ¶ä½¿ç”¨å‘½åå‚æ•° (å­—å…¸åˆ—è¡¨) å…¼å®¹ PyMySQL çš„æ‰¹é‡æ‰§è¡Œè¦æ±‚ã€‚
    """

    # 1. å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼ˆå‘½åå‚æ•°æ ¼å¼ï¼‰
    data = [dict(zip(keys, row)) for row in data_iter]

    # 2. æ„å»º INSERT è¯­å¥ (ä½¿ç”¨å‘½åå ä½ç¬¦)
    cols = ", ".join([f"`{col}`" for col in keys])  # åˆ—åç”¨åå¼•å·åŒ…è£¹
    named_placeholders = ", ".join([f":{col}" for col in keys])
    insert_sql = f"INSERT INTO {table.name} ({cols}) VALUES ({named_placeholders})"

    # 3. æ„å»º ON DUPLICATE KEY UPDATE è¯­å¥
    primary_keys = ['date', 'code', 'adjust']
    update_cols = [col for col in keys if col not in primary_keys]

    if not update_cols:
        update_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']

    # ON DUPLICATE KEY UPDATE ä»ç„¶å¼•ç”¨ VALUES()
    update_parts = [f"`{col}`=VALUES(`{col}`)" for col in update_cols]
    update_sql = "ON DUPLICATE KEY UPDATE " + ", ".join(update_parts)

    final_sql = insert_sql + " " + update_sql

    # 4. å‡†å¤‡å‚æ•°åˆ—è¡¨
    params = data

    # 5. æ‰§è¡Œæ‰¹é‡æ“ä½œ
    try:
        conn.execute(text(final_sql), params)
    except Exception as e:
        logger.error(f"æ‰¹é‡ Upsert å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸»é”®æˆ–æ•°æ®æ ¼å¼ã€‚", exc_info=True)
        # æ‰“å°è°ƒè¯•ä¿¡æ¯åˆ°æ—¥å¿—
        logger.debug(f"SQL: {final_sql}")
        if params:
            logger.debug(f"First Row Parameters: {params[0]}")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸


# ------------------- é‡è¯•è£…é¥°å™¨ï¼ˆåŒæ­¥ï¼‰ -------------------
def sync_retry(max_retries=2, backoff_base=1.6):
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exc = None
            for attempt in range(1, max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exc = e
                    if attempt == max_retries:
                        stock_item = args[0]
                        logger.error(f"è‚¡ç¥¨ {stock_item.get('code', 'N/A')} - æœ€ç»ˆé‡è¯•å¤±è´¥ã€‚", exc_info=False)
                        raise
                    sleep_for = (backoff_base ** (attempt - 1)) + random.uniform(0, 0.5)
                    logger.warning(
                        f"è‚¡ç¥¨ {args[0].get('code', 'N/A')} å°è¯• {attempt} å¤±è´¥: {e}ï¼Œç­‰å¾… {sleep_for:.2f}s åé‡è¯•...")
                    time.sleep(sleep_for)
            raise last_exc

        return wrapper

    return decorator


# ------------------- è·å–/ç¼“å­˜å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ -------------------
@sync_retry(max_retries=2, backoff_base=1.5)
def fetch_stock_list_safe():
    """å°è¯•å¤šæ¥å£è·å–è‚¡ç¥¨åˆ—è¡¨"""
    try:
        df = ak.stock_info_a_code_name()
        if not df.empty and "code" in df.columns:
            return df[["code", "name"]]
    except Exception:
        pass

    try:
        df = ak.stock_zh_a_spot_em()
        if "code" in df.columns and "name" in df.columns:
            return df[["code", "name"]]
        if "ä»£ç " in df.columns and "åç§°" in df.columns:
            df = df.rename(columns={"ä»£ç ": "code", "åç§°": "name"})
            return df[["code", "name"]]
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise Exception(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    raise Exception("æœªèƒ½ä»ä»»ä½•æ¥å£è·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")


def get_stock_list_manager():
    cache_file = CONFIG["CACHE_FILE"]
    today_str = datetime.now().strftime("%Y-%m-%d")

    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            if cache.get("time") == today_str:
                logger.info("[ç³»ç»Ÿ] ä»ç¼“å­˜åŠ è½½å…¨é‡è‚¡ç¥¨åˆ—è¡¨ã€‚")
                return pd.DataFrame(cache["data"])
        except Exception:
            logger.warning("[ç³»ç»Ÿ] ç¼“å­˜æ–‡ä»¶æŸåæˆ–è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°è·å–ã€‚")
            pass

    logger.info("[ç³»ç»Ÿ] æ­£åœ¨è·å–å…¨é‡è‚¡ç¥¨åˆ—è¡¨...")
    df = fetch_stock_list_safe()
    if df is None or df.empty:
        raise Exception("è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")

    with open(cache_file, "w", encoding="utf-8") as f:
        data = {
            "time": today_str,
            "data": df.to_dict(orient="records")
        }
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"[ç³»ç»Ÿ] æˆåŠŸè·å–å¹¶ç¼“å­˜ {len(df)} æ”¯è‚¡ç¥¨åˆ—è¡¨ã€‚")

    return df


def filter_stock_list(df):
    if df is None or df.empty:
        return []
    df["code"] = df["code"].astype(str)

    target_stocks = CONFIG["TARGET_STOCKS"]
    if target_stocks:
        logger.info(f"[è¿‡æ»¤] ä»…æŠ“å– TARGET_STOCKS ä¸­æŒ‡å®šçš„ {len(target_stocks)} æ”¯è‚¡ç¥¨ã€‚")
        df = df[df["code"].isin([str(c) for c in target_stocks])]

    mask = pd.Series(False, index=df.index)
    if CONFIG["EXCLUDE_GEM"]:
        mask |= df["code"].str.startswith(("300", "301"))
    if CONFIG["EXCLUDE_KCB"]:
        mask |= df["code"].str.startswith(("688", "689"))
    if CONFIG["EXCLUDE_BJ"]:
        mask |= df["code"].str.startswith(("8", "4", "92"))
    if CONFIG["EXCLUDE_ST"] and "name" in df.columns:
        mask |= df["name"].str.contains("ST|é€€", na=False)

    df_filtered = df[~mask].copy()

    df_filtered['symbol'] = df_filtered["code"].apply(lambda x: f"sh{x}" if x.startswith("6") else f"sz{x}")
    df_filtered['symbol'] = df_filtered['symbol'].astype(str)

    return df_filtered[['code', 'name', 'symbol']].to_dict('records')


# ------------------- æ•°æ®æŠ“å–ï¼ˆåŒæ­¥å‡½æ•°ï¼‰ -------------------
@sync_retry(max_retries=CONFIG["RETRY_TIMES"], backoff_base=CONFIG["RETRY_BACKOFF_BASE"])
def fetch_data_only_sync(item: dict, start_date: str, end_date: str):
    """åŒæ­¥æŠ“å–å•åªè‚¡ç¥¨ï¼ˆå†…éƒ¨å¸¦é‡è¯•ï¼‰"""
    time.sleep(random.uniform(0.6, 1.2))

    code = item['code']
    symbol = item['symbol']
    adjust_type = CONFIG["ADJUST"]

    df = ak.stock_zh_a_daily(
        symbol=symbol,
        start_date=start_date,
        end_date=end_date,
        adjust=adjust_type
    )

    if df is None or df.empty:
        raise Exception(f"æ¥å£è¿”å›ç©ºæˆ–æ— æ•°æ®: {symbol}")

    if df.shape[1] < 5:
        raise Exception("è¿”å›åˆ—æ•°è¿‡å°‘")

    df = df.iloc[:, :7]
    df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount']

    df['code'] = code
    df['adjust'] = adjust_type
    df['date'] = pd.to_datetime(df['date']).dt.date

    df = df[['date', 'code', 'open', 'high', 'low', 'close', 'volume', 'amount', 'adjust']]

    return df


# ------------------- å¼‚æ­¥è°ƒåº¦å™¨ -------------------
async def main_scheduler(target_list):
    # æ ¹æ®é…ç½®è®¡ç®—æœ€ç»ˆçš„ start_date å’Œ end_date
    if CONFIG["TARGET_START_DATE"] and CONFIG["TARGET_END_DATE"]:
        start_date = CONFIG["TARGET_START_DATE"]
        end_date = CONFIG["TARGET_END_DATE"]
        logger.info(f"[èŒƒå›´] ä½¿ç”¨ç”¨æˆ·æŒ‡å®šæ—¥æœŸ: {start_date} ~ {end_date}")
    else:
        end_date = datetime.now().strftime("%Y%m%d")
        days_to_subtract = max(0, CONFIG["DAYS"] - 1)
        start_date = (datetime.now() - timedelta(days=days_to_subtract)).strftime("%Y%m%d")
        logger.info(f"[èŒƒå›´] åŸºäº DAYS={CONFIG['DAYS']} è®¡ç®—çš„èŒƒå›´: {start_date} ~ {end_date}")

    table_name = CONFIG["MYSQL_TABLE"]
    total_stocks = len(target_list)

    logger.info(f"\n[ä»»åŠ¡å¯åŠ¨] æŠ“å–èŒƒå›´: {start_date} ~ {end_date}")
    logger.info(
        f"[é…ç½®] ç›®æ ‡: {total_stocks} æ”¯ | å¹¶å‘ä¸Šé™: {CONFIG['MAX_WORKERS']} | å•æ¬¡è¶…æ—¶: {CONFIG['REQUEST_TIMEOUT']}s")

    loop = asyncio.get_running_loop()
    sem = asyncio.Semaphore(CONFIG["MAX_WORKERS"])
    all_results = []
    failed_items = []

    with ThreadPoolExecutor(max_workers=CONFIG["MAX_WORKERS"]) as pool:

        async def fetch_with_limit(item):
            async with sem:
                try:
                    coro = loop.run_in_executor(pool, fetch_data_only_sync, item, start_date, end_date)
                    df = await asyncio.wait_for(coro, timeout=CONFIG["REQUEST_TIMEOUT"])
                    logger.debug(f"[{item['code']}] æˆåŠŸæŠ“å– {len(df)} æ¡æ•°æ®ã€‚")
                    return df
                except asyncio.TimeoutError:
                    logger.warning(f"[{item['code']}] è¶…æ—¶ ({CONFIG['REQUEST_TIMEOUT']}s)ã€‚")
                    return ("timeout", item)
                except Exception as e:
                    logger.error(f"[{item['code']}] æŠ“å–å¤±è´¥: {e}", exc_info=False)
                    return ("error", item, str(e))

        tasks = [asyncio.create_task(fetch_with_limit(item)) for item in target_list]
        pbar = tqdm(asyncio.as_completed(tasks), total=len(tasks), unit="stock")
        success_count = 0

        for coro in pbar:
            res = await coro
            if isinstance(res, tuple):
                tag = res[0]
                if tag in ("timeout", "error"):
                    failed_items.append(res[1])
            elif res is not None:
                all_results.append(res)
                success_count += 1

            pbar.set_postfix({"æˆåŠŸæŠ“å–": success_count, "æ€»æ•°": total_stocks, "å¤±è´¥å¾…é‡è¯•": len(failed_items)})

    # ç¬¬ä¸€è½®å®Œæˆ
    logger.info(f"\n[ç¬¬ä¸€è½®å®Œæˆ] æˆåŠŸ {len(all_results)} / {total_stocks}ï¼Œå¤±è´¥ {len(failed_items)}ã€‚")

    # ä½å¹¶å‘é‡è¯•
    if failed_items:
        retry_results = []
        retry_workers = min(2, max(1, CONFIG["MAX_WORKERS"] // 2))
        logger.info(f"[é‡è¯•] å¯¹ {len(failed_items)} æ”¯è‚¡ç¥¨è¿›è¡Œä½å¹¶å‘é‡è¯•ï¼ˆå¹¶å‘ {retry_workers}ï¼‰...")

        async def retry_run(item):
            async with asyncio.Semaphore(retry_workers):
                try:
                    with ThreadPoolExecutor(max_workers=1) as temp_pool:
                        coro = loop.run_in_executor(temp_pool, fetch_data_only_sync, item, start_date, end_date)
                        df = await asyncio.wait_for(coro, timeout=max(CONFIG["REQUEST_TIMEOUT"] * 1.2, 30))
                        logger.info(f"[{item['code']}] é‡è¯•æˆåŠŸè¡¥æŠ“ã€‚")
                        return df
                except Exception as e:
                    logger.warning(f"[{item['code']}] é‡è¯•å†æ¬¡å¤±è´¥å¹¶æ”¾å¼ƒ: {e}", exc_info=False)
                    return None

        retry_tasks = [asyncio.create_task(retry_run(it)) for it in failed_items]
        for r in tqdm(asyncio.as_completed(retry_tasks), total=len(retry_tasks), unit="retry"):
            df = await r
            if df is not None:
                retry_results.append(df)

        logger.info(f"[é‡è¯•ç»“æœ] æˆåŠŸè¡¥æŠ“ {len(retry_results)} æ¡")
        all_results.extend(retry_results)

    if not all_results:
        logger.warning("\n[è­¦å‘Š] æœªæŠ“å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œå¯¼å…¥ç»ˆæ­¢ã€‚")
        return

    # åˆå¹¶å¹¶å¯¼å…¥æ•°æ®åº“
    final_df = pd.concat(all_results, ignore_index=True)
    logger.info(f"\n[å¯¼å…¥] å‡†å¤‡æ‰¹é‡ Upsert {len(final_df)} æ¡æ•°æ®åˆ°è¡¨ {table_name} ...")

    try:
        engine = get_db_engine()
        final_df.to_sql(
            name=table_name,
            con=engine,
            if_exists='append',
            index=False,
            chunksize=5000,
            method=mysql_upsert_method
        )
        logger.info(f"[å¯¼å…¥] æ‰¹é‡ Upsert æˆåŠŸã€‚å…±å¤„ç† {len(final_df)} æ¡è®°å½•ã€‚")

    except Exception as e:
        logger.critical(f"âŒ ä»»åŠ¡ç»ˆæ­¢ï¼šæ‰¹é‡ Upsert å¤±è´¥ã€‚")
        print(f"âŒ æ‰¹é‡ Upsert å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ MySQL è¡¨å·²è®¾ç½® (date, code, adjust) ä¸ºè”åˆä¸»é”®ï¼")


# ------------------- ä¸»å…¥å£ -------------------
def main():
    start_time = time.time()
    logger.info("=" * 60)
    logger.info("ğŸ“¢ å†å²æ•°æ®å¯¼å…¥ä»»åŠ¡å¯åŠ¨")
    logger.info(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {os.path.abspath(LOG_FILE)}")
    logger.info("=" * 60)

    try:
        df_base = get_stock_list_manager()
    except Exception as e:
        logger.critical(f"[ç»ˆæ­¢] æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨: {e}", exc_info=True)
        return

    target_list = filter_stock_list(df_base)
    if not target_list:
        logger.warning("[ç»ˆæ­¢] è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¿‡æ»¤æ¡ä»¶æˆ– TARGET_STOCKSã€‚")
        return

    try:
        asyncio.run(main_scheduler(target_list))
    except Exception as e:
        logger.critical(f"\nâŒ ä¸»è°ƒåº¦å™¨è¿è¡Œå‡ºé”™: {e}", exc_info=True)

    elapsed_time = time.time() - start_time
    logger.info("=" * 60)
    logger.info(f"âœ… å†å²æ•°æ® Upsert ä»»åŠ¡å®Œæˆ | æ€»è€—æ—¶: {elapsed_time:.1f}s")
    logger.info("=" * 60)


if __name__ == "__main__":

    # æœ‰æ•°æ®å°±æ‰§è¡Œ mainï¼Œæ²¡æœ‰æ•°å°±ç­‰æœ‰æ•°æ®ååœ¨æ‰§è¡Œ
    # ä¸€èˆ¬16:00å¼€å§‹æœ‰æ•°æ®
    if ak.stock_zh_a_daily(
        symbol="sh600519",
        start_date= CONFIG["TARGET_START_DATE"],
        end_date= CONFIG["TARGET_END_DATE"],
        adjust="qfq"
    ).empty:
        date = CONFIG["TARGET_START_DATE"]
        logger.warning(f"âœ…ä»Šå¤© {date}è¿˜æ²¡æ•°æ®ï¼Œç­‰ã€ak.stock_zh_a_dailyã€‘æ¥å£æœ‰æ•°æ®åœ¨æ‰§è¡Œï¼")
    else:
        main()